{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e1bcb47-dbd1-4d5b-ac37-ef8fa72db755",
   "metadata": {},
   "source": [
    "# Vectors & word embeddings \n",
    "A workshop by UMN LATIS and the Libraries. \n",
    "\n",
    "Portions of the [Constellate's TAP Institute Introduction to Vector Databases and Semantic Searching](https://github.com/wjbmattingly/tap-2024-vector-databases?ref=cms-prod.constellate.org) and [the Carpentries' Text Analysis in Python](https://carpentries-incubator.github.io/python-text-analysis/) lessons are re-used in this lesson. Both lessons are licensed under [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/) as is the content presented below.  \n",
    "\n",
    "## What we'll cover in this session\n",
    "    - Representing words as numbers\n",
    "    - TF-IDF\n",
    "    - Word vectors\n",
    "    - What are they? \n",
    "    - How they work + Benefits\n",
    "    - Word Vectors with Spacy\n",
    "    - Interpreting results\n",
    "    - Word embeddings in 2d plots\n",
    "    - Calculating distance between pairs\n",
    "    - Semantic Search\n",
    "        - Document embedding\n",
    "    - Transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa34013-f237-44b5-8cf3-21640513bf2c",
   "metadata": {},
   "source": [
    "## Install required libraries\n",
    "If you're working from your own machine you can use pip install to make sure you have downloaded all of the Python packages you'll need to use today. \n",
    "\n",
    "If you're working on notebooks.latis.umn.edu, there's no need to install any of these, since they're included in the virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8c54b1-2499-41f2-a6e3-4bf82bc4d00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use !pip install to install libraries we'll use today\n",
    "# numpy 2 does not work with sentence-transformers\n",
    "!pip install spacy scikit-learn pandas matplotlib 'numpy<2' \n",
    "\n",
    "# or use conda\n",
    "!conda install spacy scikit-learn pandas numpy matplotlib 'numpy<2'\n",
    "\n",
    "# This command downloads the medium-sized English language model for spaCy.\n",
    "# It uses the Python module-running option to run spaCy's download command for the \"en_core_web_md\" model.\n",
    "!python -m spacy download en_core_web_md "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78008016-7c30-410c-9896-afec9178ee5c",
   "metadata": {},
   "source": [
    "## Changing words into numbers\n",
    "- Computers require numerical representations of things like images and words to be able to store, process, and manipulate them. \n",
    "- For most text analysis methods that will allows us to work with text as data, then, it's critical to use structured formats that represent words as numbers.\n",
    "- At the base level computers already use encoding standards like Unicode to represent words as numbers, but Unicode represents specific characters in numerical form, not entire words. Since we're interested in working with the meanings of words and not characters, we'll use other methods to create numerical representations of our texts.\n",
    "- Rather than manually assigning words to specific numbers on our own, we can utilize existing vector frameworks to transform our texts to numerical formats that allow us to analyze meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9666ba7a-1234-4050-bcdb-722e47e59f54",
   "metadata": {},
   "source": [
    "### TF-IDF: Term Frequency, Inverse Document Frequency\n",
    "\n",
    "- TF-IDF is a statistical measure using a matrix to evaluate how important a word is to a document or corpus. \n",
    "- In a TF-IDF matrix:\n",
    " - each row represents a document\n",
    " - each column represents a unique word\n",
    " - each cell contains a score for that word in the document\n",
    " \n",
    "The score increases relative to the number of times that a word appears in a document, offset by its frequency in the entire corpus. So words that are more common across the entire corpus will have lower scores in a particular document, which helps us account for common terms. \n",
    "\n",
    "Let's use the spacy package to transform a collection of State of the Union addresses into a TF-IDF Document Term Matrix.\n",
    "\n",
    "#### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c44e0a7-5c52-47a9-9c3c-40eaf3957cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import glob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41553b10-b212-4939-95dd-d1ae2c32c8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create three simple documents\n",
    "document = ['feed the duck', 'feed the goose', 'duck duck goose']\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "# Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(document)\n",
    "\n",
    "# Create a DataFrame for better readability\n",
    "df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260fb9de-55a9-47d9-8801-d18491c22a82",
   "metadata": {},
   "source": [
    "- Each row is a document (e.g., row 0 is \"feed the duck\"). \n",
    "- Each column is a word in our list of documents. \n",
    "- Each cell has a score for the word that increases relative to the number of times that a word appears in a document, offset by its frequency in the entire corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440af74d-81cf-496f-9f7b-e6572b2eeeb3",
   "metadata": {},
   "source": [
    "#### Tokenize the SOTU corpus with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c78d8de8-a5b1-4000-aaf0-e8cc8acec230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all speech file paths\n",
    "sotu = glob.glob(\"../sotu_text/*.txt\")\n",
    "\n",
    "def tokenize_speeches(speeches_glob, n):\n",
    "    # Initialize a list to store preprocessed speeches\n",
    "    processed_speeches = []\n",
    "    \n",
    "    # Process each speech\n",
    "    for speech_path in speeches_glob[0:n]:\n",
    "        with open(speech_path, 'r') as file:\n",
    "            text = file.read()\n",
    "    \n",
    "            #tokenize each document using spacy\n",
    "            doc = nlp(text)\n",
    "            \n",
    "            # Filter tokens and join them into a single string\n",
    "            # ignore stop words, punctuation, spaces and digits\n",
    "            tokens = [token.text.lower() for token in doc if not (token.is_stop or token.is_punct or token.is_space or token.is_digit or token.like_num)]\n",
    "    \n",
    "            # Re-join the list of tokens into a single string\n",
    "            processed_text = ' '.join(tokens)\n",
    "    \n",
    "            # Append each cleaned and tokenized document to our list\n",
    "            processed_speeches.append(processed_text)\n",
    "            \n",
    "    return processed_speeches\n",
    "\n",
    "processed_speeches = tokenize_speeches(sotu, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1900a63-f1ed-48e4-b777-668ff27b68b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fellow citizens senate house representatives benignant providence almighty god representatives states people brought deliberate public good gratitude nation sovereign arbiter human events commensurate boundless blessings enjoy peace plenty contentmen'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the first 250 characters of the first speech, tokenized:\n",
    "processed_speeches[0][0:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5689c4ad-83b6-49f6-a854-899ed0d14eb1",
   "metadata": {},
   "source": [
    "### Convert to a TF-IDF Vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8604db8-aea6-4bdc-9a02-67ebac2687e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TfidfVectorizer\n",
    "# Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the first 5 preprocessed speeches to a TF-IDF matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(processed_speeches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c44374-53db-4986-ade5-a7775887efc6",
   "metadata": {},
   "source": [
    "You can type in the name of the class and Shift+Tab to get a pop up with the documentation for `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049b315e-e395-403b-a7b8-aca5396c8cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c992c2-e6f8-4790-bf63-d039ce856198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the matrix shape: 5 rows, by 5,770 columns\n",
    "# each row is a document, each column is a token\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8254b6-a925-4bf7-920f-24607adf954b",
   "metadata": {},
   "source": [
    "#### Use Pandas to view the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b4d3c2-6967-4037-9f82-b7b077deb9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for better readability\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the DataFrame\n",
    "df_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92645368-cad4-43af-ac18-8876802bb8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of top words to extract for each document\n",
    "top_n = 5\n",
    "\n",
    "# Find the top n words for each document\n",
    "top_words_per_document = []\n",
    "for index, row in df_tfidf.iterrows():\n",
    "    top_words = row.sort_values(ascending=False).head(top_n).index.tolist()\n",
    "    top_words_per_document.append(top_words)\n",
    "\n",
    "# Print the top words for each document\n",
    "for doc_index, words in enumerate(top_words_per_document):\n",
    "    print(f\"Document {doc_index + 1} top words:\\n {words}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59715e1c-233b-4a40-abca-49c13dd9b45d",
   "metadata": {},
   "source": [
    "In TF-IDF the words are assigned numbers based on their importance to each document and the corpus, but the numbers don't actually tell us anything about a word's meaning or its relationship to other words. Words vectors give us a way to understand more about how words in a corpus relate to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3c9a46-b75a-4f62-b9b6-81ed001f15ca",
   "metadata": {},
   "source": [
    "## Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c03448-250a-4433-a52a-bbe6474c22c8",
   "metadata": {},
   "source": [
    "When people refer to word vectors, or word embeddings, they're talking about a way of representing each word as a numerical vector in a high-dimensional space, typically from 50 to 300 dimensions. \n",
    "\n",
    "Key properties of word vectors:\n",
    "\n",
    "1. Dimensionality: In TF-IDF there as many dimensions as there are words in the vocabulary. Word vectors typically have fewer dimensions (e.g., 100 or 300).\n",
    "\n",
    "2. Density: Unlike TF-IDF where a vector consists mostly of 0s, word vectors are dense, meaning most elements are non-zero.\n",
    "\n",
    "3. Learned from data: Word vectors are typically learned from large text corpora using machine learning techniques. They capture semantic and syntactic relationships between words based on their usage patterns in the text.\n",
    "\n",
    "4. Semantic relationships: Similar words have similar vectors. For example, the vectors for \"king\" and \"queen\" might be close to each other.\n",
    "\n",
    "5. Arithmetic operations: Word vectors often exhibit interesting arithmetic properties. A classic example is: vector(\"king\") - vector(\"man\") + vector(\"woman\") ≈ vector(\"queen\").\n",
    "\n",
    "## How Word Vectors Work\n",
    "\n",
    "Word vectors work on the principle of distributional semantics, which states that words that occur in similar contexts tend to have similar meanings. Machine learning models analyze large amounts of text data to learn these representations.\n",
    "\n",
    "There are different popular models out there, such as Word2Vec, developed by Google. We'll stick with spaCy, and use the vector model that comes with \"en_core_web_md\".\n",
    "\n",
    "For more information about how these models produce meaningful word embeddings, and how the models are trained, see the Carpentries' lesson on [the Word2Vec algorithm](https://carpentries-incubator.github.io/python-text-analysis/08-wordEmbed_word2vec-algorithm/index.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a811c97-2bfc-48a5-9462-a169a5831436",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = nlp(\"freedom\")\n",
    "vector = token.vector\n",
    "\n",
    "#the vector has 1 row and 300 columns, for each dimension\n",
    "vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c4bb7b-a2da-4e31-937c-e2e6da14f0ab",
   "metadata": {},
   "source": [
    "The vector itself isn't very interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d73b94-eb5d-446b-a4ea-8f58df088a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(vector.reshape(-1, len(vector)))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59366d37-4316-4186-846d-27069630d546",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "But we can compare different word vectors to measure their similarity, in terms of the mathematical differences between the vectors. We'll first use spaCy's similarity method.\n",
    "\n",
    "It’s important to note that the effectiveness of the similarity measurement depends on the quality of the model's embeddings and whether the model has been trained on a relevant corpus for the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd257fe-2298-49bc-a2e3-d09ca25bd995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_words(word1, word2):\n",
    "    similarity = nlp(word1).similarity(nlp(word2))\n",
    "    print(f\"Similarity between '{word1}' and '{word2}': {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547d700e-e27e-4a4a-b994-3e5675eff644",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_words('king', 'president')\n",
    "compare_words('king', 'pauper')\n",
    "compare_words('president', 'pauper')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272de361-230b-4f36-945c-8ca59c94bce6",
   "metadata": {},
   "source": [
    "What does this tell us? It means that semantically and syntactically, king and president are used in more similar ways than king and pauper This is because the embeddings produced are the result of an embedding model that saw a lot of English texts and in those texts, kings and presidents are represented in similar ways, as you would expect. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bdd03b-7320-4acd-926d-bad76552cd00",
   "metadata": {},
   "source": [
    "### Show word embeddings on a 2d plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380e19c4-1935-4543-977e-f51c748ed062",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['nurse', 'doctor', 'farmer', 'athlete', 'librarian', 'teacher']\n",
    "\n",
    "# create a vector for each word in a list called vectors\n",
    "vectors = [nlp(word).vector for word in words]\n",
    "\n",
    "# Reduce the dimensions of the vectors using Principal component analysis (PCA)\n",
    "# We are choosing 2 compenents so we can plot them in a 2D space\n",
    "pca = PCA(n_components = 2)\n",
    "vectors_2d = pca.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb30dcc8-664c-49eb-b1ea-8d41bc936c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the words in 2D space\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], c='b', alpha=0.7)\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]), xytext=(5, 2), \n",
    "                 textcoords='offset points', ha='right', va='bottom',\n",
    "                 bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "                 arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))\n",
    "\n",
    "plt.title(\"Word Embeddings in 2D Space\")\n",
    "plt.xlabel(\"First Principal Component\")\n",
    "plt.ylabel(\"Second Principal Component\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "#plt.savefig('../outputs/word_embeddings_2d.svg', format='svg')\n",
    "#print(\"Plot saved as 'word_embeddings_2d.svg'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0a09e4-fd68-4810-95f9-8e480fc7fafd",
   "metadata": {},
   "source": [
    "This visualization helps us understand how word embeddings capture semantic relationships. Words with similar meanings or uses tend to cluster together in the vector space, while words with different meanings are farther apart. Even in this simplified 2D representation, we can see how the embedding space organizes words in a way that reflects their semantic relationships.\n",
    "\n",
    "Remember, though, that this is a significant simplification of the original high-dimensional space. In the full embedding space, these relationships are even more nuanced and accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e71a7f9-3983-46a1-9b8a-aa1b478eb259",
   "metadata": {},
   "source": [
    "### Euclidean Distance\n",
    "\n",
    "The Euclidian distance formula makes use of the Pythagorean theorem, where $a^2 + b^2 = c^2$. We can draw a triangle between two points, and calculate the hypotenuse to find the distance. This distance formula works in two dimensions, but can also be generalized over as many dimensions as we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bbf414-9ef6-43ff-9c02-c056cbc59989",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate and print the Euclidean distances between pairs\n",
    "print(\"\\nDistances between pairs:\")\n",
    "for i in range(len(words)):\n",
    "    for j in range(i+1, len(words)):\n",
    "        distance = np.linalg.norm(vectors_2d[i] - vectors_2d[j])\n",
    "        print(f\"{words[i]} - {words[j]}: {distance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a924f5e-d9c1-4a7c-b15f-cb27b3545cc7",
   "metadata": {},
   "source": [
    "## Semantic search & vector databases\n",
    "In this section we will create a vector database with many State of the Union documents, and query it semantically. \n",
    "\n",
    "### What are Vector Databases?\n",
    "Vector databases are specialized storage and retrieval systems designed to handle high-dimensional vector data efficiently. Unlike traditional databases that work with structured data like numbers and text, vector databases are optimized for managing and querying vector embeddings - numerical representations of data points in a multi-dimensional space.\n",
    "\n",
    "At their core, vector databases address the challenge of similarity search in large datasets. They excel at finding the most similar items to a given query, which is crucial for applications like recommendation systems, image recognition, and natural language processing. For instance, in a vector database storing product information, you could easily find similar products based on various attributes, all encoded as vectors.\n",
    "\n",
    "The key advantage of vector databases lies in their ability to perform fast approximate nearest neighbor (ANN) searches. Traditional databases might struggle with the \"curse of dimensionality\" when dealing with high-dimensional data, but vector databases employ specialized indexing techniques to maintain efficiency. This makes them particularly useful for AI and machine learning applications, where data is often represented in high-dimensional vector spaces.\n",
    "\n",
    "For those new to the concept, you can think of a vector database as a system that organizes information in a way that mirrors how our brains associate related concepts. Just as we can quickly recall words or images that are similar to a given prompt, vector databases can rapidly retrieve data points that are \"close\" to each other in a mathematical sense. This capability opens up exciting possibilities for creating more intelligent and intuitive data-driven applications across various domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a02cdec-f211-46ed-b88c-5213c24a5910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using !pip install\n",
    "#!pip install annoy txtai sentence-transformers rank-bm25\n",
    "\n",
    "# or conda\n",
    "#!conda install python-annoy txtai sentence-transformers\n",
    "!pip install rank-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0ed5747-cfdc-4d31-8db3-409e6f04e169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from annoy import AnnoyIndex\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88611f8e-4734-4e1a-8fb2-f5107a254a3c",
   "metadata": {},
   "source": [
    "Let's load all of the sotu speeches into an unprocessed and untokenized list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e68e361f-f7d7-48dc-ad51-9ed000b5d32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236\n"
     ]
    }
   ],
   "source": [
    "speeches = []\n",
    "for speech in sotu:\n",
    "    with open(speech, 'r') as file:\n",
    "        speeches.append(file.read())\n",
    "print(len(speeches))        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c025e26-0aeb-431d-bf67-1792ffc13db0",
   "metadata": {},
   "source": [
    "### Vectorize with sentence-transformers\n",
    "Now that we have all our documents, it comes time to vectorize them, or convert them into a sequence of vectors. This is where we pass the texts to a machine learning model and capture the output vector for each of them. To do this, though, we need a model loaded. We will be using the sentence-transformers library. It makes this process as simple as possible with only two lines of code. First, we will need to load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc0d5e80-197e-4b40-ae30-181fe31530b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # This is a standard, efficient model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99aaa4a-3b77-43c8-8feb-fcd7f3f8f2ba",
   "metadata": {},
   "source": [
    "Now that we have loaded our model we can (optionally) specify the specific device. The code below will put it onto your GPU, if available. If you don't know if this is enabled on your device, then it likely is not. The steps to activate cuda are very specific and require you to install certain packages in a certain way. If you do not have cuda, then the default will be the cpu. With 236 documents, this will not be an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23f7e218-b4e4-4559-b0b3-38f0756c0072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad985b39-8e0c-4c64-b42b-d1680be4260a",
   "metadata": {},
   "source": [
    "Let's encode the speeches! We can do that with a single line. I like to set show_progress_bar to True. This allows me to see how long things take on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64eb5efc-959f-429e-9f0c-7e37392f647a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b477f5d5424f85b1c64a23f96b3362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = model.encode(speeches, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e330e53-37f5-497e-891e-0920c35003a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speeches</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\n Fellow-Citizens of the Senate and of the ...</td>\n",
       "      <td>[0.006504625, -0.0010013514, 0.009295043, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n Fellow-Citizens of the Senate and House o...</td>\n",
       "      <td>[-0.020296935, -0.019036775, 0.08283116, -0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n Fellow Citizens of the Senate and of the ...</td>\n",
       "      <td>[-0.07354095, 0.06733189, 0.05767685, -0.03805...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\n To the Senate and House of Representative...</td>\n",
       "      <td>[-0.06475066, 0.064519495, 0.06715353, -0.0519...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n To the Congress of the United States: \\n\\...</td>\n",
       "      <td>[-0.05823529, 0.022737347, 0.026741277, -0.026...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>\\n\\n Fellow-Citizens of the Senate and House o...</td>\n",
       "      <td>[-0.0370819, 0.055570964, 0.0578656, -0.012479...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>\\n\\n Fellow Citizens of the Senate and of the ...</td>\n",
       "      <td>[0.041680336, -0.07579356, 0.116802275, -0.068...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>\\n\\n Fellow-Citizens of the Senate and House o...</td>\n",
       "      <td>[-0.061863095, 0.039531752, 0.069955446, -0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>\\n\\n Fellow-Citizens of the Senate and House o...</td>\n",
       "      <td>[-0.01136333, 0.027026301, 0.016789565, -0.037...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>\\n\\n To the Senate and House of Representative...</td>\n",
       "      <td>[-0.030191807, 0.007926432, 0.047670547, 0.048...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>236 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              speeches  \\\n",
       "0    \\n\\n Fellow-Citizens of the Senate and of the ...   \n",
       "1    \\n\\n Fellow-Citizens of the Senate and House o...   \n",
       "2    \\n\\n Fellow Citizens of the Senate and of the ...   \n",
       "3    \\n\\n To the Senate and House of Representative...   \n",
       "4    \\n\\n To the Congress of the United States: \\n\\...   \n",
       "..                                                 ...   \n",
       "231  \\n\\n Fellow-Citizens of the Senate and House o...   \n",
       "232  \\n\\n Fellow Citizens of the Senate and of the ...   \n",
       "233  \\n\\n Fellow-Citizens of the Senate and House o...   \n",
       "234  \\n\\n Fellow-Citizens of the Senate and House o...   \n",
       "235  \\n\\n To the Senate and House of Representative...   \n",
       "\n",
       "                                             embedding  \n",
       "0    [0.006504625, -0.0010013514, 0.009295043, -0.0...  \n",
       "1    [-0.020296935, -0.019036775, 0.08283116, -0.03...  \n",
       "2    [-0.07354095, 0.06733189, 0.05767685, -0.03805...  \n",
       "3    [-0.06475066, 0.064519495, 0.06715353, -0.0519...  \n",
       "4    [-0.05823529, 0.022737347, 0.026741277, -0.026...  \n",
       "..                                                 ...  \n",
       "231  [-0.0370819, 0.055570964, 0.0578656, -0.012479...  \n",
       "232  [0.041680336, -0.07579356, 0.116802275, -0.068...  \n",
       "233  [-0.061863095, 0.039531752, 0.069955446, -0.02...  \n",
       "234  [-0.01136333, 0.027026301, 0.016789565, -0.037...  \n",
       "235  [-0.030191807, 0.007926432, 0.047670547, 0.048...  \n",
       "\n",
       "[236 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(speeches)\n",
    "df.columns = ['speeches']\n",
    "df[\"embedding\"] = list(embeddings)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef8ed3d-cb60-4073-b099-d1526e2fc23b",
   "metadata": {},
   "source": [
    "At this stage, we now have all the data necessary to create a vector database! \n",
    "\n",
    "### BM25\n",
    "BM25 (Best Matching 25) is a ranking function used in information retrieval systems, particularly in search engines. It's an advanced form of TF-IDF (Term Frequency-Inverse Document Frequency) that provides a way to rank documents based on their relevance to a given search query.\n",
    "\n",
    "BM25 improves upon simpler ranking methods by incorporating document length normalization. This means it can account for the fact that longer documents are more likely to contain a given term simply due to their length, rather than because of relevance.\n",
    "\n",
    "The algorithm calculates a score for each document based on the query terms it contains. It considers both how often a term appears in a document (term frequency) and how rare the term is across all documents (inverse document frequency). However, it also applies a saturation function to prevent common terms from dominating the score.\n",
    "\n",
    "For those new to information retrieval, BM25 can be thought of as a way of determining which documents in a collection are most relevant to a user's search query. It's widely used in practice due to its effectiveness and relatively simple implementation.\n",
    "\n",
    "Now, let's look at how we can implement BM25 in Python using a DataFrame's 'speeches' field.\n",
    "\n",
    "To do that, we first need to tokenize our text. We could use `nlp()` to use our spaCy model from earlier, but that would be quite time consuming. To save time let's just split our documents up by whitespace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49ffbe09-b55e-4003-9ad6-74f5bf7a80ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs = []\n",
    "for doc in df[\"speeches\"]:\n",
    "    split_doc = doc.split()\n",
    "    # to use a slower but better tokenizer using spaCy:\n",
    "    # split_doc = nlp(doc)\n",
    "    tokenized_docs.append(split_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32017f2c-852d-41eb-b4e1-a0123c894425",
   "metadata": {},
   "source": [
    "Our BM250kapi tool expects each document to be a list of tokens though, and our current speeches are strings. So let's convert them to make processing easier.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e75883-2af2-4b0a-a0dd-66c29780cb19",
   "metadata": {},
   "source": [
    "This single line of code will create a BM25 index for us! At this stage, we have essentially created a search engine index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "994660b0-9119-46c4-9b04-6857b9209f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_index = BM25Okapi(tokenized_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b731c6a-12cb-42e6-9a45-6d53a2868950",
   "metadata": {},
   "source": [
    "To query it, we need to create a query, tokenize the query, and then use the `get_scores()` method to retrieve the results. It's also best practice to sort these based on the scores. Finally, we can retrieve the results from the original DataFrame. In the cell below, we will have all the code necessary to perform these operations. I've done this so that you can more easily test this out with multiple queries. To understand what's happening in the code, though, let's breakdown each step here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0f54ede-e26a-4478-a88e-6a98972bfb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 5.3509 - Speech 100\n",
      " \n",
      "\n",
      "Mr. President, Mr. Speaker, Members of the 96th Congress, fellow citizens: \n",
      "\n",
      "This last few months has not been an easy time for any of us. As we meet tonight, it has never been more clear that the state of our Union depends on the state of the worl\n",
      "------------- \n",
      "\n",
      "Score: 5.0278 - Speech 51\n",
      " Mr. President, Mr. Speaker, Members of the Congress: \n",
      "\n",
      "This 82d Congress faces as grave a task as any Congress in the history of our Republic. The actions you take will be watched by the whole world. These actions will measure the ability of a free p\n",
      "------------- \n",
      "\n",
      "Score: 4.9385 - Speech 97\n",
      " \n",
      "\n",
      "To the Congress of the United States: \n",
      "\n",
      "I have the honor to report to the Congress on the state of the Union. \n",
      "\n",
      "This is the eighth such report that, as President, I have been privileged to present to you and to the country. On previous occasions, i\n",
      "------------- \n",
      "\n",
      "Score: 4.9246 - Speech 128\n",
      " \n",
      "\n",
      "Mr. President, Mr. Speaker, Members of the 96th Congress, and my fellow citizens:Tonight I want to examine in a broad sense the state of our American Union-how we are building a new foundation for a peaceful and a prosperous world. \n",
      "\n",
      "Our children w\n",
      "------------- \n",
      "\n",
      "Score: 4.9190 - Speech 95\n",
      " To the Congress of the United States: \n",
      "\n",
      "My State of the Union Address will be devoted to a discussion of the most important challenges facing our country as we enter the 1980's. \n",
      "\n",
      "Over the coming year, those challenges will receive my highest priorit\n",
      "------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Soviet Union\"\n",
    "tokenized_query = query.split()\n",
    "doc_scores = bm25_index.get_scores(tokenized_query)\n",
    "ranked_docs = sorted(enumerate(doc_scores), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "for idx, score in ranked_docs:\n",
    "    print(f\"Score: {score:.4f} - Speech {idx}\\n {df['speeches'][idx][0:250]}\")\n",
    "    print(\"-------------\", \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55db7fa3-b82d-4be2-871c-61e70b7459fd",
   "metadata": {},
   "source": [
    "- `query = \"Soviet\"`: This line defines the search query. \n",
    "- `tokenized_query = query.split()`: This line tokenizes the query string. The split() method without arguments splits the string on whitespace, creating a list of individual words. For our simple query, this results in [\"Soviet\", \"Union\"]. \n",
    "- `doc_scores = bm25_index.get_scores(tokenized_query)` - Use the pre-computed BM25 index (bm25_index) to score each document in the corpus based on the tokenized query.\n",
    "- `get_scores()` method calculates a relevance score for each document in relation to the query.\n",
    "- The result `doc_scores` is a list of floating-point numbers, where each number represents the relevance score of the corresponding document in the original corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e85eb75-0051-4db3-9064-bf3425b6534d",
   "metadata": {},
   "source": [
    "### More information\n",
    "- [Transformers and LLMs](https://carpentries-incubator.github.io/python-text-analysis/10-finetuning-transformers/index.html)\n",
    "- [The Word2Vec algorithm](https://carpentries-incubator.github.io/python-text-analysis/08-wordEmbed_word2vec-algorithm/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c7db5e-f0ff-4446-a06d-8fac04a65680",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
