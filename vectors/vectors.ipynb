{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e1bcb47-dbd1-4d5b-ac37-ef8fa72db755",
   "metadata": {},
   "source": [
    "# Vectors & word embeddings \n",
    "A workshop by UMN LATIS and the Libraries.\n",
    "\n",
    "## What we'll cover in this session\n",
    "    - Representing words as numbers\n",
    "    - TF-IDF\n",
    "    - Word vectors\n",
    "    - What are they? \n",
    "    - How they work + Benefits\n",
    "    - Word Vectors with Spacy\n",
    "    - Interpreting results\n",
    "    - Word embeddings in 2d plots\n",
    "    - Calculating distance between pairs\n",
    "    - Semantic Search\n",
    "        - Document embedding\n",
    "    - Transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa34013-f237-44b5-8cf3-21640513bf2c",
   "metadata": {},
   "source": [
    "## Install required libraries\n",
    "If you're working from your own machine you can use pip install to make sure you have downloaded all of the Python packages you'll need to use today. \n",
    "\n",
    "If you're working on notebooks.latis.umn.edu, there's no need to install any of these, since they're included in the virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8c54b1-2499-41f2-a6e3-4bf82bc4d00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use !pip install to install libraries we'll use today\n",
    "!pip install spacy\n",
    "\n",
    "# This command downloads the medium-sized English language model for spaCy.\n",
    "# It uses the Python module-running option to run spaCy's download command for the \"en_core_web_md\" model.\n",
    "!python -m spacy download en_core_web_md "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78008016-7c30-410c-9896-afec9178ee5c",
   "metadata": {},
   "source": [
    "## Changing words into numbers\n",
    "- Computers require numerical representations of things like images and words to be able to store, process, and manipulate them. \n",
    "- For most text analysis methods that will allows us to work with text as data, then, it's critical to use structured formats that represent words as numbers.\n",
    "- At the base level computers already use encoding standards like Unicode to represent words as numbers, but Unicode represents specific characters in numerical form, not entire words. Since we're interested in working with the meanings of words and not characters, we'll use other methods to create numerical representations of our texts.\n",
    "- Rather than manually assigning words to specific numbers on our own, we can utilize existing vector frameworks to transform our texts to numerical formats that allow us to analyze meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9666ba7a-1234-4050-bcdb-722e47e59f54",
   "metadata": {},
   "source": [
    "### TF-IDF: Term Frequency, Inverse Document Frequency\n",
    "\n",
    "- TF-IDF is a statistical measure using a matrix to evaluate how important a word is to a document or corpus. \n",
    "- In a TF-IDF matrix:\n",
    " - each row represents a document\n",
    " - each column represents a unique word\n",
    " - each cell contains a score for that word in the document\n",
    " \n",
    "The score increases relative to the number of times that a word appears in a document, offset by its frequency in the entire corpus. So words that are more common across the entire corpus will have lower scores in a particular document, which helps us account for common terms. \n",
    "\n",
    "Let's use the spacy package to transform a collection of State of the Union addresses into a TF-IDF Document Term Matrix.\n",
    "\n",
    "#### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c44e0a7-5c52-47a9-9c3c-40eaf3957cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import glob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440af74d-81cf-496f-9f7b-e6572b2eeeb3",
   "metadata": {},
   "source": [
    "#### Tokenize the SOTU corpus with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78d8de8-a5b1-4000-aaf0-e8cc8acec230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all speech file paths\n",
    "sotu = glob.glob(\"../sotu_text/*.txt\")\n",
    "\n",
    "# Initialize a list to store preprocessed speeches\n",
    "processed_speeches = []\n",
    "\n",
    "# Process each speech\n",
    "for speech_path in sotu[0:5]:\n",
    "    with open(speech_path, 'r') as file:\n",
    "        text = file.read()\n",
    "        doc = nlp(text)\n",
    "        # Filter tokens and join them into a single string\n",
    "        # ignore stop words, punctuation, spaces and digits\n",
    "        tokens = [token.text.lower() for token in doc if not (token.is_stop or token.is_punct or token.is_space or token.is_digit or token.like_num)]\n",
    "        processed_text = ' '.join(tokens)\n",
    "        processed_speeches.append(processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5689c4ad-83b6-49f6-a854-899ed0d14eb1",
   "metadata": {},
   "source": [
    "### Convert to a TF-IDF Vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8604db8-aea6-4bdc-9a02-67ebac2687e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the first 5 preprocessed speeches to a TF-IDF matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(processed_speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683b300a-fc27-4145-94a1-800c8d6e4b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8254b6-a925-4bf7-920f-24607adf954b",
   "metadata": {},
   "source": [
    "#### Use Pandas to view the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b4d3c2-6967-4037-9f82-b7b077deb9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for better readability\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the DataFrame\n",
    "df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92645368-cad4-43af-ac18-8876802bb8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of top words to extract for each document\n",
    "top_n = 5\n",
    "\n",
    "# Find the top n words for each document\n",
    "top_words_per_document = []\n",
    "for index, row in df_tfidf.iterrows():\n",
    "    top_words = row.sort_values(ascending=False).head(top_n).index.tolist()\n",
    "    top_words_per_document.append(top_words)\n",
    "\n",
    "# Print the top words for each document\n",
    "for doc_index, words in enumerate(top_words_per_document):\n",
    "    print(f\"Document {doc_index + 1} top words: {words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59715e1c-233b-4a40-abca-49c13dd9b45d",
   "metadata": {},
   "source": [
    "In TF-IDF the words are assigned numbers based on their importance to each document and the corpus, but the numbers don't actually tell us anything about a word's meaning or its relationship to other words. Words vectors give us a way to understand more about how words in a corpus relate to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3c9a46-b75a-4f62-b9b6-81ed001f15ca",
   "metadata": {},
   "source": [
    "## Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c03448-250a-4433-a52a-bbe6474c22c8",
   "metadata": {},
   "source": [
    "When people refer to word vectors, or word embeddings, they're talking about a way of representing each word as a numerical vector in a high-dimensional space, typically from 50 to 300 dimensions. \n",
    "\n",
    "Key properties of word vectors:\n",
    "\n",
    "1. Dimensionality: In TF-IDF there as many dimensions as there are words in the vocabulary. Word vectors typically have fewer dimensions (e.g., 100 or 300).\n",
    "\n",
    "2. Density: Unlike TF-IDF where a vector consists mostly of 0s, word vectors are dense, meaning most elements are non-zero.\n",
    "\n",
    "3. Learned from data: Word vectors are typically learned from large text corpora using machine learning techniques. They capture semantic and syntactic relationships between words based on their usage patterns in the text.\n",
    "\n",
    "4. Semantic relationships: Similar words have similar vectors. For example, the vectors for \"king\" and \"queen\" might be close to each other.\n",
    "\n",
    "5. Arithmetic operations: Word vectors often exhibit interesting arithmetic properties. A classic example is: vector(\"king\") - vector(\"man\") + vector(\"woman\") â‰ˆ vector(\"queen\").\n",
    "\n",
    "## How Word Vectors Work\n",
    "\n",
    "Word vectors work on the principle of distributional semantics, which states that words that occur in similar contexts tend to have similar meanings. Machine learning models analyze large amounts of text data to learn these representations.\n",
    "\n",
    "There are different popular models out there, such as Word2Vec, developed by Google. We'll stick with spaCy, and use the vector model that comes with \"en_core_web_md\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a811c97-2bfc-48a5-9462-a169a5831436",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = nlp(\"freedom\")\n",
    "vector = token.vector\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd257fe-2298-49bc-a2e3-d09ca25bd995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_words(word1, word2):\n",
    "    similarity = nlp(word1).similarity(nlp(word2))\n",
    "    print(f\"Similarity between '{word1}' and '{word2}': {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547d700e-e27e-4a4a-b994-3e5675eff644",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_words('king', 'president')\n",
    "compare_words('king', 'pauper')\n",
    "compare_words('president', 'pauper')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bdd03b-7320-4acd-926d-bad76552cd00",
   "metadata": {},
   "source": [
    "### Show word embeddings on a 2d plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380e19c4-1935-4543-977e-f51c748ed062",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['nurse', 'doctor', 'farmer', 'athlete', 'librarian', 'teacher']\n",
    "\n",
    "vectors = [nlp(word).vector for word in words]\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "vectors_2d = pca.fit_transform(vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb30dcc8-664c-49eb-b1ea-8d41bc936c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the words in 2D space\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], c='b', alpha=0.7)\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]), xytext=(5, 2), \n",
    "                 textcoords='offset points', ha='right', va='bottom',\n",
    "                 bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "                 arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))\n",
    "\n",
    "plt.title(\"Word Embeddings in 2D Space\")\n",
    "plt.xlabel(\"First Principal Component\")\n",
    "plt.ylabel(\"Second Principal Component\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Instead of plt.show(), we'll save the plot as an SVG\n",
    "#plt.savefig('../outputs/word_embeddings_2d.svg', format='svg')\n",
    "#print(\"Plot saved as 'word_embeddings_2d.svg'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bbf414-9ef6-43ff-9c02-c056cbc59989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print the Euclidean distances between pairs\n",
    "print(\"\\nDistances between pairs:\")\n",
    "for i in range(len(words)):\n",
    "    for j in range(i+1, len(words)):\n",
    "        distance = np.linalg.norm(vectors_2d[i] - vectors_2d[j])\n",
    "        print(f\"{words[i]} - {words[j]}: {distance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b09c94-05e5-44ca-b7cc-1358f4e026cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
