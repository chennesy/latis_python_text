{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e1bcb47-dbd1-4d5b-ac37-ef8fa72db755",
   "metadata": {},
   "source": [
    "# Vectors & word embeddings \n",
    "A workshop by UMN LATIS and the Libraries. \n",
    "\n",
    "Portions of the [Constellate's TAP Institute Introduction to Vector Databases and Semantic Searching](https://github.com/wjbmattingly/tap-2024-vector-databases?ref=cms-prod.constellate.org), [the Carpentries' Text Analysis in Python](https://carpentries-incubator.github.io/python-text-analysis/), and [UC Berkeley D-Lab's Text Analysis](https://github.com/dlab-berkeley/Python-Text-Analysis/tree/main) lessons are re-used in this lesson. These lessons are licensed under [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/) as is the content presented below.  \n",
    "\n",
    "## What we'll cover in this session\n",
    "    - Representing words as numbers\n",
    "    - TF-IDF\n",
    "    - Word vectors\n",
    "    - What are they? \n",
    "    - How they work + Benefits\n",
    "    - Word Vectors with Spacy\n",
    "    - Interpreting results\n",
    "    - Word embeddings in 2d plots\n",
    "    - Calculating distance between pairs\n",
    "    - Semantic Search\n",
    "        - Document embedding\n",
    "    - Transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa34013-f237-44b5-8cf3-21640513bf2c",
   "metadata": {},
   "source": [
    "## Install required libraries\n",
    "If you're working from your own machine you can use pip install to make sure you have downloaded all of the Python packages you'll need to use today. \n",
    "\n",
    "If you're working on notebooks.latis.umn.edu, there's no need to install any of these, since they're included in the virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8c54b1-2499-41f2-a6e3-4bf82bc4d00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use !pip install to install libraries we'll use today\n",
    "# numpy 2 does not work with sentence-transformers\n",
    "#!pip install spacy scikit-learn pandas matplotlib 'numpy<2' \n",
    " \n",
    "# or use conda\n",
    "#!conda install spacy scikit-learn pandas numpy matplotlib 'numpy<2'\n",
    "\n",
    "# This command downloads the medium-sized English language model for spaCy.\n",
    "# It uses the Python module-running option to run spaCy's download command for the \"en_core_web_md\" model.\n",
    "#!python -m spacy download en_core_web_md "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78008016-7c30-410c-9896-afec9178ee5c",
   "metadata": {},
   "source": [
    "## Changing words into numbers\n",
    "- Computers require numerical representations of things like images and words to be able to store, process, and manipulate them. \n",
    "- For most text analysis methods that will allows us to work with text as data, then, it's critical to use structured formats that represent words as numbers.\n",
    "- At the base level computers already use encoding standards like Unicode to represent words as numbers, but Unicode represents specific characters in numerical form, not entire words. Since we're interested in working with the meanings of words and not characters, we'll use other methods to create numerical representations of our texts.\n",
    "- Rather than manually assigning words to specific numbers on our own, we can utilize existing vector frameworks to transform our texts to numerical formats that allow us to analyze meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f1b6e0-3225-4d1a-96e1-57e7cbb6e8d6",
   "metadata": {},
   "source": [
    "## The Bag-of-Words Representation\n",
    "\n",
    "The idea of bag-of-words (BoW), as the name suggests, is quite intuitive: we take a document and toss it in a bag. The action of \"throwing\" the document in a bag disregards relative position between words, so what is \"in the bag\" is essentially an unsorted set of words [(Jurafsky & Martin, 2024, p.62)](https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf). In return, we have a list of unique words and the frequency of each of them. \n",
    "\n",
    "For example, as shown in the following illustration, the word \"coffee\" appears twice. \n",
    "\n",
    "<img src='bow-illustration-1.png' alt=\"BoW-Part2\" width=\"600\">\n",
    "\n",
    "As you may have realized now, with a bag-of-words representation, we make heavy use of word frequency but not too much of word order. \n",
    "\n",
    "In the context of sentiment analysis, the sentiment of a tweet is conveyed more strongly by specific words. For example, if a tweet contains the word \"happy\", it likely conveys positive sentiment, but not always (e.g., \"not happy\" denotes the opposite sentiment). When these words come up more often, they'll probably more strongly convey the sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d7b915-fa9c-46f9-9f09-fb9cc0892df4",
   "metadata": {},
   "source": [
    "## Document Term Matrix\n",
    "\n",
    "Now let's implement the idea of bag-of-words. Before we go deep into that, let's step back for a moment. In practice, text analysis often involves handling many documents; from now on, we use the term **document** to indicate a piece of text that we perform analysis on. It could be a phrase, a sentence, a tweet, and etc, as long as it could be represented by a string of text, the length dosen't really matter. \n",
    "\n",
    "Imagine we have four documents (i.e., the four phrases shown above), we toss them all in the bag. Instead of a word-frequency list, we would expect a document-term matrix (DTM) in return. In a DTM, the word list is the **vocabulary** (V) that holds all unique words occur across the documents. For each **document** (D), we count the number of occurence of each word in the vocabulary, and then plug the number into the matrix. In other words, the DTM we will need to construct is a $D \\times V$ matrix, where each row corresponds to a document, and each column corresponds to a token (or \"term\").\n",
    "\n",
    "In the following example, the unique tokens in this set of documents, in alphabetical order, are in columns. For each document, we mark the occurence of each word showing up in the document. The numerical representation for each document is a row in the matrix. For example, \"the coffee roaster\" or the first document has numerical representation $[0, 1, 0, 0, 0, 1, 1, 0]$.\n",
    "\n",
    "Note that the left index column now displays these documents as texts, but typically we would just assign an index to each of them. \n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cccccccccccc}\n",
    " & \\text{americano} & \\text{coffee} & \\text{iced} & \\text{light} & \\text{roast} & \\text{roaster} & \\text{the} & \\text{time} \\\\\\hline\n",
    "\\text{the coffee roaster} &0 &1\t&0\t&0\t&0\t&1\t&1\t&0 \\\\ \n",
    "\\text{light roast} &0 &0\t&0\t&1\t&1\t&0\t&0\t&0 \\\\\n",
    "\\text{iced americano} &1 &0\t&1\t&0\t&0\t&0\t&0\t&0 \\\\\n",
    "\\text{coffee time} &0 &1\t&0\t&0\t&0\t&0\t&0\t&1 \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "To create a DTM, we will use `CountVectorizer` from the package `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53e0e416-6193-439b-8427-1bdc9e782d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f01fbe-d6f4-4483-93eb-48bd7d7d0514",
   "metadata": {},
   "source": [
    "Let's use our toy example to take a closer look.\n",
    "\n",
    "The first step is to initialize a CountVectorizer object. Within the round paratheses is the parameter setting we may choose to specify. Let's take a look at the documentation and see what options are available.\n",
    "\n",
    "For now we can just leave it blank for the default setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37f14d83-8813-41df-a319-34a80107b19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A toy example containing four documents (phrases)\n",
    "test = ['the coffee roaster',\n",
    "        'light roast',\n",
    "        'iced americano',\n",
    "        'coffee time']\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b77540a-6f46-4353-8543-ce4d0cbe00aa",
   "metadata": {},
   "source": [
    "The second step is to `fit` this `CountVectorizer` object to the data, which means creating a vocabulary of tokens from the set of documents. Thirdly, we `transform` our data according to the \"fitted\" `CountVectorizer` object, which means taking each of the document and transforming it into a DTM according to the vocabulary established by the \"fitting\" step.\n",
    "\n",
    "It may sound a bit complex but steps 2 and 3 can actually be done in one swoop using a `fit_transform` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff610d7b-f961-4454-a9f6-6af193879edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x8 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and transform to create DTM\n",
    "test_count = vectorizer.fit_transform(test)\n",
    "test_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5e673a-e7c7-4888-ae65-069eae648bce",
   "metadata": {},
   "source": [
    "Apparently the return is a \"sparse matrix\"—a matrix that contains a lot zeros. It actually makes sense. For each document we definitely have words that don't occur at all, which are counted zero in the DTM. This sparse matrix is stored in a \"Compressed Sparse Row\" format, which is a memory-saving format that is designed to deal with sparse matrix. \n",
    "\n",
    "Let's convert it to a dense matrix, where those zeros are probably organized, as in a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2134d735-a62b-4919-bbf3-82bb496a7b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 1, 0, 0, 0, 1, 1, 0],\n",
       "        [0, 0, 0, 1, 1, 0, 0, 0],\n",
       "        [1, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert DTM to a dense matrix \n",
    "test_count.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e0c4da-ed09-4367-ae3b-06022dde67ec",
   "metadata": {},
   "source": [
    "So this is our DTM! It is the same as shown above, but to make it more reader-friendly, let's convert it to a dataframe. The column names should be tokens in the vocabulary, which we can access with `get_feature_names_out()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d60b6ae-cfcb-4178-84ab-bcc2b5e46d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['americano', 'coffee', 'iced', 'light', 'roast', 'roaster', 'the',\n",
       "       'time'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the vocabulary\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e0e0c3b-429f-4cc6-affc-e8428dce67da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Create a DTM dataframe\n",
    "test_dtm = pd.DataFrame(data=test_count.todense(),\n",
    "                        columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d21f25f1-0731-483a-a3f7-48a1b9aa4185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>americano</th>\n",
       "      <th>coffee</th>\n",
       "      <th>iced</th>\n",
       "      <th>light</th>\n",
       "      <th>roast</th>\n",
       "      <th>roaster</th>\n",
       "      <th>the</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   americano  coffee  iced  light  roast  roaster  the  time\n",
       "0          0       1     0      0      0        1    1     0\n",
       "1          0       0     0      1      1        0    0     0\n",
       "2          1       0     1      0      0        0    0     0\n",
       "3          0       1     0      0      0        0    0     1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9666ba7a-1234-4050-bcdb-722e47e59f54",
   "metadata": {},
   "source": [
    "### TF-IDF: Term Frequency, Inverse Document Frequency\n",
    "\n",
    "- TF-IDF is a statistical measure using a matrix to evaluate how important a word is to a document or corpus. \n",
    "- In a TF-IDF matrix:\n",
    "    - each row represents a document\n",
    "    - each column represents a unique word\n",
    "    - each cell contains a score for that word in the document\n",
    " \n",
    "The score increases relative to the number of times that a word appears in a document, offset by its frequency in the entire corpus. So words that are more common across the entire corpus will have lower scores in a particular document, which helps us account for common terms. \n",
    "\n",
    "Let's use the spacy package to transform a collection of State of the Union addresses into a TF-IDF Document Term Matrix.\n",
    "\n",
    "#### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c44e0a7-5c52-47a9-9c3c-40eaf3957cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import glob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "# note that we installed numpy 1.* since 2 does not work with the sentence-transformers package we work with later in the lesson\n",
    "import numpy as np\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41553b10-b212-4939-95dd-d1ae2c32c8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create three simple documents\n",
    "document = ['feed the duck', 'feed the goose', 'duck duck goose']\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "tfidf_matrix = vectorizer.fit_transform(document)\n",
    "\n",
    "# Create a DataFrame for better readability\n",
    "df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260fb9de-55a9-47d9-8801-d18491c22a82",
   "metadata": {},
   "source": [
    "- Each row is a document (e.g., row 0 is \"feed the duck\"). \n",
    "- Each column is a word in our list of documents. \n",
    "- Each cell has a score for the word that increases relative to the number of times that a word appears in a document, offset by its frequency in the entire corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c44374-53db-4986-ade5-a7775887efc6",
   "metadata": {},
   "source": [
    "You can type in the name of the class and Shift+Tab to get a pop up with the documentation for `TfidfVectorizer` to look into more options you could explore for creating TF-IDF matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049b315e-e395-403b-a7b8-aca5396c8cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440af74d-81cf-496f-9f7b-e6572b2eeeb3",
   "metadata": {},
   "source": [
    "#### Tokenize the SOTU corpus with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78d8de8-a5b1-4000-aaf0-e8cc8acec230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all speech file paths\n",
    "sotu = glob.glob(\"../sotu_kaggle/*.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe068015-9c81-49f9-86da-c9ef05f67121",
   "metadata": {},
   "source": [
    "For those of you who made it to the Text as Data intro workshop, this will be some review. In short, we'll create a function to read in each speech as an item in a list called `processed_speeches`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1900a63-f1ed-48e4-b777-668ff27b68b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_speeches(speeches_glob, n):\n",
    "    # Initialize a list to store preprocessed speeches\n",
    "    processed_speeches = []\n",
    "    \n",
    "    # Process each speech\n",
    "    for speech_path in speeches_glob[0:n]:\n",
    "        with open(speech_path, 'r') as file:\n",
    "            text = file.read()\n",
    "    \n",
    "            #tokenize each document using spacy\n",
    "            doc = nlp(text)\n",
    "            \n",
    "            # Filter tokens and join them into a single string\n",
    "            # ignore stop words, punctuation, spaces and digits\n",
    "            tokens = [token.text.lower() for token in doc if not (token.is_stop or token.is_punct or token.is_space or token.is_digit or token.like_num)]\n",
    "    \n",
    "            # Re-join the list of tokens into a single string\n",
    "            processed_text = ' '.join(tokens)\n",
    "    \n",
    "            # Append each cleaned and tokenized document to our list\n",
    "            processed_speeches.append(processed_text)\n",
    "            \n",
    "    return processed_speeches\n",
    "\n",
    "# let's just read in the first 5 speeches to start off\n",
    "processed_speeches = tokenize_speeches(sotu, 5)\n",
    "\n",
    "# look at the first 250 characters of the first speech, tokenized:\n",
    "processed_speeches[0][0:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5689c4ad-83b6-49f6-a854-899ed0d14eb1",
   "metadata": {},
   "source": [
    "### Convert to a TF-IDF Vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8604db8-aea6-4bdc-9a02-67ebac2687e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the same vectorizer settings from before to\n",
    "# Fit and transform the first 5 preprocessed speeches to a TF-IDF matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(processed_speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c992c2-e6f8-4790-bf63-d039ce856198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the matrix shape: 5 rows, by 5,067 columns\n",
    "# each row is a document, each column is a token\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8254b6-a925-4bf7-920f-24607adf954b",
   "metadata": {},
   "source": [
    "#### Use Pandas to view the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b4d3c2-6967-4037-9f82-b7b077deb9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for better readability\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the DataFrame\n",
    "df_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92645368-cad4-43af-ac18-8876802bb8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of top words to extract for each document\n",
    "top_n = 5\n",
    "\n",
    "# Find the top n words for each document\n",
    "top_words_per_document = []\n",
    "for index, row in df_tfidf.iterrows():\n",
    "    top_words = row.sort_values(ascending=False).head(top_n).index.tolist()\n",
    "    top_words_per_document.append(top_words)\n",
    "\n",
    "# Print the top words for each document\n",
    "for doc_index, words in enumerate(top_words_per_document):\n",
    "    print(f\"Document {doc_index + 1} top words:\\n {words}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59715e1c-233b-4a40-abca-49c13dd9b45d",
   "metadata": {},
   "source": [
    "In TF-IDF the words are assigned numbers based on their importance to each document and the corpus, but the numbers don't actually tell us anything about a word's meaning or its relationship to other words. Words vectors give us a way to understand more about how words in a corpus relate to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3c9a46-b75a-4f62-b9b6-81ed001f15ca",
   "metadata": {},
   "source": [
    "## Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c03448-250a-4433-a52a-bbe6474c22c8",
   "metadata": {},
   "source": [
    "When people refer to word vectors, or word embeddings, they're talking about a way of representing each word as a numerical vector in a high-dimensional space, typically from 50 to 300 dimensions. \n",
    "\n",
    "Key properties of word vectors:\n",
    "\n",
    "1. Dimensionality: In TF-IDF there as many dimensions as there are words in the vocabulary. Word vectors typically have fewer dimensions (e.g., 100 or 300).\n",
    "\n",
    "2. Density: Unlike TF-IDF where a vector consists mostly of 0s, word vectors are dense, meaning most elements are non-zero.\n",
    "\n",
    "3. Learned from data: Word vectors are typically learned from large text corpora using machine learning techniques. They capture semantic and syntactic relationships between words based on their usage patterns in the text. The relationships between the words come entirely from the text corpora they were trained on - not from your documents!\n",
    "\n",
    "4. Semantic relationships: Similar words have similar vectors. For example, the vectors for \"king\" and \"queen\" might be close to each other.\n",
    "\n",
    "5. Arithmetic operations: Word vectors often exhibit interesting arithmetic properties. A classic example is: vector(\"king\") - vector(\"man\") + vector(\"woman\") ≈ vector(\"queen\").\n",
    "\n",
    "## How Word Vectors Work\n",
    "\n",
    "Word vectors work on the principle of distributional semantics, which states that words that occur in similar contexts tend to have similar meanings. Machine learning models analyze large amounts of text data to learn these representations.\n",
    "\n",
    "There are different popular models out there, such as Word2Vec, developed by Google. We'll stick with spaCy, and use the vector model that comes with \"en_core_web_md\".\n",
    "\n",
    "For more information about how these models produce meaningful word embeddings, and how the models are trained, see the Carpentries' lesson on [the Word2Vec algorithm](https://carpentries-incubator.github.io/python-text-analysis/08-wordEmbed_word2vec-algorithm/index.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a811c97-2bfc-48a5-9462-a169a5831436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a word to an nlp() object \n",
    "token = nlp(\"freedom\")\n",
    "# save the nlp vector attribute for \"freedom\"\n",
    "vector = token.vector\n",
    "\n",
    "#the vector has 1 row and 300 columns, for each dimension\n",
    "vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c4bb7b-a2da-4e31-937c-e2e6da14f0ab",
   "metadata": {},
   "source": [
    "The vector itself isn't very interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d73b94-eb5d-446b-a4ea-8f58df088a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll reshape the numpy array\n",
    "# -1 infers how many rows are needed and will resolve to 1 since we have a 1d vector\n",
    "df = pd.DataFrame(vector.reshape(-1, len(vector)))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59366d37-4316-4186-846d-27069630d546",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "But we can compare different word vectors to measure their similarity, in terms of the mathematical differences between the vectors. We'll first use spaCy's similarity method.\n",
    "\n",
    "It’s important to note that the effectiveness of the similarity measurement depends on the quality of the model's embeddings and whether the model has been trained on a relevant corpus for the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd257fe-2298-49bc-a2e3-d09ca25bd995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_words(word1, word2):\n",
    "    similarity = nlp(word1).similarity(nlp(word2))\n",
    "    print(f\"Similarity between '{word1}' and '{word2}': {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547d700e-e27e-4a4a-b994-3e5675eff644",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_words('king', 'president')\n",
    "compare_words('king', 'pauper')\n",
    "compare_words('president', 'pauper')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272de361-230b-4f36-945c-8ca59c94bce6",
   "metadata": {},
   "source": [
    "What does this tell us? It means that semantically and syntactically, king and president are used in more similar ways than king and pauper This is because the embeddings produced are the result of an embedding model that saw a lot of English texts and in those texts, kings and presidents are represented in similar ways, as you would expect. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bdd03b-7320-4acd-926d-bad76552cd00",
   "metadata": {},
   "source": [
    "### Word embeddings on a 2d plot with PCA\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space while retaining as much of the variance (information) as possible.\n",
    "\n",
    "Our original word vectors are in a high-dimensional space (e.g., 300 dimensions). But visualizing or analyzing high-dimensional data can be challenging. PCA helps reduce these 300 dimensions to a lower number (in this case, 2), making it easier to plot and interpret the relationships between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380e19c4-1935-4543-977e-f51c748ed062",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['nurse', 'doctor', 'farmer', 'athlete', 'librarian', 'teacher']\n",
    "\n",
    "# create a vector for each word in a list called vectors\n",
    "vectors = [nlp(word).vector for word in words]\n",
    "\n",
    "# Reduce the dimensions of the vectors using Principal component analysis (PCA)\n",
    "# We are choosing 2 compenents so we can plot them in a 2D space\n",
    "pca = PCA(n_components = 2)\n",
    "vectors_2d = pca.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb30dcc8-664c-49eb-b1ea-8d41bc936c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the words in 2D space\n",
    "plt.figure(figsize=(12, 8))\n",
    "# scatter plot with first column - the first PC x-coordinates - and 2nd column of array - 2nd PC y-coordinates\n",
    "# c='b' color blue\n",
    "# alpha=0.7 is transparency of points\n",
    "plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], c='b', alpha=0.7)\n",
    "\n",
    "# xytext=(5, 2): offset of the annotation text - 5 points right, 2 points above\n",
    "# ha is horizontal alignment\n",
    "# va vertical\n",
    "# bbox - properties of bounding box\n",
    "# arrowprops - style of arrow annotating points\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]), xytext=(5, 2), \n",
    "                 textcoords='offset points', ha='right', va='bottom',\n",
    "                 bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "                 arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))\n",
    "\n",
    "plt.title(\"Word Embeddings in 2D Space\")\n",
    "plt.xlabel(\"First Principal Component\")\n",
    "plt.ylabel(\"Second Principal Component\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "#plt.savefig('../outputs/word_embeddings_2d.svg', format='svg')\n",
    "#print(\"Plot saved as 'word_embeddings_2d.svg'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0a09e4-fd68-4810-95f9-8e480fc7fafd",
   "metadata": {},
   "source": [
    "This visualization helps us understand how word embeddings capture semantic relationships. Words with similar meanings or uses tend to cluster together in the vector space, while words with different meanings are farther apart. Even in this simplified 2D representation, we can see how the embedding space organizes words in a way that reflects their semantic relationships.\n",
    "\n",
    "Remember, though, that this is a significant simplification of the original high-dimensional space. In the full embedding space, these relationships are even more nuanced and accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e71a7f9-3983-46a1-9b8a-aa1b478eb259",
   "metadata": {},
   "source": [
    "### Euclidean Distance\n",
    "\n",
    "The Euclidian distance formula makes use of the Pythagorean theorem, where $a^2 + b^2 = c^2$. We can draw a triangle between two points, and calculate the hypotenuse to find the distance. This distance formula works in two dimensions, but can also be generalized over as many dimensions as we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bbf414-9ef6-43ff-9c02-c056cbc59989",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate and print the Euclidean distances between pairs\n",
    "print(\"\\nDistances between pairs:\")\n",
    "for i in range(len(words)):\n",
    "    # Loop through the remaining words (starting from the next word after i)\n",
    "    for j in range(i+1, len(words)):\n",
    "        # Calculate the Euclidean distance between the 2D vectors of word i and word j\n",
    "        distance = np.linalg.norm(vectors_2d[i] - vectors_2d[j])\n",
    "        # Print the distance between word i and word j with 4 decimal places\n",
    "        print(f\"{words[i]} - {words[j]}: {distance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a924f5e-d9c1-4a7c-b15f-cb27b3545cc7",
   "metadata": {},
   "source": [
    "## Semantic search & vector databases\n",
    "In this section we will create a vector database with many State of the Union documents, and query it semantically. \n",
    "\n",
    "### What are Vector Databases?\n",
    "Vector databases are specialized storage and retrieval systems designed to handle high-dimensional vector data efficiently. Unlike traditional databases that work with structured data like numbers and text, vector databases are optimized for managing and querying vector embeddings - numerical representations of data points in a multi-dimensional space.\n",
    "\n",
    "At their core, vector databases address the challenge of similarity search in large datasets. They excel at finding the most similar items to a given query, which is crucial for applications like recommendation systems, image recognition, and natural language processing. For instance, in a vector database storing product information, you could easily find similar products based on various attributes, all encoded as vectors.\n",
    "\n",
    "The key advantage of vector databases lies in their ability to perform fast approximate nearest neighbor (ANN) searches. Traditional databases might struggle with the \"curse of dimensionality\" when dealing with high-dimensional data, but vector databases employ specialized indexing techniques to maintain efficiency. This makes them particularly useful for AI and machine learning applications, where data is often represented in high-dimensional vector spaces.\n",
    "\n",
    "For those new to the concept, you can think of a vector database as a system that organizes information in a way that mirrors how our brains associate related concepts. Just as we can quickly recall words or images that are similar to a given prompt, vector databases can rapidly retrieve data points that are \"close\" to each other in a mathematical sense. This capability opens up exciting possibilities for creating more intelligent and intuitive data-driven applications across various domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a02cdec-f211-46ed-b88c-5213c24a5910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using !pip install\n",
    "#!pip install annoy txtai sentence-transformers rank-bm25\n",
    "\n",
    "# or conda\n",
    "#!conda install python-annoy txtai sentence-transformers\n",
    "#!pip install rank-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ed5747-cfdc-4d31-8db3-409e6f04e169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from annoy import AnnoyIndex\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88611f8e-4734-4e1a-8fb2-f5107a254a3c",
   "metadata": {},
   "source": [
    "Let's load all of the sotu speeches into an unprocessed and untokenized list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68e361f-f7d7-48dc-ad51-9ed000b5d32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sotu_kaggle = glob.glob(\"../sotu_kaggle/*.txt\")\n",
    "\n",
    "# use a dictionary so we can keep track of the filenames\n",
    "speeches = {}\n",
    "for speech in sorted(sotu_kaggle):\n",
    "    with open(speech, 'r') as file:\n",
    "        speech = speech.lstrip('../sotu_kaggle/')[:-4]\n",
    "        speeches[speech] = file.read()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6770409-57e7-4a09-92a2-b6a690708028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the dictionary to a df\n",
    "df = pd.DataFrame(speeches.items(), columns=['name_year', 'text'])   \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa89c462-7a4a-4b43-8a77-b144a91d63e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split name_date column by delimiter\n",
    "df[['name','year']] = df['name_year'].str.split('_',expand=True)\n",
    "df = df.drop('name_year', axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c025e26-0aeb-431d-bf67-1792ffc13db0",
   "metadata": {},
   "source": [
    "### Vectorize with sentence-transformers\n",
    "Now that we have all our documents, it comes time to vectorize them, or convert them into a sequence of vectors. This is where we pass the texts to a machine learning model and capture the output vector for each of them. To do this, though, we need a model loaded. We will be using the sentence-transformers library. It makes this process as simple as possible with only two lines of code. First, we will need to load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0d5e80-197e-4b40-ae30-181fe31530b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # This is a standard, efficient model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99aaa4a-3b77-43c8-8feb-fcd7f3f8f2ba",
   "metadata": {},
   "source": [
    "Now that we have loaded our model we can (optionally) specify the specific device. The code below will put it onto your GPU, if available. If you don't know if this is enabled on your device, then it likely is not. The steps to activate cuda are very specific and require you to install certain packages in a certain way. If you do not have cuda, then the default will be the cpu. With 236 documents, this will not be an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f7e218-b4e4-4559-b0b3-38f0756c0072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad985b39-8e0c-4c64-b42b-d1680be4260a",
   "metadata": {},
   "source": [
    "Let's encode the speeches! We can do that with a single line. I like to set show_progress_bar to True. This allows me to see how long things take on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eb5efc-959f-429e-9f0c-7e37392f647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(list(speeches.values()), show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d85b5a-02d0-44a4-8343-6333494169a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load embeddings from the pickle file\n",
    "with open('embeddings.pkl', 'rb') as f:\n",
    "    embeddings = pickle.load(f)\n",
    "\n",
    "print(\"Embeddings loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e330e53-37f5-497e-891e-0920c35003a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add embeddings \n",
    "df[\"embedding\"] = list(embeddings)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c68abf-3f5d-45b8-921a-031de0605145",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now make the year column the index and resort by year\n",
    "#df = df.set_index('year')\n",
    "#df.sort_index(inplace=True)\n",
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef8ed3d-cb60-4073-b099-d1526e2fc23b",
   "metadata": {},
   "source": [
    "At this stage, we now have all the data necessary to create a vector database! \n",
    "\n",
    "### BM25\n",
    "BM25 (Best Matching 25) is a ranking function used in information retrieval systems, particularly in search engines. It's an advanced form of TF-IDF (Term Frequency-Inverse Document Frequency) that provides a way to rank documents based on their relevance to a given search query.\n",
    "\n",
    "BM25 improves upon simpler ranking methods by incorporating document length normalization. This means it can account for the fact that longer documents are more likely to contain a given term simply due to their length, rather than because of relevance.\n",
    "\n",
    "The algorithm calculates a score for each document based on the query terms it contains. It considers both how often a term appears in a document (term frequency) and how rare the term is across all documents (inverse document frequency). However, it also applies a saturation function to prevent common terms from dominating the score.\n",
    "\n",
    "For those new to information retrieval, BM25 can be thought of as a way of determining which documents in a collection are most relevant to a user's search query. It's widely used in practice due to its effectiveness and relatively simple implementation.\n",
    "\n",
    "Now, let's look at how we can implement BM25 in Python using a DataFrame's 'speeches' field.\n",
    "\n",
    "To do that, we first need to tokenize our text. We could use `nlp()` to use our spaCy model from earlier, but that would be quite time consuming. To save time let's just split our documents up by whitespace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ffbe09-b55e-4003-9ad6-74f5bf7a80ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs = []\n",
    "for doc in df[\"text\"]:\n",
    "    split_doc = doc.split()\n",
    "    # to use a slower but better tokenizer using spaCy:\n",
    "    # split_doc = nlp(doc)\n",
    "    tokenized_docs.append(split_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32017f2c-852d-41eb-b4e1-a0123c894425",
   "metadata": {},
   "source": [
    "Our BM250kapi tool expects each document to be a list of tokens though, and our current speeches are strings. So let's convert them to make processing easier.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e75883-2af2-4b0a-a0dd-66c29780cb19",
   "metadata": {},
   "source": [
    "This single line of code will create a BM25 index for us! At this stage, we have essentially created a search engine index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994660b0-9119-46c4-9b04-6857b9209f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_index = BM25Okapi(tokenized_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b731c6a-12cb-42e6-9a45-6d53a2868950",
   "metadata": {},
   "source": [
    "To query it, we need to create a query, tokenize the query, and then use the `get_scores()` method to retrieve the results. It's also best practice to sort these based on the scores. Finally, we can retrieve the results from the original DataFrame. In the cell below, we will have all the code necessary to perform these operations. I've done this so that you can more easily test this out with multiple queries. To understand what's happening in the code, though, let's breakdown each step here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f54ede-e26a-4478-a88e-6a98972bfb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Soviet Union\"\n",
    "tokenized_query = query.split()\n",
    "doc_scores = bm25_index.get_scores(tokenized_query)\n",
    "ranked_docs = sorted(enumerate(doc_scores), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "for idx, score in ranked_docs:\n",
    "    print(f\"Score: {score:.4f} - Speech {idx}: {df['year'][idx]} {df['name'][idx]}\\n {df['text'][idx][0:250]}\")\n",
    "    print(\"-------------\", \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55db7fa3-b82d-4be2-871c-61e70b7459fd",
   "metadata": {},
   "source": [
    "- `query = \"Soviet\"`: This line defines the search query. \n",
    "- `tokenized_query = query.split()`: This line tokenizes the query string. The split() method without arguments splits the string on whitespace, creating a list of individual words. For our simple query, this results in [\"Soviet\", \"Union\"]. \n",
    "- `doc_scores = bm25_index.get_scores(tokenized_query)` - Use the pre-computed BM25 index (bm25_index) to score each document in the corpus based on the tokenized query.\n",
    "- `get_scores()` method calculates a relevance score for each document in relation to the query.\n",
    "- The result `doc_scores` is a list of floating-point numbers, where each number represents the relevance score of the corresponding document in the original corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008678a0-7a2d-424d-88a4-1995ae544754",
   "metadata": {},
   "source": [
    "### Creating a Vector Database\n",
    "Now that we have seen how to create a tradition text database and even learned to query it, let's compare these results to a vector database. To create our database, we will be using Annoy from Spotify. Annoy is a nearest neighbor algorithm that allows for us to easily and efficiently store millions of vectors in an index that we can then retrieve very efficiently. This is achieved by using a computationally efficient algorithm that is written in C. To use annoy, we first need to know the dimensions of our vectors. We can use `.shape` and examine index 1 to get our vector dimensions. Note, these will change from model to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd751c7-31f0-4f33-acd4-753305b7ac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dim = embeddings.shape[1]\n",
    "vector_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdddeb6-4d24-44c0-bc6b-f2dcd8275eaa",
   "metadata": {},
   "source": [
    "As we can see, we have a dimension of 384. Now that we know that, we can create our index which we will populate with vectors. When creating an AnnoyIndex, we need to specify two things: the number of vectors and the way in which want to measure similarity. We have a few options here. We are using angular.\n",
    "\n",
    "```AnnoyIndex(f, metric) returns a new index that's read-write and stores vector of f dimensions. Metric can be \"angular\", \"euclidean\", \"manhattan\", \"hamming\", or \"dot\". - Annoy docs```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9373bf65-465d-4fa2-af2e-7f775c8fee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "annoy_index = AnnoyIndex(vector_dim, 'angular')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c99906-fbc3-444f-897e-760e401ac4f5",
   "metadata": {},
   "source": [
    "Now that we have created our index, it's time to populate it with data. When we do this, we need to use the `add_item()` method. This will take two arguments: the index number and the embedding itself. We can use enumerate() to create our i variable that will tick up by one each time we loop over our data. This is the equivalent of doing `i=i+1` inside our loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e57705d-223a-416e-8733-cb906e4536c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, embedding in enumerate(embeddings):\n",
    "    annoy_index.add_item(i, embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cdcb8c-e0e7-4b30-b4de-979f35720c2b",
   "metadata": {},
   "source": [
    "At this stage, our index has the data but it is not yet built. To build it, we can use the `build()` method. This will take 1 argument, the number of trees we want to use. In theory, the more trees, the better, but there's a point where you have diminished returns. Unless you are working with very complex data, 10 is usually a good starting number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39459f3-4944-4ec2-8d0d-9b7a829748ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "annoy_index.build(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5321fa1a-538b-4385-b213-414a66609c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"world war\"\n",
    "query_vector = model.encode(query)\n",
    "similar_item_ids = annoy_index.get_nns_by_vector(query_vector, 5)\n",
    "\n",
    "print(f\"Search results for query: '{query}'\")\n",
    "print(\"------------------------------\", \"\\n\")\n",
    "\n",
    "for idx, row in df.iloc[similar_item_ids].iterrows():\n",
    "    print(f\"Speech {df['year'][idx]} {df['name'][idx]}\\n {df['text'][idx][0:250]}\")\n",
    "    print(\"-------------\", \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7f4749-8b79-4ee4-93c9-7d8b28c541b5",
   "metadata": {},
   "source": [
    "### Chunking by sentence groups\n",
    "This is nice but it would be much more useful if we could find matches from the most relevant sentences within each document. It's hard to see from these results if a speech is particularly relevant to our search or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71986972-ad11-47e1-a0c2-eb95667a09e3",
   "metadata": {},
   "source": [
    "To chunk our data into sentences, we'll first need to separate the text into sentences. To do that, we will use spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca4bf85-afe8-4b4e-b786-a469feca030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aa06e7-f320-49be-9849-f3887243ad89",
   "metadata": {},
   "source": [
    "With this pipeline we can now chunk our data. In the cell below, we will be creating a new dataset from our original one while preserving the original index (document) index. We'll make chunks of the original data and also preserve the subindex for each chunk within the document. This means that for any given index in our chunked data, we have access to its position within the document to which it belongs and the corpus as a whole.\n",
    "\n",
    "This will take a little while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11155002-fbe5-4c14-8d72-668ad4b91105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to chunk text into groups of 3 sentences\n",
    "def chunk_text(text, chunk_size=3):\n",
    "    # Use spaCy's NLP model to process the text and split it into sentences\n",
    "    doc = nlp(text)\n",
    "    # Convert the spaCy sentence objects into a list of sentences\n",
    "    sentences = list(doc.sents)\n",
    "    chunks = []\n",
    "    # Loop over the sentences in steps of 'chunk_size' (default is 3)\n",
    "    for i in range(0, len(sentences), chunk_size):\n",
    "        # Extract a group of 'chunk_size' sentences\n",
    "        chunk = sentences[i:i+chunk_size]\n",
    "        # Join the sentences in the chunk into a single string and append to the 'chunks' list\n",
    "        chunks.append(\" \".join([sent.text for sent in chunk]))\n",
    "    return chunks\n",
    "\n",
    "# Create chunks\n",
    "chunked_data = []\n",
    "# tqtdm gives us a progess bar while we loop through our df\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Chunking texts\"):\n",
    "    # Apply the chunk_text function to the 'text' column of the current row \n",
    "    chunks = chunk_text(row['text'])\n",
    "    # Loop through each chunk and keep track of its index within the document\n",
    "    for chunk_idx, chunk in enumerate(chunks):\n",
    "        # Convert the row's data to a dict\n",
    "        chunk_data = row.to_dict()\n",
    "        # Replace the 'text' field with the current chunk\n",
    "        chunk_data['text'] = chunk\n",
    "        # Add an index to keep track of which document this chunk belongs to\n",
    "        chunk_data['document_index'] = idx\n",
    "        # Add an index to keep track of the chunk's position within the document\n",
    "        chunk_data['chunk_index'] = chunk_idx\n",
    "        chunked_data.append(chunk_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad10728-a5ca-4476-adb0-9c6fde47ed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new DataFrame with chunks\n",
    "chunked_df = pd.DataFrame(chunked_data)\n",
    "chunked_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f958d858-a378-4dac-a880-7b8dadddd6b6",
   "metadata": {},
   "source": [
    "Unfortunately, though, our embeddings are precisely the same for each chunk. This isn't good. We want new embeddings for each chunk. Let's go ahead and repeat the steps from earlier in the notebook. This will take slightly longer as we now have ~4 times as many documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c7db5e-f0ff-4446-a06d-8fac04a65680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the chunks\n",
    "chunks = chunked_df['text'].tolist()\n",
    "chunk_embeddings = model.encode(chunks, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44062d6b-f8e5-4cfa-8b42-b03966a1b3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings from the pickle file\n",
    "with open('chunk_embeddings.pkl', 'rb') as f:\n",
    "    chunk_embeddings = pickle.load(f)\n",
    "\n",
    "print(\"Embeddings loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504faf05-bae7-49e4-872e-dd3dc83db4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the original embeddings with the new ones\n",
    "chunked_df['embedding'] = list(chunk_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a92985-436b-48a3-bbf3-c1c709cfb239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat the same indexing steps as above to create a searchable index on the chunks\n",
    "annoy_index = AnnoyIndex(vector_dim, 'angular')\n",
    "# Add items to the index\n",
    "for i, embedding in enumerate(embeddings):\n",
    "    annoy_index.add_item(i, embedding)\n",
    "annoy_index.build(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066d68a4-1467-4b33-aeab-150038d571a5",
   "metadata": {},
   "source": [
    "Query the new index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95551bdc-5b92-4614-bb5d-cd77fe238e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"warfare\"\n",
    "query_vector = model.encode(query)\n",
    "similar_item_ids = annoy_index.get_nns_by_vector(query_vector, 5)\n",
    "\n",
    "print(f\"Search results for query: '{query}'\")\n",
    "print(\"------------------------------\", \"\\n\")\n",
    "for idx, row in chunked_df.iloc[similar_item_ids].iterrows():\n",
    "    print(f\"Speech {chunked_df['year'][idx]} {chunked_df['name'][idx]}\\n {chunked_df['text'][idx]}\")\n",
    "    print(\"-------------\", \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e85eb75-0051-4db3-9064-bf3425b6534d",
   "metadata": {},
   "source": [
    "### More information\n",
    "- [Transformers and LLMs](https://carpentries-incubator.github.io/python-text-analysis/10-finetuning-transformers/index.html)\n",
    "- [The Word2Vec algorithm](https://carpentries-incubator.github.io/python-text-analysis/08-wordEmbed_word2vec-algorithm/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
