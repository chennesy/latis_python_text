{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e1bcb47-dbd1-4d5b-ac37-ef8fa72db755",
   "metadata": {},
   "source": [
    "# Vectors & word embeddings \n",
    "A workshop by UMN LATIS and the Libraries. \n",
    "\n",
    "Portions of the [Constellate's TAP Institute Introduction to Vector Databases and Semantic Searching](https://github.com/wjbmattingly/tap-2024-vector-databases?ref=cms-prod.constellate.org), [the Carpentries' Text Analysis in Python](https://carpentries-incubator.github.io/python-text-analysis/), and [UC Berkeley D-Lab's Text Analysis](https://github.com/dlab-berkeley/Python-Text-Analysis/tree/main) lessons are re-used in this lesson. These lessons are licensed under [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/) as is the content presented below.  \n",
    "\n",
    "## What we'll cover in this session\n",
    "    - Representing words as numbers\n",
    "    - TF-IDF\n",
    "    - Word vectors\n",
    "    - What are they? \n",
    "    - How they work + Benefits\n",
    "    - Word Vectors with Spacy\n",
    "    - Interpreting results\n",
    "    - Word embeddings in 2d plots\n",
    "    - Calculating distance between pairs\n",
    "    - Semantic Search\n",
    "        - Document embedding\n",
    "    - Transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa34013-f237-44b5-8cf3-21640513bf2c",
   "metadata": {},
   "source": [
    "## Install required libraries\n",
    "If you're working from your own machine you can use pip install to make sure you have downloaded all of the Python packages you'll need to use today. \n",
    "\n",
    "If you're working on notebooks.latis.umn.edu, there's no need to install any of these, since they're included in the virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8c54b1-2499-41f2-a6e3-4bf82bc4d00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use !pip install to install libraries we'll use today\n",
    "# numpy 2 does not work with sentence-transformers\n",
    "#!pip install spacy scikit-learn pandas matplotlib 'numpy<2' \n",
    " \n",
    "# or use conda\n",
    "#!conda install spacy scikit-learn pandas numpy matplotlib 'numpy<2'\n",
    "\n",
    "# This command downloads the medium-sized English language model for spaCy.\n",
    "# It uses the Python module-running option to run spaCy's download command for the \"en_core_web_md\" model.\n",
    "#!python -m spacy download en_core_web_md "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78008016-7c30-410c-9896-afec9178ee5c",
   "metadata": {},
   "source": [
    "## Changing words into numbers\n",
    "- Computers require numerical representations of things like images and words to be able to store, process, and manipulate them. \n",
    "- For most text analysis methods that will allows us to work with text as data, then, it's critical to use structured formats that represent words as numbers.\n",
    "- At the base level computers already use encoding standards like Unicode to represent words as numbers, but Unicode represents specific characters in numerical form, not entire words. Since we're interested in working with the meanings of words and not characters, we'll use other methods to create numerical representations of our texts.\n",
    "- Rather than manually assigning words to specific numbers on our own, we can utilize existing vector frameworks to transform our texts to numerical formats that allow us to analyze meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f1b6e0-3225-4d1a-96e1-57e7cbb6e8d6",
   "metadata": {},
   "source": [
    "## The Bag-of-Words Representation\n",
    "\n",
    "The idea of bag-of-words (BoW), as the name suggests, is quite intuitive: we take a document and toss it in a bag. The action of \"throwing\" the document in a bag disregards relative position between words, so what is \"in the bag\" is essentially an unsorted set of words [(Jurafsky & Martin, 2024, p.62)](https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf). In return, we have a list of unique words and the frequency of each of them. \n",
    "\n",
    "For example, as shown in the following illustration, the word \"coffee\" appears twice. \n",
    "\n",
    "<img src='bow-illustration-1.png' alt=\"BoW-Part2\" width=\"600\">\n",
    "\n",
    "As you may have realized now, with a bag-of-words representation, we make heavy use of word frequency but not too much of word order. \n",
    "\n",
    "In the context of sentiment analysis, the sentiment of a tweet is conveyed more strongly by specific words. For example, if a tweet contains the word \"happy\", it likely conveys positive sentiment, but not always (e.g., \"not happy\" denotes the opposite sentiment). When these words come up more often, they'll probably more strongly convey the sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d7b915-fa9c-46f9-9f09-fb9cc0892df4",
   "metadata": {},
   "source": [
    "## Document Term Matrix\n",
    "\n",
    "Now let's implement the idea of bag-of-words. Before we go deep into that, let's step back for a moment. In practice, text analysis often involves handling many documents; from now on, we use the term **document** to indicate a piece of text that we perform analysis on. It could be a phrase, a sentence, a tweet, and etc, as long as it could be represented by a string of text, the length dosen't really matter. \n",
    "\n",
    "Imagine we have four documents (i.e., the four phrases shown above), we toss them all in the bag. Instead of a word-frequency list, we would expect a document-term matrix (DTM) in return. In a DTM, the word list is the **vocabulary** (V) that holds all unique words occur across the documents. For each **document** (D), we count the number of occurence of each word in the vocabulary, and then plug the number into the matrix. In other words, the DTM we will need to construct is a $D \\times V$ matrix, where each row corresponds to a document, and each column corresponds to a token (or \"term\").\n",
    "\n",
    "In the following example, the unique tokens in this set of documents, in alphabetical order, are in columns. For each document, we mark the occurence of each word showing up in the document. The numerical representation for each document is a row in the matrix. For example, \"the coffee roaster\" or the first document has numerical representation $[0, 1, 0, 0, 0, 1, 1, 0]$.\n",
    "\n",
    "Note that the left index column now displays these documents as texts, but typically we would just assign an index to each of them. \n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cccccccccccc}\n",
    " & \\text{americano} & \\text{coffee} & \\text{iced} & \\text{light} & \\text{roast} & \\text{roaster} & \\text{the} & \\text{time} \\\\\\hline\n",
    "\\text{the coffee roaster} &0 &1\t&0\t&0\t&0\t&1\t&1\t&0 \\\\ \n",
    "\\text{light roast} &0 &0\t&0\t&1\t&1\t&0\t&0\t&0 \\\\\n",
    "\\text{iced americano} &1 &0\t&1\t&0\t&0\t&0\t&0\t&0 \\\\\n",
    "\\text{coffee time} &0 &1\t&0\t&0\t&0\t&0\t&0\t&1 \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "To create a DTM, we will use `CountVectorizer` from the package `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53e0e416-6193-439b-8427-1bdc9e782d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f01fbe-d6f4-4483-93eb-48bd7d7d0514",
   "metadata": {},
   "source": [
    "Let's use our toy example to take a closer look.\n",
    "\n",
    "The first step is to initialize a CountVectorizer object. Within the round paratheses is the parameter setting we may choose to specify. Let's take a look at the documentation and see what options are available.\n",
    "\n",
    "For now we can just leave it blank for the default setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37f14d83-8813-41df-a319-34a80107b19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A toy example containing four documents (phrases)\n",
    "test = ['the coffee roaster',\n",
    "        'light roast',\n",
    "        'iced americano',\n",
    "        'coffee time']\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b77540a-6f46-4353-8543-ce4d0cbe00aa",
   "metadata": {},
   "source": [
    "The second step is to `fit` this `CountVectorizer` object to the data, which means creating a vocabulary of tokens from the set of documents. Thirdly, we `transform` our data according to the \"fitted\" `CountVectorizer` object, which means taking each of the document and transforming it into a DTM according to the vocabulary established by the \"fitting\" step.\n",
    "\n",
    "It may sound a bit complex but steps 2 and 3 can actually be done in one swoop using a `fit_transform` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff610d7b-f961-4454-a9f6-6af193879edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x8 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and transform to create DTM\n",
    "test_count = vectorizer.fit_transform(test)\n",
    "test_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5e673a-e7c7-4888-ae65-069eae648bce",
   "metadata": {},
   "source": [
    "Apparently the return is a \"sparse matrix\"â€”a matrix that contains a lot zeros. It actually makes sense. For each document we definitely have words that don't occur at all, which are counted zero in the DTM. This sparse matrix is stored in a \"Compressed Sparse Row\" format, which is a memory-saving format that is designed to deal with sparse matrix. \n",
    "\n",
    "Let's convert it to a dense matrix, where those zeros are probably organized, as in a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2134d735-a62b-4919-bbf3-82bb496a7b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 1, 0, 0, 0, 1, 1, 0],\n",
       "        [0, 0, 0, 1, 1, 0, 0, 0],\n",
       "        [1, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert DTM to a dense matrix \n",
    "test_count.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e0c4da-ed09-4367-ae3b-06022dde67ec",
   "metadata": {},
   "source": [
    "So this is our DTM! It is the same as shown above, but to make it more reader-friendly, let's convert it to a dataframe. The column names should be tokens in the vocabulary, which we can access with `get_feature_names_out()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d60b6ae-cfcb-4178-84ab-bcc2b5e46d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['americano', 'coffee', 'iced', 'light', 'roast', 'roaster', 'the',\n",
       "       'time'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the vocabulary\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e0e0c3b-429f-4cc6-affc-e8428dce67da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Create a DTM dataframe\n",
    "test_dtm = pd.DataFrame(data=test_count.todense(),\n",
    "                        columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d21f25f1-0731-483a-a3f7-48a1b9aa4185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>americano</th>\n",
       "      <th>coffee</th>\n",
       "      <th>iced</th>\n",
       "      <th>light</th>\n",
       "      <th>roast</th>\n",
       "      <th>roaster</th>\n",
       "      <th>the</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   americano  coffee  iced  light  roast  roaster  the  time\n",
       "0          0       1     0      0      0        1    1     0\n",
       "1          0       0     0      1      1        0    0     0\n",
       "2          1       0     1      0      0        0    0     0\n",
       "3          0       1     0      0      0        0    0     1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9666ba7a-1234-4050-bcdb-722e47e59f54",
   "metadata": {},
   "source": [
    "### TF-IDF: Term Frequency, Inverse Document Frequency\n",
    "\n",
    "- TF-IDF is a statistical measure using a matrix to evaluate how important a word is to a document or corpus. \n",
    "- In a TF-IDF matrix:\n",
    "    - each row represents a document\n",
    "    - each column represents a unique word\n",
    "    - each cell contains a score for that word in the document\n",
    " \n",
    "The score increases relative to the number of times that a word appears in a document, offset by its frequency in the entire corpus. So words that are more common across the entire corpus will have lower scores in a particular document, which helps us account for common terms. \n",
    "\n",
    "Let's use the spacy package to transform a collection of State of the Union addresses into a TF-IDF Document Term Matrix.\n",
    "\n",
    "#### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c44e0a7-5c52-47a9-9c3c-40eaf3957cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import glob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "# note that we installed numpy 1.* since 2 does not work with the sentence-transformers package we work with later in the lesson\n",
    "import numpy as np\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41553b10-b212-4939-95dd-d1ae2c32c8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>americano</th>\n",
       "      <th>coffee</th>\n",
       "      <th>iced</th>\n",
       "      <th>light</th>\n",
       "      <th>roast</th>\n",
       "      <th>roaster</th>\n",
       "      <th>the</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.486934</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.617614</td>\n",
       "      <td>0.617614</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.619130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.785288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   americano    coffee      iced     light     roast   roaster       the  \\\n",
       "0   0.000000  0.486934  0.000000  0.000000  0.000000  0.617614  0.617614   \n",
       "1   0.000000  0.000000  0.000000  0.707107  0.707107  0.000000  0.000000   \n",
       "2   0.707107  0.000000  0.707107  0.000000  0.000000  0.000000  0.000000   \n",
       "3   0.000000  0.619130  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "       time  \n",
       "0  0.000000  \n",
       "1  0.000000  \n",
       "2  0.000000  \n",
       "3  0.785288  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create three simple documents\n",
    "#document = ['feed the duck', 'feed the goose', 'duck duck goose']\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "tfidf_matrix = vectorizer.fit_transform(test)\n",
    "\n",
    "# Create a DataFrame for better readability\n",
    "df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b22a323a-1907-4d74-8f04-1200febcc835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the coffee roaster', 'light roast', 'iced americano', 'coffee time']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260fb9de-55a9-47d9-8801-d18491c22a82",
   "metadata": {},
   "source": [
    "- Each row is a document (e.g., row 0 is \"the coffee roaster\"). \n",
    "- Each column is a word in our list of documents. \n",
    "- Each cell has a score for the word that increases relative to the number of times that a word appears in a document, offset by its frequency in the entire corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c44374-53db-4986-ade5-a7775887efc6",
   "metadata": {},
   "source": [
    "You can type in the name of the class and Shift+Tab to get a pop up with the documentation for `TfidfVectorizer` to look into more options you could explore for creating TF-IDF matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "049b315e-e395-403b-a7b8-aca5396c8cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.feature_extraction.text.TfidfVectorizer"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440af74d-81cf-496f-9f7b-e6572b2eeeb3",
   "metadata": {},
   "source": [
    "#### Tokenize the SOTU corpus with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c78d8de8-a5b1-4000-aaf0-e8cc8acec230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all speech file paths\n",
    "sotu = glob.glob(\"../sotu_kaggle/*.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe068015-9c81-49f9-86da-c9ef05f67121",
   "metadata": {},
   "source": [
    "For those of you who made it to the Text as Data intro workshop, this will be some review. In short, we'll create a function to read in each speech as an item in a list called `processed_speeches`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1900a63-f1ed-48e4-b777-668ff27b68b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mr. speaker mr. president distinguished members congress honored guests fellow citizens today marks state union address constitutional duty old republic president washington began tradition reminding nation destiny self government preservation sacred'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_speeches(speeches_glob, n):\n",
    "    # Initialize a list to store preprocessed speeches\n",
    "    processed_speeches = []\n",
    "    \n",
    "    # Process each speech\n",
    "    for speech_path in speeches_glob[0:n]:\n",
    "        with open(speech_path, 'r') as file:\n",
    "            text = file.read()\n",
    "    \n",
    "            #tokenize each document using spacy\n",
    "            doc = nlp(text)\n",
    "            \n",
    "            # Filter tokens and join them into a single string\n",
    "            # ignore stop words, punctuation, spaces and digits\n",
    "            tokens = [token.text.lower() for token in doc if not (token.is_stop or token.is_punct or token.is_space or token.is_digit or token.like_num)]\n",
    "    \n",
    "            # Re-join the list of tokens into a single string\n",
    "            processed_text = ' '.join(tokens)\n",
    "    \n",
    "            # Append each cleaned and tokenized document to our list\n",
    "            processed_speeches.append(processed_text)\n",
    "            \n",
    "    return processed_speeches\n",
    "\n",
    "# let's just read in the first 5 speeches to start off\n",
    "processed_speeches = tokenize_speeches(sotu, 5)\n",
    "\n",
    "# look at the first 250 characters of the first speech, tokenized:\n",
    "processed_speeches[0][0:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5689c4ad-83b6-49f6-a854-899ed0d14eb1",
   "metadata": {},
   "source": [
    "### Convert to a TF-IDF Vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8604db8-aea6-4bdc-9a02-67ebac2687e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the same vectorizer settings from before to\n",
    "# Fit and transform the first 5 preprocessed speeches to a TF-IDF matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(processed_speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8c992c2-e6f8-4790-bf63-d039ce856198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5067)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the matrix shape: 5 rows, by 5,067 columns\n",
    "# each row is a document, each column is a token\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8254b6-a925-4bf7-920f-24607adf954b",
   "metadata": {},
   "source": [
    "#### Use Pandas to view the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43b4d3c2-6967-4037-9f82-b7b077deb9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>13</th>\n",
       "      <th>19o8</th>\n",
       "      <th>22d</th>\n",
       "      <th>2d</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abiding</th>\n",
       "      <th>ability</th>\n",
       "      <th>...</th>\n",
       "      <th>years</th>\n",
       "      <th>yes</th>\n",
       "      <th>yield</th>\n",
       "      <th>yielding</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>zealous</th>\n",
       "      <th>zest</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.026757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076498</td>\n",
       "      <td>0.026757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007537</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008552</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025255</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017915</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.020371</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016435</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034430</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020371</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004182</td>\n",
       "      <td>0.004182</td>\n",
       "      <td>0.004182</td>\n",
       "      <td>0.003374</td>\n",
       "      <td>0.004182</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055801</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003374</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008365</td>\n",
       "      <td>0.004713</td>\n",
       "      <td>0.004182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033459</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030349</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 5067 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         13      19o8       22d        2d   abandon  abandoned  abandoning  \\\n",
       "0  0.026757  0.000000  0.000000  0.000000  0.000000   0.000000      0.0000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.008552   0.000000      0.0106   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000      0.0000   \n",
       "3  0.000000  0.004182  0.004182  0.004182  0.003374   0.004182      0.0000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000      0.0000   \n",
       "\n",
       "   abandonment   abiding   ability  ...     years       yes     yield  \\\n",
       "0       0.0000  0.000000  0.000000  ...  0.076498  0.026757  0.000000   \n",
       "1       0.0106  0.000000  0.017104  ...  0.025255  0.000000  0.000000   \n",
       "2       0.0000  0.020371  0.000000  ...  0.038828  0.000000  0.016435   \n",
       "3       0.0000  0.000000  0.013497  ...  0.055801  0.000000  0.003374   \n",
       "4       0.0000  0.000000  0.000000  ...  0.014461  0.000000  0.000000   \n",
       "\n",
       "   yielding      york     young   zealous      zest      zone     zones  \n",
       "0  0.000000  0.000000  0.007537  0.000000  0.000000  0.000000  0.026757  \n",
       "1  0.000000  0.000000  0.017915  0.000000  0.000000  0.000000  0.000000  \n",
       "2  0.000000  0.000000  0.034430  0.000000  0.020371  0.000000  0.000000  \n",
       "3  0.000000  0.008365  0.004713  0.004182  0.000000  0.033459  0.000000  \n",
       "4  0.030349  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[5 rows x 5067 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame for better readability\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the DataFrame\n",
    "df_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92645368-cad4-43af-ac18-8876802bb8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 top words:\n",
      " ['programs', 'federal', 'tax', 'government', 'program']\n",
      "\n",
      "Document 2 top words:\n",
      " ['congress', 'law', 'tariff', 'country', 'cable']\n",
      "\n",
      "Document 3 top words:\n",
      " ['great', 'people', 'tasks', 'means', 'states']\n",
      "\n",
      "Document 4 top words:\n",
      " ['government', 'statute', 'law', 'department', 'states']\n",
      "\n",
      "Document 5 top words:\n",
      " ['enemy', 'war', 'lake', 'american', 'great']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of top words to extract for each document\n",
    "top_n = 5\n",
    "\n",
    "# Find the top n words for each document\n",
    "top_words_per_document = []\n",
    "for index, row in df_tfidf.iterrows():\n",
    "    top_words = row.sort_values(ascending=False).head(top_n).index.tolist()\n",
    "    top_words_per_document.append(top_words)\n",
    "\n",
    "# Print the top words for each document\n",
    "for doc_index, words in enumerate(top_words_per_document):\n",
    "    print(f\"Document {doc_index + 1} top words:\\n {words}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59715e1c-233b-4a40-abca-49c13dd9b45d",
   "metadata": {},
   "source": [
    "In TF-IDF the words are assigned numbers based on their importance to each document and the corpus, but the numbers don't actually tell us anything about a word's meaning or its relationship to other words. Words vectors give us a way to understand more about how words in a corpus relate to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3c9a46-b75a-4f62-b9b6-81ed001f15ca",
   "metadata": {},
   "source": [
    "## Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c03448-250a-4433-a52a-bbe6474c22c8",
   "metadata": {},
   "source": [
    "When people refer to word vectors, or word embeddings, they're talking about a way of representing each word as a numerical vector in a high-dimensional space, typically from 50 to 300 dimensions. \n",
    "\n",
    "Key properties of word vectors:\n",
    "\n",
    "1. Dimensionality: In TF-IDF there as many dimensions as there are words in the vocabulary. Word vectors typically have fewer dimensions (e.g., 100 or 300).\n",
    "\n",
    "2. Density: Unlike TF-IDF where a vector consists mostly of 0s, word vectors are dense, meaning most elements are non-zero.\n",
    "\n",
    "3. Learned from data: Word vectors are typically learned from large text corpora using machine learning techniques. They capture semantic and syntactic relationships between words based on their usage patterns in the text. The relationships between the words come entirely from the text corpora they were trained on - not from your documents!\n",
    "\n",
    "4. Semantic relationships: Similar words have similar vectors. For example, the vectors for \"king\" and \"queen\" might be close to each other.\n",
    "\n",
    "5. Arithmetic operations: Word vectors often exhibit interesting arithmetic properties. A classic example is: vector(\"king\") - vector(\"man\") + vector(\"woman\") â‰ˆ vector(\"queen\").\n",
    "\n",
    "## How Word Vectors Work\n",
    "\n",
    "Word vectors work on the principle of distributional semantics, which states that words that occur in similar contexts tend to have similar meanings. Machine learning models analyze large amounts of text data to learn these representations.\n",
    "\n",
    "There are different popular models out there, such as Word2Vec, developed by Google. We'll stick with spaCy, and use the vector model that comes with \"en_core_web_md\".\n",
    "\n",
    "For more information about how these models produce meaningful word embeddings, and how the models are trained, see the Carpentries' lesson on [the Word2Vec algorithm](https://carpentries-incubator.github.io/python-text-analysis/08-wordEmbed_word2vec-algorithm/index.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a811c97-2bfc-48a5-9462-a169a5831436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert a word to an nlp() object \n",
    "token = nlp(\"freedom\")\n",
    "# save the nlp vector attribute for \"freedom\"\n",
    "vector = token.vector\n",
    "\n",
    "#the vector has 1 row and 300 columns, for each dimension\n",
    "vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c4bb7b-a2da-4e31-937c-e2e6da14f0ab",
   "metadata": {},
   "source": [
    "The vector itself isn't very interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35d73b94-eb5d-446b-a4ea-8f58df088a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.62752</td>\n",
       "      <td>0.096647</td>\n",
       "      <td>0.058525</td>\n",
       "      <td>-0.041364</td>\n",
       "      <td>0.008209</td>\n",
       "      <td>0.1223</td>\n",
       "      <td>-0.37158</td>\n",
       "      <td>1.1244</td>\n",
       "      <td>-0.10964</td>\n",
       "      <td>2.6778</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.56081</td>\n",
       "      <td>0.43747</td>\n",
       "      <td>-0.34792</td>\n",
       "      <td>-0.058516</td>\n",
       "      <td>-0.19365</td>\n",
       "      <td>0.32982</td>\n",
       "      <td>0.28227</td>\n",
       "      <td>0.10554</td>\n",
       "      <td>-0.75263</td>\n",
       "      <td>-0.24929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4       5        6       7    \\\n",
       "0 -0.62752  0.096647  0.058525 -0.041364  0.008209  0.1223 -0.37158  1.1244   \n",
       "\n",
       "       8       9    ...      290      291      292       293      294  \\\n",
       "0 -0.10964  2.6778  ... -0.56081  0.43747 -0.34792 -0.058516 -0.19365   \n",
       "\n",
       "       295      296      297      298      299  \n",
       "0  0.32982  0.28227  0.10554 -0.75263 -0.24929  \n",
       "\n",
       "[1 rows x 300 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we'll reshape the numpy array\n",
    "# -1 infers how many rows are needed and will resolve to 1 since we have a 1d vector\n",
    "df = pd.DataFrame(vector.reshape(-1, len(vector)))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59366d37-4316-4186-846d-27069630d546",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "But we can compare different word vectors to measure their similarity, in terms of the mathematical differences between the vectors. We'll first use spaCy's similarity method.\n",
    "\n",
    "Itâ€™s important to note that the effectiveness of the similarity measurement depends on the quality of the model's embeddings and whether the model has been trained on a relevant corpus for the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cd257fe-2298-49bc-a2e3-d09ca25bd995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_words(word1, word2):\n",
    "    similarity = nlp(word1).similarity(nlp(word2))\n",
    "    print(f\"Similarity between '{word1}' and '{word2}': {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "547d700e-e27e-4a4a-b994-3e5675eff644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'king' and 'president': 0.3532\n",
      "Similarity between 'king' and 'pauper': 0.0144\n",
      "Similarity between 'president' and 'pauper': 0.1030\n"
     ]
    }
   ],
   "source": [
    "compare_words('king', 'president')\n",
    "compare_words('king', 'pauper')\n",
    "compare_words('president', 'pauper')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272de361-230b-4f36-945c-8ca59c94bce6",
   "metadata": {},
   "source": [
    "What does this tell us? It means that semantically and syntactically, king and president are used in more similar ways than king and pauper This is because the embeddings produced are the result of an embedding model that saw a lot of English texts and in those texts, kings and presidents are represented in similar ways, as you would expect. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bdd03b-7320-4acd-926d-bad76552cd00",
   "metadata": {},
   "source": [
    "### Word embeddings on a 2d plot with PCA\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space while retaining as much of the variance (information) as possible.\n",
    "\n",
    "Our original word vectors are in a high-dimensional space (e.g., 300 dimensions). But visualizing or analyzing high-dimensional data can be challenging. PCA helps reduce these 300 dimensions to a lower number (in this case, 2), making it easier to plot and interpret the relationships between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "380e19c4-1935-4543-977e-f51c748ed062",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['nurse', 'doctor', 'farmer', 'athlete', 'librarian', 'teacher']\n",
    "\n",
    "# create a vector for each word in a list called vectors\n",
    "vectors = [nlp(word).vector for word in words]\n",
    "\n",
    "# Reduce the dimensions of the vectors using Principal component analysis (PCA)\n",
    "# We are choosing 2 compenents so we can plot them in a 2D space\n",
    "pca = PCA(n_components = 2)\n",
    "vectors_2d = pca.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb30dcc8-664c-49eb-b1ea-8d41bc936c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+cAAAK7CAYAAACQxf6EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAACCPklEQVR4nOzdd3gU5eL28XuTbHojDQIJhN4REFBEKVIFFOyIDRFsKCIqqEcFfFFQOYoejxzAXhAUFTuCGJqAdBCINAmhhRZIL5vdef/Ij9U1AbOQZUj2+7muXGan7T274/HcmZlnLIZhGAIAAAAAAKbxMTsAAAAAAADejnIOAAAAAIDJKOcAAAAAAJiMcg4AAAAAgMko5wAAAAAAmIxyDgAAAACAySjnAAAAAACYjHIOAAAAAIDJKOcAAAAAAJiMcg4AKLe5c+fKYrFozpw5peZddNFFslgs+vHHH0vNq1+/vtq2bevRbIsXL5bFYtHixYvPuNx7770ni8Vy2p9/Wt8dXbt2VYsWLSpse2eSlJSkIUOG/ONyp/Y/NTXVOa1r167q2rWrx7KdK4vFovHjx1fY9r744gvdcsstatCggYKCgpSUlKRbb71VO3fuLLVsUlKS89jw8fFRRESEmjZtqjvuuEMLFiwo93vabDZNnz5d7du3V1RUlIKDg1WnTh0NGDBAX375ZYXtGwCg8vIzOwAAoPLo2rWrLBaLkpOTdfPNNzunZ2Rk6LffflNISIiSk5PVu3dv57z9+/frjz/+0OjRo82IfFrvvvuumjRpUmp6s2bNTEhjrjfffNPsCGe0cuVKJSQkVNj2XnzxRdWoUUP/+te/VK9ePe3bt08vvPCC2rZtq1WrVql58+Yuy3fq1ElTpkyRJOXk5Gj79u2aPXu2evfureuvv16ffPKJrFbrGd/z9ttv1xdffKFRo0ZpwoQJCggI0B9//KH58+frxx9/1LXXXlth+wcAqJwo5wCAcouJiVGLFi1KnV1esmSJ/Pz8dPfddys5Odll3qnX3bp1O+f3z8/PV1BQ0DlvR5JatGihdu3aVci2KrsL/Q8Sl156aYVu75tvvlFcXJzLtCuvvFJJSUl69dVX9dZbb7nMi4yMdMnQo0cPjRgxQuPHj9eECRP09NNP68UXXzzt++3Zs0dz5szRs88+qwkTJjind+/eXcOHD5fD4aigPQMAVGZc1g4AcEu3bt20fft2HTp0yDlt8eLFat++vfr27at169YpOzvbZZ6vr6+uuOIKSVJBQYGefPJJ1a1bV/7+/qpVq5ZGjBihkydPurxPUlKS+vfvry+++EJt2rRRYGCgs9j8/vvv6tOnj4KDgxUTE6P77rvP5T0risVi0YMPPqh3331XjRs3VlBQkNq1a6dVq1bJMAy9/PLLqlu3rkJDQ3XllVdq165dZW5n2bJluvTSSxUUFKRatWrpmWeekd1ud1mmqKhIEydOVJMmTRQQEKDY2FjdddddOnr0qMtyNptNY8aMUY0aNRQcHKzLL79cq1evLvN9V61apU6dOikwMFA1a9bUk08+KZvNVmq5v1/WnpqaKovFoilTpuiVV15x7mPHjh21atWqUuvPnDlTjRo1UkBAgJo1a6ZZs2ZpyJAhSkpKcllu2rRpuuiiixQaGqqwsDA1adJETz31VJnZ/+rvl7WfujQ/OTlZ999/v2JiYhQdHa3rrrtOBw8e/Mft/b2YS1LNmjWVkJCgffv2/eP6p4wfP17NmzfXG2+8oYKCgtMud/z4cUlSfHx8mfN9fP78v2Onbs/46KOPNHr0aNWoUUNBQUHq0qWLNmzY4LLe2rVrNWjQICUlJTkvz7/lllu0d+/eUu9x4MAB3XPPPUpMTJS/v79q1qypG264QYcPH3Yuk5WVpccee8zl381Ro0YpNze33J8JAODsceYcAOCWbt266fXXX9fixYt1yy23SCo5O96/f3916tRJFotFy5YtU9++fZ3z2rZtq4iICBmGoYEDB2rRokV68skndcUVV2jz5s0aN26cVq5cqZUrVyogIMD5XuvXr1dKSoqefvpp1a1bVyEhITp8+LC6dOkiq9WqN998U9WrV9fHH3+sBx980K39sNvtKi4udplmsVjk6+vrMu3bb7/Vhg0bNHnyZFksFo0dO1b9+vXTnXfeqT/++ENvvPGGMjMzNXr0aF1//fXauHGjLBaLc/309HQNGjRITzzxhJ577jl99913mjhxok6cOKE33nhDkuRwODRgwAAtW7ZMY8aM0WWXXaa9e/dq3Lhx6tq1q9auXeu8YmD48OH64IMP9Nhjj6lnz57asmWLrrvuulJ/nNi2bZu6d++upKQkvffeewoODtabb76pWbNmlfsz+u9//6smTZpo6tSpkqRnnnlGffv21Z49exQRESFJmjFjhu69915df/31evXVV5WZmakJEyaosLDQZVuzZ8/WAw88oIceekhTpkyRj4+Pdu3apW3btpU7z98NGzZM/fr106xZs7Rv3z49/vjjuu222/Tzzz+7va0//vhDe/fu1cCBA91a7+qrr9bkyZO1du1aXX755WUu07RpU0VGRmrChAny8fFRr169Sv3h4u+eeuoptW3bVm+99ZYyMzM1fvx4de3aVRs2bFC9evUklfwRpXHjxho0aJCioqJ06NAhTZs2Te3bt9e2bdsUExMjqaSYt2/fXjabTU899ZRatWql48eP68cff9SJEydUvXp15eXlqUuXLtq/f79zma1bt+rZZ5/Vb7/9pp9++snluAYAeIABAIAbMjIyDB8fH+Oee+4xDMMwjh07ZlgsFmP+/PmGYRhGhw4djMcee8wwDMNIS0szJBljxowxDMMw5s+fb0gyXnrpJZdtzpkzx5BkzJgxwzmtTp06hq+vr7F9+3aXZceOHWtYLBZj48aNLtN79uxpSDKSk5PPmP/dd981JJX54+vr67KsJKNGjRpGTk6Oc9q8efMMSUbr1q0Nh8PhnD516lRDkrF582bntC5duhiSjK+++splu8OHDzd8fHyMvXv3GoZhGJ988okhyfj8889dlluzZo0hyXjzzTcNwzCMlJQUQ5LxyCOPuCz38ccfG5KMO++80znt5ptvNoKCgoz09HTntOLiYqNJkyaGJGPPnj0uObt06eJ8vWfPHkOS0bJlS6O4uNg5ffXq1YYk45NPPjEMwzDsdrtRo0YN45JLLnHJs3fvXsNqtRp16tRxTnvwwQeNyMhI42xIMsaNG+d8feo7fOCBB1yWe+mllwxJxqFDh9zavs1mM7p27WqEh4cbaWlpLvPq1Klj9OvX77TrTps2zZBkzJkz54zv8d133xkxMTHOYy06Otq48cYbja+//tplueTkZEOS0bZtW5fjKzU11bBarcawYcNO+x7FxcVGTk6OERISYrz22mvO6UOHDjWsVquxbdu20647adIkw8fHx1izZo3L9Llz5xqSjO+///6M+wcAOHdc1g4AcEu1atV00UUXOe87X7JkiXx9fdWpUydJUpcuXZz3mf/9fvNTZzT/Pqr4jTfeqJCQEC1atMhleqtWrdSoUSOXacnJyWrevLkuuugil+mDBw92az8++OADrVmzxuXn119/LbVct27dFBIS4nzdtGlTSdJVV13lcibx1PS/X1IcFhama665plRWh8OhpUuXSio5Ox8ZGamrr75axcXFzp/WrVurRo0azs/61Od56623umzvpptukp+f68VwycnJ6t69u6pXr+6c5uvr6zKQ3z/p16+fy5UErVq1ctnH7du3Kz09XTfddJPLerVr13YeD6d06NBBJ0+e1C233KKvvvpKx44dK3eO0/n75/r3fOVhGIbuvvtuLVu2TB988IESExPdymAYRrmW69u3r9LS0vTll1/qscceU/PmzTVv3jxdc801ZV71MXjwYJfjq06dOrrssstcxnTIycnR2LFj1aBBA/n5+cnPz0+hoaHKzc1VSkqKc7kffvhB3bp1cx6jZfn222/VokULtW7d2uUY7N27d4U/xQAAUDYuawcAuK1bt2565ZVXdPDgQSUnJ+viiy9WaGiopJJy/u9//1uZmZlKTk6Wn5+f83Lf48ePy8/PT7GxsS7bs1gsqlGjhvPe3FPKukf3+PHjqlu3bqnpNWrUcGsfmjZtWq4B4aKiolxe+/v7n3H63+89/ms5/nvWU/t7+PBhnTx50rmNvztVZE8t//d99fPzU3R0tMu048ePl/mZuPM5/X2bp245yM/Pd8lT1j5Wr15de/bscb6+/fbbVVxcrJkzZ+r666+Xw+FQ+/btNXHiRPXs2bPcmdzJ908Mw9CwYcP00Ucf6f3339eAAQPcznDqDwE1a9b8x2WDgoI0cOBA56XzaWlpuuqqq/Tf//5X999/v8so8af77jZt2uR8PXjwYC1atEjPPPOM2rdvr/DwcFksFvXt29flMzh69Og/jnZ/+PBh7dq167SjzlfEH1MAAGdGOQcAuO1UOV+8eLEWL17svL9ckrOIL1261DlQ3KniHh0dreLiYh09etSloBuGofT0dLVv397lfcq6xzU6Olrp6emlppc17ULw1wG3TjmV9VS5PDWg2fz588vcRlhYmMvy6enpqlWrlnN+cXFxqT9snI/P6VSeM+3jX91111266667lJubq6VLl2rcuHHq37+/duzYoTp16lRYrvI4Vczfffddvf3227rtttvOahvffPONQkJCzmrk/9q1a+uee+7RqFGjtHXrVpdyfrrv7tRnnpmZqW+//Vbjxo3TE0884VymsLBQGRkZLuvFxsZq//79Z8wSExOjoKAgvfPOO6edDwDwLC5rBwC4rXPnzvL19dXcuXO1detWl5G+IyIi1Lp1a73//vtKTU11eYRa9+7dJUkfffSRy/Y+//xz5ebmOuefSbdu3bR161aXM4iS3Bro7HzKzs7W119/7TJt1qxZ8vHxUefOnSVJ/fv31/Hjx2W329WuXbtSP40bN5Yk5+f88ccfu2zv008/LTW4Xbdu3bRo0SKX4my32zVnzpwK27fGjRurRo0a+vTTT12mp6WlacWKFaddLyQkRFdddZX+9a9/qaioSFu3bq2wTOVhGIaGDx+ud999V9OnT9ddd911VtuZMGGCtm3bpocffliBgYGnXS47O1s5OTllzjt1+fnfz7x/8sknLpfM7927VytWrHAeAxaLRYZhuAygKElvvfVWqScBXHXVVUpOTtb27dtPm7F///7avXu3oqOjyzwG/2kAOwDAuePMOQDAbeHh4Wrbtq3mzZsnHx+fUvcXd+nSxTnC91/Lec+ePdW7d2+NHTtWWVlZ6tSpk3O09jZt2uj222//x/ceNWqU3nnnHfXr108TJ050jtb++++/u7UPW7ZsKVVoJal+/fqlLrs/F9HR0br//vuVlpamRo0a6fvvv9fMmTN1//33q3bt2pKkQYMG6eOPP1bfvn318MMPq0OHDrJardq/f7+Sk5M1YMAAXXvttWratKluu+02TZ06VVarVT169NCWLVs0ZcoUhYeHu7zv008/ra+//lpXXnmlnn32WQUHB+u///1vhT4Wy8fHRxMmTNC9996rG264QUOHDtXJkyc1YcIExcfHuzwibPjw4QoKClKnTp0UHx+v9PR0TZo0SREREaWumPC0kSNH6u2339bQoUPVsmVLl8fDBQQEqE2bNi7Lnzx50rlMbm6utm/frtmzZ2vZsmW66aabXJ5dXpbt27erd+/eGjRokLp06aL4+HidOHFC3333nWbMmKGuXbvqsssuc1nnyJEjuvbaazV8+HBlZmZq3LhxCgwM1JNPPimp5N/Bzp076+WXX1ZMTIySkpK0ZMkSvf3224qMjHTZ1nPPPacffvhBnTt31lNPPaWWLVvq5MmTmj9/vkaPHq0mTZpo1KhR+vzzz9W5c2c98sgjatWqlRwOh9LS0rRgwQI9+uijuuSSS872IwcAlIdpQ9EBACq1MWPGGJKMdu3alZp3akRzf39/Izc312Vefn6+MXbsWKNOnTqG1Wo14uPjjfvvv984ceKEy3JnGiV727ZtRs+ePY3AwEAjKirKuPvuu42vvvrqnEdrl2TMnDnTuawkY8SIES7rnxrJ/OWXX3aZfmqU7c8++8w5rUuXLkbz5s2NxYsXG+3atTMCAgKM+Ph446mnnjJsNpvL+jabzZgyZYpx0UUXGYGBgUZoaKjRpEkT49577zV27tzpXK6wsNB49NFHjbi4OCMwMNC49NJLjZUrVxp16tRxGa3dMAzjl19+MS699FIjICDAqFGjhvH4448bM2bMKPdo7X/fx1OfyV9HTjcMw5gxY4bRoEEDw9/f32jUqJHxzjvvGAMGDDDatGnjXOb99983unXrZlSvXt3w9/c3atasadx0000uo9ufzt/f89R3+PeRxU99B/90DNSpU+e03/9fR5j/+7IWi8UIDQ01GjdubNx+++3Gjz/++I/ZDcMwTpw4YUycONG48sorjVq1ahn+/v5GSEiI0bp1a2PixIlGXl5eqX348MMPjZEjRxqxsbFGQECAccUVVxhr16512e7+/fuN66+/3qhWrZoRFhZm9OnTx9iyZUuZx8K+ffuMoUOHGjVq1DCsVqvz8z98+LBzmZycHOPpp582GjdubPj7+xsRERFGy5YtjUceecRl1H8AgGdYDKOcw4wCAACUw8mTJ9WoUSMNHDhQM2bMMDtOpbJ48WJ169ZNn332mW644Qaz4wAAziMuawcAAGctPT1dzz//vLp166bo6Gjt3btXr776qrKzs/Xwww+bHQ8AgEqDcg4AAM5aQECAUlNT9cADDygjI0PBwcG69NJL9b///c9l9HEAAHBmXNYOAAAAAIDJeJQaAAAAAAAmo5wDAAAAAGAyyjkAAAAAACbzqgHhHA6HDh48qLCwMFksFrPjAAAAAACqOMMwlJ2drZo1a8rH5/Tnx72qnB88eFCJiYlmxwAAAAAAeJl9+/YpISHhtPO9qpyHhYVJKvlQwsPDTU4Dd9hsNi1YsEC9evWS1Wo1Ow5wXnDcwxtx3MNbcezDG3nLcZ+VlaXExERnHz0dryrnpy5lDw8Pp5xXMjabTcHBwQoPD6/S/+ICf8VxD2/EcQ9vxbEPb+Rtx/0/3VrtVeUcZ6egoEDbt29XWlqa8vPzZbfbz3uG4uJibdy4UVlZWfLzO/fD1sfHR0FBQUpISFCTJk0UHBxcASkBAAAA4OxQznFahYWF+uqrL7V9+0rZ7emqXr1YoaFSBXRjt/n4OFSzZrp8fP5QRTxkoKhIOnlS2rDBT99+G6cGDdpr4MAbKOkAAAAATEE5R5kKCwv14Yfv6NixherZM0rNmtVTeHiAaXnsdrt27gxQw4YN5evrW2HbzckpUkrKUS1ZMk/vv5+hO++8h4IOAAAA4LzjOeco05dfztWxYwt1xx11demlCaYWc08KDfVX+/a1dOed9ZWbu1Sffvqx2ZEAAAAAeCHKOUrJy8vTjh2r1L17tGrWPPOIglVFbGyIeveurtTUtTp58qTZcQAAAAB4Gco5Svn9999lGEfUtGms2VHOq8aNY+Tnd1xbt241OwoAAAAAL0M5Ryl79+5VzZrFCg31NzvKeeXv76ukJEN79+4xOwoAAAAAL0M5Ryl5ebkKDT3zM/iqqtBQq/Lzs82OAQAAAMDLUM5Rit1eJD8/7yznfn4+stuLzI4BAAAAwMtQznFeDRkyTwMHzj7jMklJUzV16qrzlAgAAAAAzEc5h0ekpp6UxTJBGzeme/y9xo9frNat/+fx9wEAAAAAT6GcAwAAAABgMso5ztr8+bt0+eXvKDJysqKjX1L//rO0e3eGJKlu3dckSW3aTJfFMkFdu77nsu6UKSsUH/9vRUe/pBEjvpPNZj/t+2RmFui++75Tp07zVK3aS7ryyve1aVPJGfn33tuoCROWaNOmw7JYJshimaD33tvoXO+ee75RXNzLCg+f5LIeAAAAAFxI/MwOgMorN7dIo0d3VMuWccrNtenZZ5N17bVztHHjfVq9epg6dHhLP/10u5o3j5O/v69zveTkVMXHhyo5+U7t2pWhm2+eq9ata2j48ItLvYdhGOrXb5aqVQvU9Omd1aJFQ7311gZ17/6Bdux4SDff3FxbthzR/Pm79NNPd0iSIiICnOtFRQXp++9vVUREgKZPX+dcLyoq6Lx9TgAAAADwTyjnOGvXX9/M5fXbb1+juLgp2rbtqGJjQyRJ0dHBqlEj1GW5atUC9cYbfeXr66MmTWLUr19DLVq0p8xynpycqt9+O6JDhx5RWtoeNWwYpSlTemnevN81d+423XPPxQoN9Zefn4/L+/z88x799tsRHTnymAICSg7zv68HAAAAABcKyjnO2u7dGXrmmWStWrVfx47lyeEwJElpaZlq1iz2tOs1bx4nX98/76iIjw/Vb78dKbXcsWPHlJy8XTk5RYqL+7cMw5DFUvKIt/z8Yucl9GVZt+6gcnKKFB39ksv0f1oPAAAAAMxAOcdZu/rqT5SYGKGZM69WzZphcjgMtWgxTUVFp79/XJKsVtehDiwWi7PY/9W6deu0cuWvioz00w8/3KQTJ04oKSlJvr4ll8gHBVnkcDjKfA+Hw1B8fKgWLx5Sal5kZGA59xAAAAAAzg/KOc7K8eN5Skk5punT++uKK+pIkpYvT3POP3WPud1ednkuj549e2rPHouSk1dq9uxZat++oTp1aqrQ0JLL1z/77DPl5+fLak2S3e5a7tu2jVd6eo78/HyUlBR51hkAAAAA4HygnOOsVKsWpOjoIM2YsV7x8WFKS8vUE0/85JwfFxeioCA/zZ+/SwkJ4QoM9FNEhHtnrH18fHTvvT310Uf79PXXJ3T8+C5t3Pgf1at3kfbu9VeHDknauvV7+fiEac+eE9q4MV0JCeEKC/NXjx711LFjogYOnK0XX+yhxo1jdPBgtr7/fqcGDmyidu1qVvRHAgAAAABnjUep4az4+Fg0e/YNWrfuoFq0eFOPPPKjXn65p3O+n5+PXn/9Kk2fvk41a76iAQNmn9X7WCwW/fDDbbrqqmb64Qd/vfKKTWPHrtGCBasVGmrokks6yOHYom7daqtbt/cVG/uyPvlkiywWi77/frA6d66joUO/VqNG/9GgQXOVmnpS1auHVNTHAAAAAAAVgjPnOGs9etTTtm0jXKYZxjjn78OGtdWwYW1d5r/33sBS25k6tY/L69TUUS6vw8ICNHVqb40YUU8NGzZURkaGFi5cqOXLf1DNmvGKiAjV1Vfnad68x+Tr6yvDkA4flgoKAvSvf12l1167Sv83jhwAAAAAXJAo56hUbDabMjIy1KFDB9WvX19r165VZmaWMjOztGDBArVocZVWr5aOHpXsdsnXV4qNlTp0kBITy/suNHkAAAAA5xflHKX4+QXIZjv7gdw8affu3frss7llzvv119Xav7+HCgutCg2VrFbJZpMOHZIWLJB69frngm6z2eXn5++B5AAAAABwepRzlBISEqpDh8xOUbYmTZpo7NgxKioqks1mc/4zP79Ay5cXqKDAqqioP5f395eioqSMDGn1aikhQWe8xD0rq1jBwRGe3xEAAAAA+AvKOUqpV6+eNmywKjOzwO0R1s+HoKAgBQUFuUw7fFgqLpb+7ylrpYSGllzqfuSIVL162cvk59u0d6+P+vSpX8GJAQAAAODMGK0dpTRq1Eh+fvHasuWI2VHKraCg5B5zq7Xs+VZryfyCgtNvY9u2ozKMWDVr1swzIQEAAADgNCjnKCUgIEAtW3ZWcnK2du3KMDtOuQQGlgz+ZrOVPd9mK5kfeJoLAdLSMvXjj8fVuPFlCj3d6XcAAAAA8BAua0eZ+vW7Wnl5uZo9+yt16HBCzZrFqlatMFku0GeSxcWVjMp+6JBc7jk/JSdHio8vWe4UwzCUnp6jrVuPavXqPNWseZWuu+6m8xcaAAAAAP4P5Rxl8vPz0403DtJPP8Vo06YVWrEiVUFBexUa6iM/E44ah8OhQ4cOKT7eJh+fsi/4OHZM2rxZKir680z6qUvZ/f2lVq2kGTNKli0uNpSXZyg3N1hBQYlq1eoy9erVS/7+jNQOAAAA4PyjnOO0/Pz81KdPH/Xq1Ut79+5VWlqa8vPzZbfbz3uW4uJiHTy4VjVrtpPfaf46kJAgBQRI33wj7dtXcim71SrVri317y81bfrnsr6+vgoMDFRiYqKSkpLk6+t7nvYEAAAAAEqjnOMf+fj4qG7duqpbt65pGWz/dzN53759ZT3dqG+S+vWTRo+Wtm6VTpyQqlWTmjeXTnOyHQAAAAAuCJRzVDk+PlLLlmanAAAAAIDy43wiAAAAAAAmo5wDAAAAAGAyyjkAAAAAACajnAMAAAAAYDLKOQAAAAAAJqOcAwAAAABgskpbzidNmiSLxaJRo0aZHQUAAAAAgHNSKcv5mjVrNGPGDLVq1crsKAAAAAAAnLNKV85zcnJ06623aubMmapWrZrZcQAAAAAAOGd+Zgdw14gRI9SvXz/16NFDEydOPOOyhYWFKiwsdL7OysqSJNlsNtlsNo/mRMU69X3xvcGbcNzDG3Hcw1tx7MMbectxX979q1TlfPbs2Vq/fr3WrFlTruUnTZqkCRMmlJq+YMECBQcHV3Q8nAcLFy40OwJw3nHcwxtx3MNbcezDG1X14z4vL69cy1kMwzA8nKVC7Nu3T+3atdOCBQt00UUXSZK6du2q1q1ba+rUqWWuU9aZ88TERB07dkzh4eHnIzYqiM1m08KFC9WzZ09ZrVaz4wDnBcc9vBHHPbwVxz68kbcc91lZWYqJiVFmZuYZe2ilOXO+bt06HTlyRBdffLFzmt1u19KlS/XGG2+osLBQvr6+LusEBAQoICCg1LasVmuV/vKrMr47eCOOe3gjjnt4K459eKOqftyXd98qTTnv3r27fvvtN5dpd911l5o0aaKxY8eWKuYAAAAAAFQWlaach4WFqUWLFi7TQkJCFB0dXWo6AAAAAACVSaV7lBoAAAAAAFVNpTlzXpbFixebHQEAAAAAgHPGmXMAAAAAAExGOQcAAAAAwGSUcwAAAAAATEY5BwAAAADAZJRzAAAAAABMRjkHAAAAAMBklHMAAAAAAExGOQcAAAAAwGSUcwAAAAAATEY5BwAAAADAZJRzAAAAAABMRjkHAAAAAMBklHMAAAAAAExGOQcAAAAAwGSUcwAAAAAATEY5BwAAAADAZJRzAAAAAABMRjkHAAAAAMBklHMAAAAAAExGOQcAAAAAwGSUcwAAAAAATEY5BwAAAADAZJRzAAAAAABMRjkHAAAAAMBklHMAAAAAAExGOQcAAAAAwGSUcwAAAAAATEY5BwAAAADAZJRzAAAAAABMRjkHAAAAAMBklHMAAAAAAExGOQcAAAAAwGSUcwAAAAAATEY5BwAAAADAZJRzAAAAAABMRjkHAAAAAMBklHMAAAAAAExGOQcAAAAAwGSUcwAAAAAATEY5BwAAAADAZJRzAAAAAABMRjkHAAAAAMBklHMAAAAAAExGOQcAAAAAwGSUcwAAAAAATEY5BwAAAADAZJRzAAAAAABMRjkHAAAAAMBklHMAAAAAAExGOQcAAAAAwGSUcwAAAAAATEY5BwAAAADAZJRzAAAAAABMRjkHAAAAAMBklHMAAAAAAExGOQcAAAAAwGSUcwAAAAAATEY5BwAAAADAZJRzAAAAAABMRjkHAAAAAMBklHMAAAAAAExGOQcAAAAAwGSUcwAAAAAATEY5BwAAAADAZJRzAAAAAABMRjkHAAAAAMBklHMAAAAAAExGOQcAAAAAwGSUcwAAAAAATEY5BwAAAADAZJRzAAAAAABMRjkHAAAAAMBklHMAAAAAAExGOQcAAAAAwGR+ZgcAcO7sdruOHDmi/Px82e12s+OcNavVqtDQUEVHR8tisZgdBwAAADhvKOdAJZaamqpNmzbp999/VX7+IUk2SQ6zY50DX0kBqlatvpo1a6c2bdooJibG7FAAAACAx1HOgUpq/fr1+vrr6YqKOqp27ULVqFG0QkP95edXOe9WMQzJZrPrxIkCpaRs1oYNa7R2bUPdfvtDSkhIMDseAAAA4FGUc6AS2rhxo77++n9q3z5Xffu2rFKXgEdHB6tBgyj16lWsjz9O0Ycf/kdDhoxSfHy82dEAAAAAj6mcp9gAL2YYhpKTv1Xz5ifVt2+DKlXM/yogwE+33tpUoaE7tGLFL2bHAQAAADyKcg5UMgcOHFBm5m61b1+zyhbzUwIC/HTRRZHavv1X2Ww2s+MAAAAAHkM5ByqZ7du3KyQkS7VrR5gd5bxo1ixWRUUHtWfPHrOjAAAAAB5DOQcqmczMTMXEWOTjU7XPmp8SHR0ki6VAWVlZZkcBAAAAPIZyDlQyhYWFCggwzI5x3lgsFgUEWFRYWGh2FAAAAMBjKOdAJfRPt5obhqF77vlGUVEvymKZoI0b089PMA+p4rfWAwAAADxKDaiK5s/fpffe26jFi4eoXr1qiokJNjsSAAAAgDOgnANV0O7dJxQfH6bLLks8623YbHZZrb4VmOr0iors8vc/P+8FAAAAXIi4rB2oYoYMmaeHHvpBaWmZslgmKClpqubP36XLL39HkZGTFR39kvr3n6XduzOc66SmnpTFMkGffrpVXbu+p8DAifroo80aMmSeBg6crRdeWKbq1acoMnKyJkxYrOJihx5/fIGiol5UQsIreuedDS4ZDhzI0s03z1W1ai8qOvolDRgwW6mpJ10yDhw4W5MmLVPNmv9Wo0b/OV8fDwAAAHBBopwDVcxrr/XRc891VUJCuA4delRr1gxXbm6RRo/uqDVrhmvRojvk42PRtdfOkcPhOrDc2LE/aeTIS5SSMkK9ezeQJP388x4dPJitpUuH6JVXemv8+CXq33+WqlUL0q+/DtN997XTffd9q337MiVJeXk2dev2vkJDrVq6dIiWL79LoaH+6tPnIxUV2Z3vtWjRHqWkHNPChbfr228Hn78PCAAAALgAcVk7UMVERAQqLCxAvr4W1agRKkm6/vpmLsu8/fY1ioubom3bjqpFizjn9FGjLtF11zV1WTYqKkivv36VfHwsatw4Ri+99Ivy8mx66qkrJElPPnm5Jk9erl9+2adBgyI0e/YW+fhY9NZb18jyfyO5vfvuAEVGTtbixanq1au+JCkkxKq33rqGy9kBAAAAUc4Br7B7d4aeeSZZq1bt17Fjec4z5mlpmS7lvF27mqXWbd48zuWZ6tWrh6pFi1jna19fH0VHB+vIkVxJ0rp1B7VrV4bCwia5bKegoPj/LqUvKectW1anmAMAAAD/h3IOeIGrr/5EiYkRmjnzatWsGSaHw1CLFtNcLjOXpJAQ/1LrWq2ud79YLNLevXv0zTff6Morr1RISIgsFjkLv8Nh6OKLa+rjj68rta3Y2D9HjQ8JsVbErgEAAABVAuUcqOKOH89TSsoxTZ/eX1dcUUeStHx52jltMzIyUtu2bdPWrVt15ZVXuswrLt6vlJTjiosLUXh4wDm9DwAAAOAtGBAOqOKqVQtSdHSQZsxYr127MvTzz3s0evSP57TNmJgYPfTQQ2revLl++OEHZWdnKyOjZPT3gQPryd+/WH37fqBly/Zqz54TWrIkVQ8//IP278+qiF0CAAAAqhzKOVDF+fhYNHv2DVq37qBatHhTjzzyo15+uec5bzc4OFhXX321hg8fLovFotWr1+jzzz9Xx47t9MgjkXI4Tuq66z5V06b/1dChXys/v5gz6QAAAMBpcFk7UAWNGnWpRo261Pm6R4962rZthMsyhjHO+XtSUqTL61Pee29gqWmLFw9xeV2zZk0dPfovbdy4UT/99JO2b9+uiy5qJZttnaZOvUodOnT4v/eTjhyRTpyQXnxxoOLiSm0aAAAA8FqUcwDnzGazKT4+XnfeeafWrFmjdevWKTAwQAsXLlTjxo2VlRWh1aulo0clu13y9ZViY6UOHaTERLPTAwAAAOajnAOVjI+Pj4qLLf+84Hn0448/at269c7Xfn6+KioqksNh6N13P1RY2IPKz5dCQyWrVbLZpEOHpAULpF69/rmg2+1yPjMdAAAAqIoo50AlExgYqKNHzU7hqk+fPmrTpo3y8/OdP3l5eTpw4IBOnIhSfr4UFfXn8v7+Ja8zMqTVq6WEhJJHtJXF4TBUVCQFBQWdn50BAAAATEA5ByqZuLg4/fabVFhYrICAC+NfYavVqoSEhFLTDx+WvvxSCgwse73Q0JJL3Y8ckapXL3uZtLRMSWGKjY2tuMAAAADABYbR2oFKpmnTpioujtaOHcfNjvKPCgpKLkm3Wsueb7WWzC8oOP02tm49ooiI+qpVq5ZnQgIAAAAXAMo5UMlERkaqVq1WWrYsXXl5NrPjnFFgYMngb7bTxLTZSuaf7sx6enqONm8uUrNm7bnnHAAAAFUa5RyohK655gbl5LTQhx+m6NixPLPjnFZcXMmo7Dk5Zc/PySmZ//fHqhmGoT/+OKH33/9D0dFd1blzZ8+HBQAAAEx0YdywCsAt1atX1513PqgPPpimN97YqerV89WwYahCQ/1ltV5Yf3MLDCy59zwtTQoOlvz8pOJiKS9PCgiQ6taV1q8veQ56UZFdJ08W6Pff85WVFa5atbrrttuGMhgcAAAAqjzKOVBJVa9eXaNG/Uu7d+/W1q1btGnTFuXnZ8luLzY7WilHj0qbNkknTvz5nPOoKKlVK2nt2j+Xs1oDFBoapaZNW6t58+ZKTEzkcnYAAAB4Bco5UIlZrVY1adJETZo0kXSD2XHOyOGQtm4tKejVqknNm0s+F9ZJfgAAAMA0lHMA54WPj9SypdkpAAAAgAsT560AAAAAADAZ5RwAAAAAAJNRzgEAAAAAMBnlHAAAAAAAk1HOAQAAAAAwGeUcAAAAAACTUc4BAAAAADAZ5RwAAAAAAJNRzgEAAAAAMBnlHAAAAAAAk1HOAQAAAAAwGeUcAAAAAACTVZpyPmnSJLVv315hYWGKi4vTwIEDtX37drNjAQAAAABwzipNOV+yZIlGjBihVatWaeHChSouLlavXr2Um5trdjQAAAAAAM6Jn9kBymv+/Pkur999913FxcVp3bp16ty5s0mpAAAAAAA4d5WmnP9dZmamJCkqKuq0yxQWFqqwsND5OisrS5Jks9lks9k8GxAV6tT3xfcGb8JxD2/EcQ9vxbEPb+Qtx315989iGIbh4SwVzjAMDRgwQCdOnNCyZctOu9z48eM1YcKEUtNnzZql4OBgT0YEAAAAAEB5eXkaPHiwMjMzFR4eftrlKmU5HzFihL777jstX75cCQkJp12urDPniYmJOnbs2Bk/FFx4bDabFi5cqJ49e8pqtZodBzgvOO7hjTju4a049uGNvOW4z8rKUkxMzD+W80p3WftDDz2kr7/+WkuXLj1jMZekgIAABQQElJputVqr9JdflfHdwRtx3MMbcdzDW3HswxtV9eO+vPtWacq5YRh66KGH9OWXX2rx4sWqW7eu2ZEAAAAAAKgQlaacjxgxQrNmzdJXX32lsLAwpaenS5IiIiIUFBRkcjoAAAAAAM5epXnO+bRp05SZmamuXbsqPj7e+TNnzhyzowEAAAAAcE4qzZnzSjhuHQAAAAAA5VJpzpwDAAAAAFBVUc4BAAAAADAZ5RwAAAAAAJNRzgEAAAAAMBnlHAAAAAAAk1HOAQAAAAAwGeUcAAAAAACTUc4BAAAAADAZ5RwAAAAAAJNRzgEAAAAAMBnlHAAAAAAAk1HOAQAAAAAwGeUcAAAAAACTUc4BAAAAADAZ5RwAAAAAAJNRzgEAAAAAMBnlHAAAAAAAk/mZHQAAgL9zOBzKyclRfn6+HA6H2XEqnJ+fn4KCghQaGmp2FAAAcIGgnAMALhhpaWnasmWLtm37VTk5hyTZJBlmx/IAH0lWxcY2UvPmF6tFixaKiYkxOxQAADAR5RwAcEFYtmyZFi36QBERx9WyZbDq1q2moKAQ+fpWvTuwiosdys4u1I4da7Ry5QotX15HgwY9oAYNGpgdDQAAmIRyDgAw3fLly7Vo0dvq2lXq0qWlLBaL2ZHOi+bN41Rc7NBnn/2u2bPf0C23PKT69eubHQsAAJig6p2OAABUKgUFBUpOnquOHYvVtWuS1xTzU/z8fHTTTU2UkJCmhQu/NTsOAAAwCeUcAGCq7du3y24/pI4dE82OYhpfXx916BCv9PStysjIMDsOAAAwAeUcAGCqlJStql3brvDwALOjmKphwyj5+2coJSXF7CgAAMAElHMAgKlOnEhXjRreXcwlyWr1VXS0RSdOnDA7CgAAMAHlHABgqsLCXAUE+Jod44IQEGCosLDQ7BgAAMAElHMAgOkuxEHgxo9frNat/3de39PHRzKMqvhcdwAA8E8o5wCAC17Xru9p1Kj5ZscAAADwGLfL+XPPPae8vLxS0/Pz8/Xcc89VSCgAAKoiwzBUXOwwOwYAALgAuV3OJ0yYoJycnFLT8/LyNGHChAoJBQDAKUOGzNOSJXv12mu/ymKZIItlglJTT2rbtqPq2/djhYa+oOrVp+j227/UsWN//vF4/vxduvzydxQZOVnR0S+pf/9Z2r3b9TFl+/dnadCguYqKelEhIS+oXbsZ+vXX/S7LfPjhJiUlTVVExGQNGjRX2dl/3hNuGIZeeukX1av3moKCntdFF/1Pc+duc85fvDhVFssE/fjjLrVrN0MBARO1bNleD31SAACgMnO7nBuGUea9gZs2bVJUVFSFhAIA4JTXXuujjh0TNHx4Wx069KgOHXpUVquPunR5T61b19Datfdo/vxbdfhwjm666TPnerm5RRo9uqPWrBmuRYvukI+PRddeO0cOR8k93Tk5RerS5T0dPJitr7++RZs23acxYzo550vS7t0nNG/edn377WB9++0tWrJkryZPXu6c//TTP+vddzdq2rR+2rr1AT3yyKW67bYvtGRJqss+jBnzkyZN6q6UlBFq1aq6Zz8wAABQKfmVd8Fq1arJYrHIYrGoUaNGLgXdbrcrJydH9913n0dCAgC8V0REoPz9fRUcbFWNGqGSpGefTVbbtvF64YXuzuXeeWeAEhNf1Y4dx9WoUbSuv76Zy3befvsaxcVN0bZtR9WiRZxmzfpNR4/mas2a4YqKCpIkNWjg+kdmh8PQe+8NUFhYyaPebr+9lRYt2qPnny8p/6+8sko//3yHOnZMlCTVq1dNy5enafr0derSJcm5neee66qePetX+GcDAACqjnKX86lTp8owDA0dOlQTJkxQRESEc56/v7+SkpLUsWNHj4QEAOCv1q07pOTkPQoNfaHUvN27M9SoUbR2787QM88ka9Wq/Tp2LM95RjwtLVMtWsRp48Z0tWkT7yzmZUlKinQWc0mKjw/VkSO5kqRt246qoKBYPXt+6LJOUZFdbdrEu0xr167mWe8rAADwDuUu53feeackqW7durrssstktVo9FgoAgDNxOAxdfXVjvfhij1Lz4uNLzq5fffUnSkyM0MyZV6tmzTA5HIZatJimoiK7JCkoqOz/BM6bN0/Hjx/X8eORslpd7/6yWCzOkn/qn999N1i1aoW7LPf357aHhPifxV4CAABvUu5yfkqXLl3kcDi0Y8cOHTlyRA6H66iznTt3rrBwAABIkr+/r+z2P/9707ZtDX3+eYqSkiLl51d6+JTjx/OUknJM06f31xVX1JEkLV+e5rJMq1bV9dZbG5SRke9y9rx9+/aaP3++fvtti44f99OOHTvUsGFDl9u5DMNQRESRAgJ8lZaW6XIJOwAAwNlwu5yvWrVKgwcP1t69e2UYhss8i8Uiu91eYeEAAJBKLi//9dcDSk09qdBQf40Y0UEzZ67XLbd8rscfv0wxMcHatStDs2dv0cyZV6tatSBFRwdpxoz1io8PU1papp544ieXbd5yS0u98MJyDRw4W5MmdVd8fJg2bDikmjXDNHToUG3ePE9//JGiWbM+Uc2a8erSpYvzv3uZmZmaPftDDRpUV4888qMcDkOXX15bWVmFWrFin0JD/XXnna1N+KQAAEBl5fZo7ffdd5/atWunLVu2KCMjQydOnHD+ZGRk/PMGAABw02OPXSZfXx81a/Zfxca+rKIiu375Zajsdod69/5ILVq8qYcfnq+IiAD5+Fjk42PR7Nk3aN26g2rR4k098siPevnlni7b9Pf31YIFtykuLkR9+85Sy5bTNHnyL/L19ZHFYlG1atUUFRWlO++8Q1arVZ98MlsrV66UzWZTRESEOna8VHXr7tHIkRdp0qTlatr0v+rd+yN9880O1a1bzaRPCgAAVFZunznfuXOn5s6dqwYNGngiDwAApTRqFK2VK+8uNf2LL24+7To9etTTtm0jXKYZxjiX13XqRGru3JvKXH/8+K4aP76rJCkpKUmpqalasmSJmjXbq+nTp6tz587av3+/srK2acOG+xQUFPR/7yEdOSLt3Ss1bZokh2OcyngCKQAAgAu3y/kll1yiXbt2Uc4BAF7DYrGobt26qlu3rrOkf/rpZ4qNjVF+fr6+/PJL3XLLLdq/36LVq6WjRyW7XfL1lWJjpQ4dpMREs/cCAABcyNwu5w899JAeffRRpaenq2XLlqVGbW/VqlWFhQMAVH1/HQH9QnfqNq6WLVuqZs2a2r59u4qKbNqxY6fmzPlWublXKz9fCg2VrFbJZpMOHZIWLJB69frngu5wyGXgOQAA4D3cLufXX3+9JGno0KHOaRaLRYZhMCAcAMBtAQEhKiwsNjtGuaxbt07Ll/9S5rzdu3cqIkKKivpzmr9/yeuMDGn1aikhQWe8xL2gQIqNDazg1AAAoDJwu5zv2bPHEzkAAF4qJqaW9u9fbnaMcunevbu6d+8uyfUM9+HD0pdfSoGn6dWhoSWXuh85IlWvXvYyBQXFOnpUat06uqJjAwCASsDtcl6nTh1P5AAAeKmmTZvrs8/8deJEvqpVC/rnFUx0ukvOCwpK7jH/251eTlarlJtbstzp7NhxXHZ7jJo1a1YBSQEAQGXj9qPUJOnDDz9Up06dVLNmTe3du1eSNHXqVH311VcVGg4AUPU1bNhQVmuClizZ63yOeGUTGFgy+JvNVvZ8m61k/unOrBcWFmvlykNKTGyt8PBwzwUFAAAXLLfL+bRp0zR69Gj17dtXJ0+edN5jHhkZqalTp1Z0PgBAFefv76++fQdr06ZIffvtTtlslW/skri4klHZc3LKnp+TUzI/Lq6seUX66KMUnTjRVH36XO3ZoAAA4ILl9mXt//nPfzRz5kwNHDhQkydPdk5v166dHnvssQoNBwDwDm3atJF0r77++m1t3rxNjRr5qV69CAUFWeXrWzlGL4+Oln7/veT+8+Bgyc9PKi6W8vJKzpi3aCHt2FGyrM3mUE5OkbZvP6HUVF8FBDTW7bePUK1atczdCQAAYJqzGhCu5P9EuQoICFBubm6FhAIAeJ82bdqoTp3ntHXrVm3btkFbt+6QVCzJYXa0cktPl7ZulU6e/PM555GRUvPm0uLFf13SVz4+oUpKukz9+1+kJk2aKCQkxJTMAADgwuB2Oa9bt642btxYamC4H374gUFsAADnJCoqSldccYWuuOIKORwOFRYWyuGoPOVcKnlW+bZtJQU9MlJq1kzy+dtNZH5+fvL39+eZ5gAAwMntcv74449rxIgRKigokGEYWr16tT755BNNmjRJb731licyAgC8kI+Pj4KCLuzR20/nkkvMTgAAACobt8v5XXfdpeLiYo0ZM0Z5eXkaPHiwatWqpddee02DBg3yREYAAAAAAKo0t8u5JA0fPlzDhw/XsWPH5HA4FFfW8LMAAAAAAKBczqqcnxITE1NROQAAAAAA8FpuP+f88OHDuv3221WzZk35+fnJ19fX5QcAAAAAALjH7TPnQ4YMUVpamp555hnFx8cz0iwAAAAAAOfI7XK+fPlyLVu2TK1bt/ZAHAAAAAAAvI/bl7UnJibKMAxPZAEAAAAAwCu5Xc6nTp2qJ554QqmpqR6IAwAAAACA93H7svabb75ZeXl5ql+/voKDg2W1Wl3mZ2RkVFg4AAAAAAC8gdvlfOrUqR6IAQAAAACA93K7nN95552eyAEAAAAAgNdyu5xLkt1u17x585SSkiKLxaJmzZrpmmuu4TnnAAAAAACcBbfL+a5du9S3b18dOHBAjRs3lmEY2rFjhxITE/Xdd9+pfv36nsgJAAAAAECV5fZo7SNHjlT9+vW1b98+rV+/Xhs2bFBaWprq1q2rkSNHeiIjAAAAAABVmttnzpcsWaJVq1YpKirKOS06OlqTJ09Wp06dKjQcAAAAAADewO0z5wEBAcrOzi41PScnR/7+/hUSCgAAAAAAb+J2Oe/fv7/uuece/frrrzIMQ4ZhaNWqVbrvvvt0zTXXeCIjAAAAAABVmtvl/PXXX1f9+vXVsWNHBQYGKjAwUJ06dVKDBg302muveSIjAAAAAABVmtv3nEdGRuqrr77Szp07lZKSIklq1qyZGjRoUOHhAAAAAADwBmf1nHNJatiwobOQWyyWCgsEAAAAAIC3cfuydkl6++231aJFC+dl7S1atNBbb71V0dkAAAAAAPAKbp85f+aZZ/Tqq6/qoYceUseOHSVJK1eu1COPPKLU1FRNnDixwkMCAAAAAFCVuV3Op02bppkzZ+qWW25xTrvmmmvUqlUrPfTQQ5RzAAAAAADc5PZl7Xa7Xe3atSs1/eKLL1ZxcXGFhAIAAAAAwJu4Xc5vu+02TZs2rdT0GTNm6NZbb62QUAAAAAAAeJOzGq397bff1oIFC3TppZdKklatWqV9+/bpjjvu0OjRo53LvfLKKxWTEgAAAACAKsztcr5lyxa1bdtWkrR7925JUmxsrGJjY7VlyxbncjxeDQAAAACA8nG7nCcnJ3siBwAAAAAAXuusnnMOAAAAAAAqjttnzgsKCvSf//xHycnJOnLkiBwOh8v89evXV1g4AAAAAAC8gdvlfOjQoVq4cKFuuOEGdejQgXvLAQAAAAA4R26X8++++07ff/+9OnXq5Ik8AAAAAAB4HbfvOa9Vq5bCwsI8kQUAAAAAAK/kdjn/97//rbFjx2rv3r2eyAMAAAAAgNdx+7L2du3aqaCgQPXq1VNwcLCsVqvL/IyMjAoLBwAAAACAN3C7nN9yyy06cOCAXnjhBVWvXp0B4QAAAAAAOEdul/MVK1Zo5cqVuuiiizyRBwAAAAAAr+P2PedNmjRRfn6+J7IAAAAAAOCV3C7nkydP1qOPPqrFixfr+PHjysrKcvkBAAAAAADucfuy9j59+kiSunfv7jLdMAxZLBbZ7faKSQYAAAAAgJdwu5wnJyd7IgcAAAAAAF7L7XLepUsXT+QAAAAAAMBruV3OJenkyZN6++23lZKSIovFombNmmno0KGKiIio6HwAAAAAAFR5bg8It3btWtWvX1+vvvqqMjIydOzYMb3yyiuqX7++1q9f74mMAAAAAABUaW6fOX/kkUd0zTXXaObMmfLzK1m9uLhYw4YN06hRo7R06dIKDwkAAAAAQFXmdjlfu3atSzGXJD8/P40ZM0bt2rWr0HAAAAAAAHgDty9rDw8PV1paWqnp+/btU1hYWIWEAgAAAADAm7hdzm+++WbdfffdmjNnjvbt26f9+/dr9uzZGjZsmG655RZPZAQAAAAAoEpz+7L2KVOmyGKx6I477lBxcbEkyWq16v7779fkyZMrPCAAAAAAAFWd2+Xc399fr732miZNmqTdu3fLMAw1aNBAwcHBnsgHAAAAAECVV+7L2u12uzZv3qz8/HxJUnBwsFq2bKlWrVrJYrFo8+bNcjgcHgsKAAAAAEBVVe5y/uGHH2ro0KHy9/cvNc/f319Dhw7VrFmzKjQcAAAAAADeoNzl/O2339Zjjz0mX1/fUvN8fX01ZswYzZgxo0LDAQAAAADgDcpdzrdv365LL730tPPbt2+vlJSUCgkFAAAAAIA3KXc5z83NVVZW1mnnZ2dnKy8vr0JCAQAAAADgTcpdzhs2bKgVK1acdv7y5cvVsGHDCgkFAAAAAIA3KXc5Hzx4sJ5++mlt3ry51LxNmzbp2Wef1eDBgys0HAAAAAAA3qDczzl/5JFH9MMPP+jiiy9Wjx491KRJE1ksFqWkpOinn35Sp06d9Mgjj3gyKwAAAAAAVVK5y7nVatWCBQv06quvatasWVq6dKkMw1CjRo30/PPPa9SoUbJarZ7MCgAAAABAlVTuci6VFPQxY8ZozJgxnsoDAAAAAIDXKfc95wAAAAAAwDMo5wAAAAAAmIxyDgAAAACAySjnAAAAAACYjHIOAAAAAIDJyjVa++jRo8u9wVdeeeWswwAAAAAA4I3KVc43bNhQro1ZLJZzCgMAAAAAgDcqVzlPTk72dA4AAAAAALwW95wDAAAAAGCycp05/7s1a9bos88+U1pamoqKilzmffHFFxUS7HTefPNNvfzyyzp06JCaN2+uqVOn6oorrvDoewIAAAAA4ElunzmfPXu2OnXqpG3btunLL7+UzWbTtm3b9PPPPysiIsITGZ3mzJmjUaNG6V//+pc2bNigK664QldddZXS0tI8+r4AAAAAAHiS2+X8hRde0Kuvvqpvv/1W/v7+eu2115SSkqKbbrpJtWvX9kRGp1deeUV33323hg0bpqZNm2rq1KlKTEzUtGnTPPq+AAAAAAB4ktuXte/evVv9+vWTJAUEBCg3N1cWi0WPPPKIrrzySk2YMKHCQ0pSUVGR1q1bpyeeeMJleq9evbRixYoy1yksLFRhYaHzdVZWliTJZrPJZrN5JCc849T3xfcGb8JxD2/EcQ9vxbEPb+Qtx31598/tch4VFaXs7GxJUq1atbRlyxa1bNlSJ0+eVF5enrubK7djx47JbrerevXqLtOrV6+u9PT0MteZNGlSmX8sWLBggYKDgz2SE561cOFCsyMA5x3HPbwRxz28Fcc+vFFVP+7L25PdLudXXHGFFi5cqJYtW+qmm27Sww8/rJ9//lkLFy5U9+7d3Q7qrr8/S90wjNM+X/3JJ5/U6NGjna+zsrKUmJioXr16KTw83KM5UbFsNpsWLlyonj17ymq1mh0HOC847uGNOO7hrTj24Y285bg/dQX3P3G7nL/xxhsqKCiQVFJ+rVarli9fruuuu07PPPOMu5srt5iYGPn6+pY6S37kyJFSZ9NPCQgIUEBAQKnpVqu1Sn/5VRnfHbwRxz28Ecc9vBXHPrxRVT/uy7tvZ3VZ+yk+Pj4aM2aMxowZ4+5m3Obv76+LL75YCxcu1LXXXuucvnDhQg0YMMDj7w8AAAAAgKec1XPO7Xa7vvzyS6WkpMhisahp06YaMGCA/PzOanPlNnr0aN1+++1q166dOnbsqBkzZigtLU333XefR98XAAAAAABPcrtNb9myRQMGDFB6eroaN24sSdqxY4diY2P19ddfq2XLlhUe8pSbb75Zx48f13PPPadDhw6pRYsW+v7771WnTh2PvScAAAAAAJ7mdjkfNmyYmjdvrrVr16patWqSpBMnTmjIkCG65557tHLlygoP+VcPPPCAHnjgAY++BwAAAAAA55Pb5XzTpk0uxVySqlWrpueff17t27ev0HAAAAAAAHgDH3dXaNy4sQ4fPlxq+pEjR9SgQYMKCQUAAAAAgDdxu5y/8MILGjlypObOnav9+/dr//79mjt3rkaNGqUXX3xRWVlZzh8AAAAAAPDP3L6svX///pKkm266SRaLRZJkGIYk6eqrr3a+tlgsstvtFZUTAAAAAIAqy+1ynpyc7IkcAAAAAAB4LbfLeZcuXTyRAwAAAAAAr1Wucr5582a1aNFCPj4+2rx58xmXbdWqVYUEAwAAAADAW5SrnLdu3Vrp6emKi4tT69atZbFYnPeZ/xX3mQMAAAAA4L5ylfM9e/YoNjbW+TsAAAAAAKg45SrnderUKfN3AAAAAABw7tx+zvmkSZP0zjvvlJr+zjvv6MUXX6yQUAAAAAAAeBO3y/n06dPVpEmTUtObN2+u//3vfxUSCgAAAAAAb+J2OU9PT1d8fHyp6bGxsTp06FCFhAIAAAAAwJu4Xc4TExP1yy+/lJr+yy+/qGbNmhUSCgAAAAAAb1KuAeH+atiwYRo1apRsNpuuvPJKSdKiRYs0ZswYPfrooxUeEAAAAACAqs7tcj5mzBhlZGTogQceUFFRkSQpMDBQY8eO1ZNPPlnhAQEAAAAAqOrcLucWi0UvvviinnnmGaWkpCgoKEgNGzZUQECAJ/IBAAAAAFDluV3OTwkNDVX79u0rMgsAAAAAAF7J7XKem5uryZMna9GiRTpy5IgcDofL/D/++KPCwgEAAAAA4A3OakC4JUuW6Pbbb1d8fLwsFosncgEAAAAA4DXcLuc//PCDvvvuO3Xq1MkTeQAAAAAA8DpuP+e8WrVqioqK8kQWAAAAAAC8ktvl/P/9v/+nZ599Vnl5eZ7IAwAAAACA13H7svZ///vf2r17t6pXr66kpCRZrVaX+evXr6+wcAAAAAAAeAO3y/nAgQM9EAMAAAAAAO/ldjkfN26cJ3IAAAAAAOC13L7nHAAAAAAAVKxynTmPiorSjh07FBMTo2rVqp3x2eYZGRkVFg4AAAAAAG9QrnL+6quvKiwsTJI0depUT+YBAAAAAMDrlKuc33nnnZKk4uJiSVLv3r1Vo0YNz6UCAAAAAMCLuHXPuZ+fn+6//34VFhZ6Kg8AAAAAAF7H7QHhLrnkEm3YsMETWQAAAAAA8EpuP0rtgQce0KOPPqr9+/fr4osvVkhIiMv8Vq1aVVg4AAAAAAC8gdvl/Oabb5YkjRw50jnNYrHIMAxZLBbZ7faKSwcAAAAAgBdwu5zv2bPHEzkAAAAAAPBabpXz7Oxs7dixQzabTR06dFBMTIyncgEAAAAA4DXKXc43b96sq666Sunp6TIMQ+Hh4Zo7d6569OjhyXwAAAAAAFR55R6t/YknnlDt2rW1bNkyrV27Vl26dNGDDz7oyWwAAAAAAHiFcp85X7t2rb7//nu1a9dOkvTOO+8oLi5OOTk5Cg0N9VhAAAAAAMD5k5+fr99//13btv2m9PTdKijIlWE4Kvx97Ha79uzZo40bf5avr2+FbttisSggIFjR0Ylq1uwiNWvWTGFhYRX6HhWt3OX82LFjql27tvN1dHS0goODdfToUco5AAAAAFQBW7du1RdfvC2H44Bq17arbdsQBQVZ5eNjqfD3stvt2rcvX4mJ2RVezg3DUEHBce3f/7sWLFio+fOrq0ePW9SpU6cKfZ+KVO5ybrFYlJ2drcDAQElyPjotOztbWVlZzuXCw8MrPiUAAAAAwKO2bt2qzz9/U82aHVPv3vUUFhbg0fez2+2qVi1PDRvWqvBy/lf5+TYtX56mhQvfkmEYuvzyyz32Xuei3OXcMAw1atSo1LQ2bdo4f+c55wAAAABQ+RQWFurLL99Vs2bHdN11TTxyptwsQUFW9ehRT76+qfrpp4/UqFEjxcXFmR2rlHKX8+TkZE/mAAAAAACYZPv27Sou3q+ePetWqWJ+isViUefOdfTrr1u1devWyl3Ou3Tp4skcAAAAAACTbN36mxISbIqICDQ7isf4+fmoSROrtm1br27dupkdp5RyP0oNAAAAAFA1HTmyR3XqBJsdw+Pq1InQ0aN75HBU/Ojz54pyDgAAAABerqAgV0FBVrNjeFxQkJ+kYhUUFJgdpRTKOQAAAAB4OcNwnNW95l27vqdRo+Z7IJFnlOyjgzPnAAAAAACUpbIV/YpGOQcAAAAAVBlFRZXz8d7lGq39uuuuK/cGv/jii7MOAwAAAAC4MOXmFun++7/TF1+kKCwsQI891tFl/okT+Xr44fn65psdKiwsVpcuSXr99T5q2DDaucwvv6Tpqad+1po1BxQQ4KfmzSM1b16CHnvsJy1ZsldLluzVa6/9Kknas+dhJSVFasmSVD3++EJt2nRYUVFBuvPOizRx4pXy8ys519y163tq0SJO/v6++uCDTWrePE5Llgw5b59LRSnXmfOIiAjnT3h4uBYtWqS1a9c6569bt06LFi1SRESEx4ICAAAAAMzz+OMLlZycqi+/vFkLFtymxYv3at26Q875Q4Z8pbVrD+rrrwdp5cq7ZRiG+vadJZut5Ez2xo3p6t79AzVvHquVK+/WkiV3qlu3mrLbDb32Wh917Jig4cPb6tChR3Xo0KNKTAzXgQNZ6tt3ltq3r6lNm+7TtGn99PbbGzRx4lKXbO+/v0l+fj765Zehmj69/3n9XCpKuc6cv/vuu87fx44dq5tuukn/+9//5OvrK0my2+164IEHFB4e7pmUAAAAAADT5OQU6e23N+iDDwaqZ8/6kqT33x+ohIRXJEk7dx7X119v1y+/DNVllyVKkj7++DolJr6qefN+1403NtdLL/2idu1q6s03+0kq6ZH+/g0VExMsX19f+fv7KjjYqho1Qp3v++aba5SYGK433ugri8WiJk1idPBgtsaO/UnPPtvFOYhdgwZReumlnufzI6lwbt9z/s477+ixxx5zFnNJ8vX11ejRo/XOO+9UaDgAAAAAgPl2785QUZFdHTsmOqdFRQWpceMYSVJKyjH5+fnokktqOedHRwerceMYpaQck3TqzHldt943JeWYOnZMlMXy50jynTolKienSPv3ZzmntWsXf1b7dSFxu5wXFxcrJSWl1PSUlJQLcjh6AAAAAMC5MYx/ml/2AoZh6FSvPt1z1IuLi7V69eoy+6RhSBZL6WmS6/SQEP8zB6wE3C7nd911l4YOHaopU6Zo+fLlWr58uaZMmaJhw4bprrvu8kRGAAAAAICJGjSIktXqo1Wr9junnTiRrx07jkuSmjWLVXGxQ7/+esA5//jxPO3YcVxNm8ZKklq1qq5Fi/aU2nZ+fr5+/vlnpacf0JEjR12KvsORriVL/nCZtmLFPoWF+atWrap1W3W57jn/qylTpqhGjRp69dVXdehQyc3/8fHxGjNmjB599NEKDwgAAAAAMFdoqL/uvruNHn98oaKjg1S9eqj+9a+fnfd8N2wYrQEDGmv48G80fXp/hYX564knFqlWrXANGNBYkvTkk5erZctpeuCB73Tffe3k6yt9+ukuPfBALT344IP66ae3tWTJH5o69T1dfXVv1asXr65dQzR//gGNGPGdRo68VNu3H9O4cYs1enRH53tXFW6Xcx8fH40ZM0ZjxoxRVlbJNf4MBAcAAAAAVdvLL/dSTo5N11wzW2Fh/nr00Y7KzCxwzn/33QF6+OH56t9/loqK7OrcuY6+/36wrNaS8coaNYrWggW36amnflaHDjMVFGRVixaRGjnSR6GhIXrzzdt0002faOzYNI0ePVNz516uQYP66bffZmjx4p16++2NiooK0t13t9HTT3c262PwGLfL+V9RygEAAADAO4SG+uvDD6/Vhx9e65z2+OOdnL9XqxakDz64tqxVnbp0SdIvvwyVVDJa+86dOxUZGSippLxv3PigCgsLlZycrF9/Xa5jx3apf/+Wql8/RQ8++JDz8d2GIR0+LBUUSHPmDFFcXEXv7fnn9j3nhw8f1u23366aNWvKz89Pvr6+Lj8AAAAAAJwtm82mjh07aujQoXI4HPrtt99ksVj0/fffS5L27ZO++EL68kvp229L/vnFFyXTKzO3z5wPGTJEaWlpeuaZZxQfH+8ypD0AAAAAoHI63Yjr59v06dOVnZ0jSQoJCVZoaKiys3O0ffsO/fTTOu3de7Hy86XQUMlqlWw26dAhacECqVcvKTHx9Nsu2cULs8O6Xc6XL1+uZcuWqXXr1h6IAwAAAAA43/z9g1RUZDc7hiRp2LBhOnr0qLKzs50/x48fV3p6unbuzJPdLkVF/bm8v3/J64wMafVqKSGh9OPXTiksLJbkq4CAgPOyL+5wu5wnJiZeMH9RAQAAAACcu/DwOB09us3sGJKkiIgI573lf3X4cMkl7KGhZa8XGiodPSodOSJVr172MseO5SkoKFFWa9nPXDeT2/ecT506VU888YRSU1M9EAcAAAAAcL41btxSO3faL5iz52UpKJDs9pJL2ctitZbMLygoe75hGNq2LUdNmnTwXMhz4HY5v/nmm7V48WLVr19fYWFhioqKcvkBAAAAAFQuzZs3l80Wrc2bD5sd5bQCAyVf35J7zMtis5XMDwwse35q6kkdPx6i5s2bey7kOXD7svapU6d6IAYAAAAAwCxRUVG66KLe+v77TxUU5KfmzS+8Z5PFxUmxsSWDv5V1XjgnR4qPV5mPVdu3L1OzZ+9XnTp9VLduXc+HPQtul/M777zTEzkAAAAAACYaMOBaORwOzZ37lVavPqLmzaspKSlSQUF+8vGp+BHO7Xa78vKKlZtbVO7HcrdoUXLv+aFDJfeY+/lJxcUlxTwoqGR+Xl7JqOwFBcXavz9LW7ce1x9/+CghoY9uvfXOC/YR4G6Xc6nkQ5w3b55SUlJksVjUrFkzXXPNNRfsTgIAAAAAzszHx0fXXnu96tdvqK1bN2v+/DVyOA5LKpbkqPD3czgcOnjwsGrW9JGPT/nvuD56VNq5U8rKkhwOycdHioiQGjQoGQyuhEWSnyyWMCUm9lCvXq3Vpk0b+fv7V/h+VBS3y/muXbvUt29fHThwQI0bN5ZhGNqxY4cSExP13XffqX79+p7ICQAAAADwMB8fH7Vu3VqtW7dWfv6NOnr0qAoKCuRwVHw5t9lsWrp0qTp37uz26OkOh7Rrl5SZ+Wcx/2u/t1gsCgwMVFRUlMLCwio4uWe4Xc5Hjhyp+vXra9WqVc4B4I4fP67bbrtNI0eO1HfffVfhIQEAAAAA51dQUJBq167tse3bbDb98ccfatKkyVk92qxZMw+EMpHb5XzJkiUuxVySoqOjNXnyZHXq1KlCwwEAAAAA4A3cfpRaQECAsrOzS03Pycm5oK/fBwAAAADgQuV2Oe/fv7/uuece/frrrzIMQ4ZhaNWqVbrvvvt0zTXXeCIjAAAAAABVmtvl/PXXX1f9+vXVsWNHBQYGKjAwUJ06dVKDBg302muveSIjAAAAAABVmtv3nEdGRuqrr77Srl27lJKSIsMw1KxZMzVo0MAT+QAAAAAAqPLO6jnnktSgQQMKOQAAAAAAFcDty9pvuOEGTZ48udT0l19+WTfeeGOFhAIAAAAAwJu4Xc6XLFmifv36lZrep08fLV26tEJCAQAAAADgTdwu56d7ZJrValVWVlaFhAIAAAAAwJu4Xc5btGihOXPmlJo+e/ZsNWvWrEJCAQAAAADgTdweEO6ZZ57R9ddfr927d+vKK6+UJC1atEiffPKJPvvsswoPCAAAAABAVed2Ob/mmms0b948vfDCC5o7d66CgoLUqlUr/fTTT+rSpYsnMgIAAAAAUKWd1aPU+vXrV+agcAAAAAAAwH1u33MuSSdPntRbb72lp556ShkZGZKk9evX68CBAxUaDgAAAAAAb+D2mfPNmzerR48eioiIUGpqqoYNG6aoqCh9+eWX2rt3rz744ANP5AQAAAAAoMpy+8z56NGjNWTIEO3cuVOBgYHO6VdddRXPOQcAAAAA4Cy4Xc7XrFmje++9t9T0WrVqKT09vUJCAQAAAADgTdwu54GBgcrKyio1ffv27YqNja2QUAAAAAAAeBO3y/mAAQP03HPPyWazSZIsFovS0tL0xBNP6Prrr6/wgAAAAAAAVHVul/MpU6bo6NGjiouLU35+vrp06aIGDRooLCxMzz//vCcyAgAAAABQpbk9Wnt4eLiWL1+un3/+WevXr5fD4VDbtm3Vo0cPT+QDAAAAAKDKc7ucn3LllVfqyiuvrMgsAAAAAAB4pXJf1v7rr7/qhx9+cJn2wQcfqG7duoqLi9M999yjwsLCCg8IAAAAAEBVV+5yPn78eG3evNn5+rffftPdd9+tHj166IknntA333yjSZMmeSQkAAAAAABVWbnL+caNG9W9e3fn69mzZ+uSSy7RzJkzNXr0aL3++uv69NNPPRISAAAAAICqrNzl/MSJE6pevbrz9ZIlS9SnTx/n6/bt22vfvn0Vmw4AAAAAAC9Q7nJevXp17dmzR5JUVFSk9evXq2PHjs752dnZslqtFZ8QAAAAAIAqrtzlvE+fPnriiSe0bNkyPfnkkwoODtYVV1zhnL9582bVr1/fIyEBAAAAAKjKyv0otYkTJ+q6665Tly5dFBoaqvfff1/+/v7O+e+884569erlkZAAAAAAAFRl5S7nsbGxWrZsmTIzMxUaGipfX1+X+Z999plCQ0MrPCAAAAAAAFVducv5KREREWVOj4qKOucwAAAAAAB4o3Lfcw4AAAAAADyDcg4AAAAAgMko5wAAAAAAmIxyDgAAAACAySjnAAAAAACYjHIOAAAAAIDJKOcAAAAAAJiMcg4AAAAAgMko5wAAAAAAmIxyDgAAAACAySjnAAAAAACYjHIOAAAAAIDJKOcAAAAAAJiMcg4AAAAAgMko5wAAAAAAmIxyDgAAAACAySjnAAAAAACYjHIOAAAAAIDJKkU5T01N1d133626desqKChI9evX17hx41RUVGR2NAAAAAAAzpmf2QHK4/fff5fD4dD06dPVoEEDbdmyRcOHD1dubq6mTJlidjwAAAAAAM5JpSjnffr0UZ8+fZyv69Wrp+3bt2vatGmUcwAAAABApVcpynlZMjMzFRUVdcZlCgsLVVhY6HydlZUlSbLZbLLZbKddzzAMORyOignqAT4+PrJYLGbHOK9OfV9n+t6AqobjHt6I4x7eimMf3shbjvvy7p/FMAzDw1kq3O7du9W2bVv9+9//1rBhw0673Pjx4zVhwoRS02fNmqXg4GDna7vdrvT0dO3bl6ajR/fIZsuTZPdE9ApikdUarKioJCUm1lZ8fLysVqvZoQAAAAAAf5OXl6fBgwcrMzNT4eHhp13O1HJ+uvL8V2vWrFG7du2crw8ePKguXbqoS5cueuutt864bllnzhMTE3Xs2DHnh5Kdna0PPpihjIzNionJU+PGIYqMDJTV6nsOe+ZZxcUOZWcXaseOHB06FKCQkMa6/fZ7FRsba3Y0j7HZbFq4cKF69uzJHyLgNTju4Y047uGtOPbhjbzluM/KylJMTMw/lnNTL2t/8MEHNWjQoDMuk5SU5Pz94MGD6tatmzp27KgZM2b84/YDAgIUEBBQarrVapXValV2drY+/vhtGcavuu++eoqPD61Ul4t37y4dP56nOXM2a9asGbrzzhFVuqBLf353gDfhuIc34riHt+LYhzeq6sd9effN1HIeExOjmJiYci174MABdevWTRdffLHeffdd+fic+1PgFi36SYWFKzR0aCNFRQWd8/bMEB0drDvvbKp3392kH374RnfcMdTsSAAAAAAAN1WK55wfPHhQXbt2VWJioqZMmaKjR48qPT1d6enpZ71Nu92u339frbZtwyttMT8lJMRfHTvGas+edcrNzTU7DgAAAADATZWinC9YsEC7du3Szz//rISEBMXHxzt/ztYff/yhgoJ9at68alwG3qRJjCyWY0pJSTE7CgAAAADATZWinA8ZMkSGYZT5c7aOHj0qf/8CxcWFVGBS84SE+Cs62q5jx46ZHQUAAAAA4KZKUc49oaCgQIGBqlQDwP2TwMCS/QIAAAAAVC5eW84Nw1AFjCl3QfHx0TldTQAAAAAAMEcVq6cAAAAAAFQ+lPPzyDAMFRc7zI4BAAAAALjAUM7PoGvX9zRy5A8aM2ahoqJeVI0aUzR+/GJJUmrqSVksE7Rx45+Pczt5skAWywQtXpwqSVq8OFUWywT9+OMutWs3QwEBE7Vs2V5t2pSubt3eV1jYJIWHT9LFF8/Q2rUHndtZsWKfOnd+V0FBzysx8VWNHPmDcnOLzueuAwAAAADOI8r5P3j//U0KCbHq11+H6aWXeuq555Zo4cLdbm1jzJifNGlSd6WkjFCrVtV1661fKCEhXGvWDNe6dffoiSc6yWot+Sp+++2wevf+SNdd11SbN9+nOXNu0PLlaXrwwR88sXsAAAAAgAuAn9kBLnStWlXXuHFdJUkNG0brjTdWa9GiPWrYMLrc23juua7q2bO+83VaWqYef/wyNWkS49zuKS+/vEKDB7fQqFGXOue9/vpV6tLlPU2b1k+BgXxlAAAAAFDV0PT+QatWcS6v4+PDdORIrlvbaNeupsvr0aM7atiwb/Thh5vVo0c93XhjM9WvHyVJWrfukHbtytDHH//mXN4wJIfD0J49J9S0aexZ7gkAAAAA4EJFOf8HVquvy2uLpaQo+/iUPB/9r48us9nsZW4jJMRfkpSenq6IiAiNH99Vgwe31AcfrNJ3323TuHGLNXv29br22qZyOAzde+/FGjnyklLbqV07oqJ2CwAAAABwAaGcn6XY2GBJ0qFDOWrTpmTaXweHK8vXX3+t3Nxc3XTTTWrUqJYuv9xXkZFZqlWrsd59d6Ouvbap2raN19atR9WgQZSndwEAAAAAcIFgQLizFBRk1aWXJmjy5OXatu2oli7dq6efTj7jOoMGDVJAQKiuu+4tTZ/+o2rUaKyUlFz98kuqmjYtuf987NhOWrlyn0aM+E4bN6Zr587j+vrr7Xrooe/Px24BAAAAAExAOT8H77xzjWw2h9q1m6GHH56viRO7nXH58PBwDRlyhwIDozRmzCq1b/+hvvzST3XrFmvcuC6SSgagW7JkiHbuzNAVV7yrNm2m65lnkhUfH3Y+dgkAAAAAYAIuaz+DxYuHlJo2b94g5+9Nm8Zq5cq7XeYbxjjn7127Jrm8lqTi4kItXHivUlJS9M033ygyMkLHjh3Xrl2/q1WrVjIMqXbtWpo583YFBkpxcSX3uQMAAAAAqi7K+Xlks9n02muvyeFwKDY2VvXr19e+fftksUg///yzIiNbas0ai44elex2yddXio2VOnSQEhPNTg8AAAAA8BSvvazdYrHoLwOtnxdWq1X333+/rr76atWuXVvZ2dkqLCyUYUgnT2Zq3rzfdOiQFBgoRUaW/PPQIWnBAmnfvn/e/vneHwAAAABAxfDaM+f+/v4qLDz/7xsTE6OYmBi1bdtWklRcXKz09MP66qsU2e2NFPWXQdr9/aWoKCkjQ1q9WkpIOPMl7oWFUkBAgIf3AAAAAABQ0bz2zHlkZKQKCvyUlWVCQ/8LPz8/Wa215OvbQ+HhgWUuExoqHT0qHTly+u3YbHZlZEgRETwLHQAAAAAqG68t5w0aNJCvb3Vt23bU7CgqKCi5x9xqLXu+1Voyv6Dg9NvYuTNDNls1NWnSxDMhAQAAAAAe47XlPDAwUPXrt9eGDcdVVGQ3OUvJ4G82W9nzbbaS+YFln1iXw2Fo7dp01ajRQtHR0Z4LCgAAAADwCK8t55LUuXNXnTjRULNmpZha0OPiSkZlz8kpe35OTsn8uLjS8xwOQ59//rtSU+N15ZVXeTYoAAAAAMAjvHZAOElKSEjQbbc9qI8+ekOvvrpNTZpY1aRJjCIiAuXv73teszRqJO3fXzIqe0hIyaXsNpuUm1tyxrxRI+nEiZJlbTa7srOLtGPHcW3blq+8vFq66aYRatSo0XnNDAAAAACoGF5dziWpdu3auvfesdq0aZO2bl2rDRt2SsqWdP7PpB89Km3fLmVmSg6H5OMjRURIjRv/WcxL+EjyV0REU7Vs2V6tWrVSfHz8ec8LAAAAAKgYXl/OJSk6OlpXXnmlunXrppMnTyo3N1e2090A7mEOh7Rjh5SVJYWHl5wx9/nbzQd+fn4KDg5WVFSULGd6thoAAAAAoFKgnP+FxWJRtWrVVK1aNVNz1K9v6tsDAAAAAM4zrx4QDgAAAACACwHlHAAAAAAAk1HOAQAAAAAwGeUcAAAAAACTUc4BAAAAADAZ5RwAAAAAAJNRzgEAAAAAMBnlHAAAAAAAk1HOAQAAAAAwGeUcAAAAAACTUc4BAAAAADAZ5RwAAAAAAJNRzgEAAAAAMBnlHAAAAAAAk1HOAQAAAAAwGeUcAAAAAACTUc4BAAAAADAZ5RwAAAAAAJNRzgEAAAAAMBnlHAAAAAAAk1HOAQAAAAAwGeUcAAAAAACTUc4BAAAAADAZ5RwAAAAAAJNRzgEAAAAAMBnlHAAAAAAAk1HOAQAAAAAwGeUcAAAAAACTUc4BAAAAADAZ5RwAAAAAAJNRzgEAAAAAMBnlHAAAAAAAk1HOAQAAAAAwGeUcAAAAAACTUc4BAAAAADAZ5RwAAAAAAJNRzgEAAAAAMBnlHAAAAAAAk1HOAQAAAAAwGeUcAAAAAACTUc4BAAAAADAZ5RwAAAAAAJNRzgEAAAAAMBnlHAAAAAAAk1HOAQAAAAAwGeUcAAAAAACTUc4BAAAAADAZ5RwAAAAAAJNRzgEAAAAAMBnlHAAAAAAAk1HOAQAAAAAwGeUcAAAAAACTUc4BAAAAADAZ5RwAAAAAAJNRzgEAAAAAMBnlHAAAAAAAk1HOAQAAAAAwGeUcAAAAAACTUc4BAAAAADAZ5RwAAAAAAJNRzgEAAAAAMBnlHAAAAAAAk1HOAQAAAAAwGeUcAAAAAACTUc4BAAAAADAZ5RwAAAAAAJNRzgEAAAAAMBnlHAAAAAAAk1HOAQAAAAAwGeUcAAAAAACTUc4BAAAAADAZ5RwAAAAAAJNRzgEAAAAAMBnlHAAAAAAAk1HOAQAAAAAwGeUcAAAAAACTUc4BAAAAADAZ5RwAAAAAAJNRzgEAAAAAMBnlHAAAAAAAk1HOAQAAAAAwGeUcAAAAAACTUc4BAAAAADAZ5RwAAAAAAJNRzgEAAAAAMBnlHAAAAAAAk1HOAQAAAAAwGeUcAAAAAACTUc4BAAAAADAZ5RwAAAAAAJNRzgEAAAAAMBnlHAAAAAAAk1HOAQAAAAAwGeUcAAAAAACTVbpyXlhYqNatW8tisWjjxo1mxwEAAAAA4JxVunI+ZswY1axZ0+wYAAAAAABUmEpVzn/44QctWLBAU6ZMMTsKAAAAAAAVxs/sAOV1+PBhDR8+XPPmzVNwcHC51iksLFRhYaHzdVZWliTJZrPJZrN5JCc849T3xfcGb8JxD2/EcQ9vxbEPb+Qtx315989iGIbh4SznzDAM9e3bV506ddLTTz+t1NRU1a1bVxs2bFDr1q1Pu9748eM1YcKEUtNnzZpV7oIPAAAAAMDZysvL0+DBg5WZmanw8PDTLmdqOT9def6rNWvWaMWKFZozZ46WLl0qX1/fcpfzss6cJyYm6tixY2f8UHDhsdlsWrhwoXr27Cmr1Wp2HOC84LiHN+K4h7fi2Ic38pbjPisrSzExMf9Yzk29rP3BBx/UoEGDzrhMUlKSJk6cqFWrVikgIMBlXrt27XTrrbfq/fffL3PdgICAUutIktVqrdJf/t/ZbDbt3LlTe/bsUV5enoqLi82O5Lbi4mJt3LhRWVlZ8vMr32Hr6+ur4OBgJSUlqVGjRvL39/dwSsAzvO1/swCJ4x7ei2Mf3qiqH/fl3TdTy3lMTIxiYmL+cbnXX39dEydOdL4+ePCgevfurTlz5uiSSy7xZMRKzW636/vvv9Xmzctksx1UbGyRwsIMVcbj3sfHoZo10+Xj84fKO45hQYF07JhFa9da5edXQ82bX67+/a+p0v/iAwAAAKicKsWAcLVr13Z5HRoaKkmqX7++EhISzIh0wbPb7fr009navXueOncOVYsWiYqKCjI71lmz2+3auTNADRs2lK+vr1vrnjiRr23bjmrx4lnKzs7ULbfcTkEHAAAAcEGpVI9SQ/ktWLBAu3fP06BB8ercuU6lLubnqlq1IHXqVFu33pqoffu+0/fff2d2JAAAAABwUSnLeVJSkgzDOONgcN7Mbrdr8+ZluvTSIDVoEGV2nAtGUlKkrrgiXFu3/lLlH9cAAAAAoHKplOUcZ5aamqr8/H1q3jzW7CgXnObNY1VUdEC7du0yOwoAAAAAOFHOq6DU1FSFheWrRo1Qs6NccKKjgxUVVajU1FSzowAAAACAE+W8CsrPz1doqEUWi8XsKBek0FCL8vPzzY4BAAAAAE6U8yrIbrfLz88wO8YFy2ot+YwAAAAA4EJBOfdSXbu+p1Gj5kuSkpKmaurUVc55FssEzZv3+3nNY8Z7AgAAAMCFolI85xyetWbNcIWE+Jua4dChR1WtWqCpGQAAAADALJRzKDY25Jy3YbPZZbX6ur1eUZFd/v6+DF4HAAAAwKtxWTtKXdYuSYcOZeuqqz5WUNDzqlv3NX322VbnvNTUk7JYJujTT7eqa9f3FBg4UR99tFnHj+fplls+V0LCKwoOfl4tW07TJ5/85rLdrl3f04MPfq/Ro39UTMxL6tnzQ0mlL2sfO3ahGjX6j4KDn1e9eq/p2WcXy2ZzOOePH79YrVv/Tx9+uElJSVMVETFZgwbNVXZ2oSc+IgAAAADwKMo5yvTMM8m6/vqm2rTpPt12W0vdcsvnSkk56rLM2LE/aeTIS5SSMkK9ezdQQUGxLr44Xt9+O1hbtjyge+5pq9tv/1K//rrfZb33398kPz8f/fLLUE2f3r/M9w8LC9B77w3Utm0j9NprffT22xv0/vvbXZbZvfuE5s3brm+/Haxvv71FS5bs1eTJyyv2gwAAAACA84DL2lGmG29spmHD2kqS/t//u1ILF/6h//xntd58s59zmVGjLtF11zV1We+xxy5z/v7QQ5do/vzd+uyzbbrkkgTn9AYNovTSSz3P+P5PP93Z+XtSUqQeeeSoPvxwvV588c9lHA5D7703QGFhAZKk229vpUWL9uj5593fXwAAAAAwE+UcZerYMfFvrxO0ceNhl2nt2tV0eW23OzR58nLNmbNVBw5kq7CwWIWFdoWEWCVJhmHo5MmTatw4RoZhnPE57HPnbtPUqau0a1eGcnKKVFzsUEiI6+GalBTpLOaSFB8fqiNHcs9qfwEAAADATFzWjnL7e5f++wjv//73Sr366iqNGdNJP/98hzZuvE+9e9dXUVHJM8XtdrsKCgq0d+9uTZs2TevXr1dxcXGp91m1ar8GDZqrq65qoG+/HawNG+7Vk09e7nLPuSRZra6Hr8VikcPB890BAAAAVD6cOUeZVq3arzvuuOgvrw+oTZsaZ1xn2bI0DRjQWLfd1kpSyWXnO3dmqGnTGEmSn5+fatSooXr1ghUV5dA333yjRYsWqV27di7b+eWXNNWpE6kHHmipw4cPq2HDJkpLy6zgPQQAAACACwflHGX67LNtateupi6/vLY+/nizVq8+oLffvuaM6zRoUE2ff56iFSv2qVq1QL3yykqlp+c4y/kp4eHhGjSojzIyMrRq1SqtWLFCkvTrr7+qY8dqatAgSmlpmXrrrVXat2+1QkIu0rx5Ozy2rwAAAABgNi5rR5kmTOiq2bO3qFWraXr//U36+OPr1KxZ7BnXeeaZLmrbNl69e3+krl3fV40aoRo4sMlpl4+KilLfvn01evRoSVJ6erqmTfufMjNXa8iQxnrxxd80c6aPvv12sx56qFVF7h4AAAAAXFA4c+6lFi8e4vw9NXWUyzzDGCdJeuCB9mWum5QU6Vzmr6KigjRv3qByv+8pQUFBMoxxstvt2rZtm1auXKlatfZo/PhotW/fRZs2bVJOzjYtWXKVc51x47rq/vu7au9eKTBQiouTRo26VKNGXXrG9wcAAACACxHlvIoyKuG4aL6+vmrZsqVatGihffv2aeXKlZo/f74CAwNlt9u1dOlSNWvWTAcP+mr1aunoUclul3x9pdhYqUMHKTHxn9/HqIwfDgAAAIAqjcvaqyA/Pz8VF5/+MWUXukOHDiktLU2xsbG66KKLFBYWJpvNphMnTuqddz7SggXSoUMlZ8wjI0v+eeiQtGCBtG/fP2/fZrPIz4+/SwEAAAC4cNBQqqDg4GBlZTn+8VniF6otW7Zo48aN8vf3d/7Url1bJ0+elM0WK7tdior6c3l//5LXGRnS6tVSQkLpx76dYhiGsrOlhITg87MzAAAAAFAOlPMqqF69elqyJFT79mWpdu0Is+O4rVevXurVq5fLNLvdrp07d2r58oY63Unv0NCSS92PHJGqVy97mcOHc3XyZJDq169fwakBAAAA4OxxWXsVVLt2bYWF1dOWLUfMjlLhioslq7XseVZryT3oBQWnX3/LliMKCkpU3bp1PRMQAAAAAM4C5bwKslgsatu2s9asMbRxY7rZcSqUn59ks5U9z2YrGRwuMLDs+Vu3HtGKFYVq3bqLfH19PRcSAAAAANzEZe1VVNeuXZWTk62vvpqtffsy1aJFnOrUiZSPT+W7B/2voqOlAwdc7zk/JSdHio8veazaKYZhKC0tU1u3HtXatXa1aHGjevbsef4CAwAAAEA5UM6rKIvFov79r1Z4eITWr1+mdet2KzDwgEJDLfLzq3wDxTkcDh06dEj+/jb99puPiopKzpD7+v55Kbu/v9SqlTRjRsk6NpuhnBxDBQVhCgtrpE6dLle3bt3k48MFIwAAAAAuLJTzKsxisahLly7q3LmzDhw4oD179igvL0/FxcVmR3NbcXGxDh5cq5Yt2ykw0E/ffFPy2DSbreRe89q1pf79paZN/1zHz89PQUFBSkpKUmJiYqX7gwQAAAAA70E59wIWi0UJCQlKSEgwO8pZs/3fjeZ9+/aV1WrV6NHS1q3SiRNStWpS8+YSJ8QBAAAAVFaUc1RKPj5Sy5ZmpwAAAACAisG5RgAAAAAATEY5BwAAAADAZJRzAAAAAABMRjkHAAAAAMBklHMAAAAAAExGOQcAAAAAwGSUcwAAAAAATEY5BwAAAADAZJRzAAAAAABMRjkHAAAAAMBklHMAAAAAAExGOQcAAAAAwGSUcwAAAAAATEY5BwAAAADAZJRzAAAAAABMRjkHAAAAAMBklHMAAAAAAExGOQcAAAAAwGSUcwAAAAAATEY5BwAAAADAZH5mBzifDMOQJGVlZZmcBO6y2WzKy8tTVlaWrFar2XGA84LjHt6I4x7eimMf3shbjvtT/fNUHz0dryrn2dnZkqTExESTkwAAAAAAvEl2drYiIiJOO99i/FN9r0IcDocOHjyosLAwWSwWs+PADVlZWUpMTNS+ffsUHh5udhzgvOC4hzfiuIe34tiHN/KW494wDGVnZ6tmzZry8Tn9neVedebcx8dHCQkJZsfAOQgPD6/S/+ICZeG4hzfiuIe34tiHN/KG4/5MZ8xPYUA4AAAAAABMRjkHAAAAAMBklHNUCgEBARo3bpwCAgLMjgKcNxz38EYc9/BWHPvwRhz3rrxqQDgAAAAAAC5EnDkHAAAAAMBklHMAAAAAAExGOQcAAAAAwGSUcwAAAAAATEY5R6WSmpqqu+++W3Xr1lVQUJDq16+vcePGqaioyOxoQIV68803VbduXQUGBuriiy/WsmXLzI4EeNSkSZPUvn17hYWFKS4uTgMHDtT27dvNjgWcV5MmTZLFYtGoUaPMjgJ41IEDB3TbbbcpOjpawcHBat26tdatW2d2LNNRzlGp/P7773I4HJo+fbq2bt2qV199Vf/73//01FNPmR0NqDBz5szRqFGj9K9//UsbNmzQFVdcoauuukppaWlmRwM8ZsmSJRoxYoRWrVqlhQsXqri4WL169VJubq7Z0YDzYs2aNZoxY4ZatWpldhTAo06cOKFOnTrJarXqhx9+0LZt2/Tvf/9bkZGRZkczHY9SQ6X38ssva9q0afrjjz/MjgJUiEsuuURt27bVtGnTnNOaNm2qgQMHatKkSSYmA86fo0ePKi4uTkuWLFHnzp3NjgN4VE5Ojtq2bas333xTEydOVOvWrTV16lSzYwEe8cQTT+iXX37hqsAycOYclV5mZqaioqLMjgFUiKKiIq1bt069evVymd6rVy+tWLHCpFTA+ZeZmSlJ/O87vMKIESPUr18/9ejRw+wogMd9/fXXateunW688UbFxcWpTZs2mjlzptmxLgiUc1Rqu3fv1n/+8x/dd999ZkcBKsSxY8dkt9tVvXp1l+nVq1dXenq6SamA88swDI0ePVqXX365WrRoYXYcwKNmz56t9evXc2UUvMYff/yhadOmqWHDhvrxxx913333aeTIkfrggw/MjmY6yjkuCOPHj5fFYjnjz9q1a13WOXjwoPr06aMbb7xRw4YNMyk54BkWi8XltWEYpaYBVdWDDz6ozZs365NPPjE7CuBR+/bt08MPP6yPPvpIgYGBZscBzguHw6G2bdvqhRdeUJs2bXTvvfdq+PDhLrfzeSs/swMAUsn/ERs0aNAZl0lKSnL+fvDgQXXr1k0dO3bUjBkzPJwOOH9iYmLk6+tb6iz5kSNHSp1NB6qihx56SF9//bWWLl2qhIQEs+MAHrVu3TodOXJEF198sXOa3W7X0qVL9cYbb6iwsFC+vr4mJgQqXnx8vJo1a+YyrWnTpvr8/7d39zFV1X8cwN+XZy5PVkQXxmMgQuLl6TIDrC6iIubioQDxIdANSUBISZkjhkLZDO4A20I0R60wFAHD8bCAaYtAQQf4EJG3YTRhs9Was+mU7vf3Rz/ujxuimPi7iu/XdrZ7zvd7Pt/POZeNfe75nnNqa/WU0aODxTk9EmxtbWFrazutvleuXEFYWBgCAwNRWVkJAwNOAKHZw8TEBIGBgWhtbUVMTIx2e2trK6KiovSYGdHDJYTA5s2bUV9fj5MnT8LNzU3fKRE9dOHh4Th//rzOtvXr18PLyws5OTkszGlWCg0NnfSqzB9//BEuLi56yujRweKcHisjIyNQKpVwdnZGcXExfv31V22bTCbTY2ZEM2fr1q1Yt24dFAqFdnbI8PAwn61As1p6ejoOHTqEr776ClZWVtrZIzY2NjA3N9dzdkQPh5WV1aTnKlhYWOCZZ57h8xZo1tqyZQtCQkKwe/duxMfHo7u7G/v37+dsWLA4p8fM119/DbVaDbVaPWm6I98KSLNFQkICfvvtNxQUFGB0dBQ+Pj5oamriL8o0q43fa6hUKnW2V1ZWIjk5+f+fEBERPRRBQUGor6/Hjh07UFBQADc3N5SWlmLNmjX6Tk3v+J5zIiIiIiIiIj3jzbpEREREREREesbinIiIiIiIiEjPWJwTERERERER6RmLcyIiIiIiIiI9Y3FOREREREREpGcszomIiIiIiIj0jMU5ERERERERkZ6xOCciIiIiIiLSMxbnRET0RFAqlXj77bf1ncaULl++DIlEgr6+vhmL6erqitLS0hmLBwA7d+6En5/fjMYkIiIiFudERDSLJCcnQyKRTFrUajXq6upQWFj4QPElEgmOHTs2rX7ji5WVFRQKBerq6u66j5OTE0ZHR+Hj4/NAOU7U09ODjRs3zli8+1FbWwulUgkbGxtYWlpCLpejoKAAv//+u17yeRQ9jB9kiIjo8cXinIiIZpXly5djdHRUZ3Fzc8PTTz8NKyurKfe7devWjOZRWVmJ0dFR9PT0wNfXF3Fxcejq6ppybENDQ8hkMhgZGc1YDs8++yykUumMxZuu3NxcJCQkICgoCM3Nzbhw4QJUKhX6+/vx+eef/9/zISIiehywOCciolnF1NQUMplMZzE0NJw0rd3V1RXvvfcekpOTYWNjg5SUFNy6dQsZGRmwt7eHmZkZXF1d8cEHH2j7A0BMTAwkEol2fSpz5syBTCaDl5cX9u3bBzMzMzQ0NEw59j+vop48eRISiQTt7e1QKBSQSqUICQnB4OCgzjgNDQ1QKBQwMzODra0tYmNjdY5x4rR2iUSC8vJyREZGwtzcHG5ubqipqdGJl5OTA09PT0ilUjz//PPIy8vD7du3p33+u7u7sXv3bqhUKhQVFSEkJASurq5YunQpamtrkZSUpO1bXl4Od3d3mJiYYN68eZMKd4lEgoqKCqxcuRJSqRTe3t7o6uqCWq2GUqmEhYUFgoOD8dNPP2n3GZ92X1FRAScnJ0ilUsTFxeGPP/7Q9tFoNCgoKICjoyNMTU3h5+eHlpYWbfv4d1FXV4ewsDBIpVL4+vpO+nGls7MTL7/8MszNzeHk5ITMzEz8+eefOud/9+7d2LBhA6ysrODs7Iz9+/dr293c3AAA/v7+kEgkUCqV0z7PREQ0+7A4JyKiJ1ZRURF8fHxw9uxZ5OXlYe/evWhoaMCRI0cwODiIL774QluE9/T0ANC9Ij5dxsbGMDIy0ily/zn2VHJzc6FSqXDmzBkYGRlhw4YN2rbGxkbExsbi1VdfRW9vr7aQv5u8vDy8/vrr6O/vx9q1a5GYmIiBgQFtu5WVFT799FN8//33KCsrw4EDB1BSUjLtY62qqoKlpSXS0tLu2D5nzhwAQH19PbKyspCdnY0LFy4gNTUV69evx4kTJ3T6FxYW4s0330RfXx+8vLywevVqpKamYseOHThz5gwAICMjQ2cftVqNI0eO4Pjx42hpaUFfXx/S09O17WVlZVCpVCguLsa5c+cQERGB1157DZcuXdKJk5ubi3feeQd9fX3w9PREYmIixsbGAADnz59HREQEYmNjce7cORw+fBgdHR2TclGpVFAoFOjt7UVaWho2bdqEH374AcDfP2QAQFtbG0ZHR+956wMREc1ygoiIaJZISkoShoaGwsLCQru88cYbQgghXnnlFZGVlaXt6+LiIqKjo3X237x5s1i8eLHQaDR3jA9A1NfX3zOPif1u3rwpCgsLBQDR1NQ05dhDQ0MCgOjt7RVCCHHixAkBQLS1tWn7NDY2CgDixo0bQgghgoODxZo1a6bMw8XFRZSUlOjk9dZbb+n0Wbhwodi0adOUMT788EMRGBioXc/Pzxe+vr5T9o+MjBRyuXzK9nEhISEiJSVFZ1tcXJxYsWKFTr7vvvuudr2rq0sAEAcPHtRu+/LLL4WZmZlOfoaGhuKXX37RbmtubhYGBgZidHRUCCGEg4ODeP/993XGDgoKEmlpaUKI/30Xn3zyibb94sWLAoAYGBgQQgixbt06sXHjRp0Y3377rTAwMNB+Py4uLmLt2rXado1GI+zs7ER5ebnOOOPfORERPdlm7sY2IiKiR0BYWBjKy8u16xYWFlP2/edV5uTkZCxduhTz5s3D8uXLsXLlSixbtuxf5ZGYmAhDQ0PcuHEDNjY2KC4uRmRk5JRjT0Uul2s/29vbAwCuXr0KZ2dn9PX1ISUl5b7yCg4OnrQ+8YFkR48eRWlpKdRqNa5fv46xsTFYW1tPO74QAhKJ5J79BgYGJj2sLjQ0FGVlZTrbJh7/c889BwBYsGCBzrabN2/i2rVr2jydnZ3h6Oioc4wajQaDg4OQSqUYGRlBaGjopLH7+/unHHviuffy8sLZs2ehVqtRVVWlc+wajQZDQ0Pw9vaeFEMikUAmk+Hq1av3Oj1ERPQEYnFORESzioWFBTw8PKbdd6KAgAAMDQ2hubkZbW1tiI+Px5IlS3D06NH7zqOkpARLliyBtbU17Ozs7jn2VIyNjbWfx4tejUYDADA3N7/vvO5kPO6pU6ewatUq7Nq1CxEREbCxsUF1dTVUKtW0Y3l6eqKjowO3b9/Wyf1u4467U2F/p+O/2zm52zgTY//bscfH0Wg0SE1NRWZm5qTxnJ2d7xhjPM7dciUioicX7zknIiKawNraGgkJCThw4AAOHz6M2tpa7eu/jI2N8ddff00rjkwmg4eHxx0L85kil8vR3t5+X/ucOnVq0rqXlxcA4LvvvoOLiwtyc3OhUCgwd+5c/Pzzz/cVf/Xq1bh+/To+/vjjO7aPP5jN29sbHR0dOm2dnZ3aK84PYnh4GCMjI9r1rq4uGBgYwNPTE9bW1nBwcHjgsQMCAnDx4kV4eHhMWkxMTKYVY7zfdP+miIhoduOVcyIiov8qKSmBvb09/Pz8YGBggJqaGshkMu1DzFxdXdHe3o7Q0FCYmpriqaee0mu++fn5CA8Ph7u7O1atWoWxsTE0Nzdj+/btU+5TU1MDhUKBRYsWoaqqCt3d3Th48CAAwMPDA8PDw6iurkZQUBAaGxtRX19/XzktXLgQ27dvR3Z2Nq5cuYKYmBg4ODhArVZj3759WLRoEbKysrBt2zbEx8cjICAA4eHhOH78OOrq6tDW1vZA5wQAzMzMkJSUhOLiYly7dg2ZmZmIj4+HTCYDAGzbtg35+flwd3eHn58fKisr0dfXpzNF/V5ycnLw4osvIj09HSkpKbCwsMDAwABaW1vx0UcfTSuGnZ0dzM3N0dLSAkdHR5iZmcHGxuZfHTMRET3+eOWciIjovywtLbFnzx4oFAoEBQXh8uXLaGpqgoHB3/8uVSoVWltb4eTkBH9/fz1nCyiVStTU1KChoQF+fn5YvHgxTp8+fdd9du3aherqasjlcnz22WeoqqrCCy+8AACIiorCli1bkJGRAT8/P3R2dt71SfJT2bNnDw4dOoTTp08jIiIC8+fPx9atWyGXy7WvUouOjkZZWRmKioowf/58VFRUoLKyckZeJ+bh4YHY2FisWLECy5Ytg4+Pj86V/MzMTGRnZyM7OxsLFixAS0sLGhoaMHfu3GmPIZfL8c033+DSpUt46aWX4O/vj7y8PO296dNhZGSEvXv3oqKiAg4ODoiKirqv4yQiotlFIoQQ+k6CiIiIHj6JRIL6+npER0frO5WHZufOnTh27JjOQ+6IiIgeB7xyTkRERERERKRnLM6JiIiIiIiI9IzT2omIiIiIiIj0jFfOiYiIiIiIiPSMxTkRERERERGRnrE4JyIiIiIiItIzFudEREREREREesbinIiIiIiIiEjPWJwTERERERER6RmLcyIiIiIiIiI9Y3FOREREREREpGf/AcDhi+On9fv1AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the words in 2D space\n",
    "plt.figure(figsize=(12, 8))\n",
    "# scatter plot with first column - the first PC x-coordinates - and 2nd column of array - 2nd PC y-coordinates\n",
    "# c='b' color blue\n",
    "# alpha=0.7 is transparency of points\n",
    "plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], c='b', alpha=0.7)\n",
    "\n",
    "# xytext=(5, 2): offset of the annotation text - 5 points right, 2 points above\n",
    "# ha is horizontal alignment\n",
    "# va vertical\n",
    "# bbox - properties of bounding box\n",
    "# arrowprops - style of arrow annotating points\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]), xytext=(5, 2), \n",
    "                 textcoords='offset points', ha='right', va='bottom',\n",
    "                 bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "                 arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))\n",
    "\n",
    "plt.title(\"Word Embeddings in 2D Space\")\n",
    "plt.xlabel(\"First Principal Component\")\n",
    "plt.ylabel(\"Second Principal Component\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "#plt.savefig('../outputs/word_embeddings_2d.svg', format='svg')\n",
    "#print(\"Plot saved as 'word_embeddings_2d.svg'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0a09e4-fd68-4810-95f9-8e480fc7fafd",
   "metadata": {},
   "source": [
    "This visualization helps us understand how word embeddings capture semantic relationships. Words with similar meanings or uses tend to cluster together in the vector space, while words with different meanings are farther apart. Even in this simplified 2D representation, we can see how the embedding space organizes words in a way that reflects their semantic relationships.\n",
    "\n",
    "Remember, though, that this is a significant simplification of the original high-dimensional space. In the full embedding space, these relationships are even more nuanced and accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e71a7f9-3983-46a1-9b8a-aa1b478eb259",
   "metadata": {},
   "source": [
    "### Euclidean Distance\n",
    "\n",
    "The Euclidian distance formula makes use of the Pythagorean theorem, where $a^2 + b^2 = c^2$. We can draw a triangle between two points, and calculate the hypotenuse to find the distance. This distance formula works in two dimensions, but can also be generalized over as many dimensions as we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9bbf414-9ef6-43ff-9c02-c056cbc59989",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distances between pairs:\n",
      "nurse - doctor: 8.9795\n",
      "nurse - farmer: 5.1067\n",
      "nurse - athlete: 6.6988\n",
      "nurse - librarian: 2.1792\n",
      "nurse - teacher: 5.3086\n",
      "doctor - farmer: 7.7625\n",
      "doctor - athlete: 9.6467\n",
      "doctor - librarian: 8.7666\n",
      "doctor - teacher: 5.5056\n",
      "farmer - athlete: 2.1909\n",
      "farmer - librarian: 6.8630\n",
      "farmer - teacher: 2.2624\n",
      "athlete - librarian: 8.6802\n",
      "athlete - teacher: 4.2892\n",
      "librarian - teacher: 6.4493\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the Euclidean distances between pairs\n",
    "print(\"\\nDistances between pairs:\")\n",
    "for i in range(len(words)):\n",
    "    # Loop through the remaining words (starting from the next word after i)\n",
    "    for j in range(i+1, len(words)):\n",
    "        # Calculate the Euclidean distance between the 2D vectors of word i and word j\n",
    "        distance = np.linalg.norm(vectors_2d[i] - vectors_2d[j])\n",
    "        # Print the distance between word i and word j with 4 decimal places\n",
    "        print(f\"{words[i]} - {words[j]}: {distance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a924f5e-d9c1-4a7c-b15f-cb27b3545cc7",
   "metadata": {},
   "source": [
    "## Semantic search & vector databases\n",
    "In this section we will create a vector database with many State of the Union documents, and query it semantically. \n",
    "\n",
    "### What are Vector Databases?\n",
    "Vector databases are specialized storage and retrieval systems designed to handle high-dimensional vector data efficiently. Unlike traditional databases that work with structured data like numbers and text, vector databases are optimized for managing and querying vector embeddings - numerical representations of data points in a multi-dimensional space.\n",
    "\n",
    "At their core, vector databases address the challenge of similarity search in large datasets. They excel at finding the most similar items to a given query, which is crucial for applications like recommendation systems, image recognition, and natural language processing. For instance, in a vector database storing product information, you could easily find similar products based on various attributes, all encoded as vectors.\n",
    "\n",
    "The key advantage of vector databases lies in their ability to perform fast approximate nearest neighbor (ANN) searches. Traditional databases might struggle with the \"curse of dimensionality\" when dealing with high-dimensional data, but vector databases employ specialized indexing techniques to maintain efficiency. This makes them particularly useful for AI and machine learning applications, where data is often represented in high-dimensional vector spaces.\n",
    "\n",
    "For those new to the concept, you can think of a vector database as a system that organizes information in a way that mirrors how our brains associate related concepts. Just as we can quickly recall words or images that are similar to a given prompt, vector databases can rapidly retrieve data points that are \"close\" to each other in a mathematical sense. This capability opens up exciting possibilities for creating more intelligent and intuitive data-driven applications across various domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a02cdec-f211-46ed-b88c-5213c24a5910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using !pip install\n",
    "#!pip install annoy txtai sentence-transformers rank-bm25\n",
    "\n",
    "# or conda\n",
    "#!conda install python-annoy txtai sentence-transformers\n",
    "#!pip install rank-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0ed5747-cfdc-4d31-8db3-409e6f04e169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from annoy import AnnoyIndex\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88611f8e-4734-4e1a-8fb2-f5107a254a3c",
   "metadata": {},
   "source": [
    "Here's how you can load all of the sotu speeches into an unprocessed and untokenized list. We are going to just load these in via a pickle file to save the load on the notebooks server..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e68e361f-f7d7-48dc-ad51-9ed000b5d32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sotu_kaggle = glob.glob(\"../sotu_kaggle/*.txt\")\n",
    "\n",
    "# # use a dictionary so we can keep track of the filenames\n",
    "# speeches = {}\n",
    "# for speech in sorted(sotu_kaggle):\n",
    "#     with open(speech, 'r') as file:\n",
    "#         speech = speech.lstrip('../sotu_kaggle/')[:-4]\n",
    "#         speeches[speech] = file.read() \n",
    "\n",
    "# # convert the dictionary to a df\n",
    "# df = pd.DataFrame(speeches.items(), columns=['name_year', 'text'])   \n",
    "# df.head()\n",
    "\n",
    "# # split name_date column by delimiter\n",
    "# df[['name','year']] = df['name_year'].str.split('_',expand=True)\n",
    "# df = df.drop('name_year', axis=1)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "524152ba-39e3-4bdc-8a88-bd8c327390be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>name</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gentlemen of the Senate and Gentlemen of the H...</td>\n",
       "      <td>Adams</td>\n",
       "      <td>1797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gentlemen of the Senate and Gentlemen of the H...</td>\n",
       "      <td>Adams</td>\n",
       "      <td>1798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gentlemen of the Senate and Gentlemen of the H...</td>\n",
       "      <td>Adams</td>\n",
       "      <td>1799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gentlemen of the Senate and Gentlemen of the H...</td>\n",
       "      <td>Adams</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fellow Citizens of the Senate and of the House...</td>\n",
       "      <td>Adams</td>\n",
       "      <td>1825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fellow Citizens of the Senate and of the House...</td>\n",
       "      <td>Adams</td>\n",
       "      <td>1826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fellow Citizens of the Senate and of the House...</td>\n",
       "      <td>Adams</td>\n",
       "      <td>1827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Fellow Citizens of the Senate and of the House...</td>\n",
       "      <td>Adams</td>\n",
       "      <td>1828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>To the Senate and House of Representatives of ...</td>\n",
       "      <td>Arthur</td>\n",
       "      <td>1881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>To the Senate and House of Representatives of ...</td>\n",
       "      <td>Arthur</td>\n",
       "      <td>1882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    name  year\n",
       "0  Gentlemen of the Senate and Gentlemen of the H...   Adams  1797\n",
       "1  Gentlemen of the Senate and Gentlemen of the H...   Adams  1798\n",
       "2  Gentlemen of the Senate and Gentlemen of the H...   Adams  1799\n",
       "3  Gentlemen of the Senate and Gentlemen of the H...   Adams  1800\n",
       "4  Fellow Citizens of the Senate and of the House...   Adams  1825\n",
       "5  Fellow Citizens of the Senate and of the House...   Adams  1826\n",
       "6  Fellow Citizens of the Senate and of the House...   Adams  1827\n",
       "7  Fellow Citizens of the Senate and of the House...   Adams  1828\n",
       "8  To the Senate and House of Representatives of ...  Arthur  1881\n",
       "9  To the Senate and House of Representatives of ...  Arthur  1882"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('sotu.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c025e26-0aeb-431d-bf67-1792ffc13db0",
   "metadata": {},
   "source": [
    "### Vectorize with sentence-transformers\n",
    "Now that we have all our documents, it comes time to vectorize them, or convert them into a sequence of vectors. This is where we pass the texts to a machine learning model and capture the output vector for each of them. To do this, though, we need a model loaded. We will be using the sentence-transformers library. It makes this process as simple as possible with only two lines of code. First, we will need to load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc0d5e80-197e-4b40-ae30-181fe31530b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # This is a standard, efficient model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99aaa4a-3b77-43c8-8feb-fcd7f3f8f2ba",
   "metadata": {},
   "source": [
    "Now that we have loaded our model we can (optionally) specify the specific device. The code below will put it onto your GPU, if available. If you don't know if this is enabled on your device, then it likely is not. The steps to activate cuda are very specific and require you to install certain packages in a certain way. If you do not have cuda, then the default will be the cpu. With 236 documents, this will not be an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23f7e218-b4e4-4559-b0b3-38f0756c0072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad985b39-8e0c-4c64-b42b-d1680be4260a",
   "metadata": {},
   "source": [
    "Let's encode the speeches! We can do that with a single line. I like to set show_progress_bar to True. We are going to skip this step on the notebooks server, and work from a saved pickle file with the embeddings. It took about 26 seconds for me locally to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "64eb5efc-959f-429e-9f0c-7e37392f647a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a8b22d4f70a42c59a30aa1f9e299ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#embeddings = model.encode(list(speeches.values()), show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "90d85b5a-02d0-44a4-8343-6333494169a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings from the pickle file\n",
    "with open('embeddings.pkl', 'rb') as f:\n",
    "    embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e330e53-37f5-497e-891e-0920c35003a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>name</th>\n",
       "      <th>year</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gentlemen of the Senate and Gentlemen of the H...</td>\n",
       "      <td>Adams</td>\n",
       "      <td>1797</td>\n",
       "      <td>[-0.0030435165, 0.09838831, 0.075595096, -0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gentlemen of the Senate and Gentlemen of the H...</td>\n",
       "      <td>Adams</td>\n",
       "      <td>1798</td>\n",
       "      <td>[-0.028948274, -0.0046473336, -0.009841042, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gentlemen of the Senate and Gentlemen of the H...</td>\n",
       "      <td>Adams</td>\n",
       "      <td>1799</td>\n",
       "      <td>[-0.0013630837, 0.12678212, 0.05758622, -0.001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gentlemen of the Senate and Gentlemen of the H...</td>\n",
       "      <td>Adams</td>\n",
       "      <td>1800</td>\n",
       "      <td>[0.04068835, 0.08385914, 0.10812476, 0.0049056...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fellow Citizens of the Senate and of the House...</td>\n",
       "      <td>Adams</td>\n",
       "      <td>1825</td>\n",
       "      <td>[-0.015273168, 0.089902245, -0.010282357, -0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   name  year  \\\n",
       "0  Gentlemen of the Senate and Gentlemen of the H...  Adams  1797   \n",
       "1  Gentlemen of the Senate and Gentlemen of the H...  Adams  1798   \n",
       "2  Gentlemen of the Senate and Gentlemen of the H...  Adams  1799   \n",
       "3  Gentlemen of the Senate and Gentlemen of the H...  Adams  1800   \n",
       "4  Fellow Citizens of the Senate and of the House...  Adams  1825   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.0030435165, 0.09838831, 0.075595096, -0.00...  \n",
       "1  [-0.028948274, -0.0046473336, -0.009841042, -0...  \n",
       "2  [-0.0013630837, 0.12678212, 0.05758622, -0.001...  \n",
       "3  [0.04068835, 0.08385914, 0.10812476, 0.0049056...  \n",
       "4  [-0.015273168, 0.089902245, -0.010282357, -0.0...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add embeddings \n",
    "df[\"embedding\"] = list(embeddings)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c68abf-3f5d-45b8-921a-031de0605145",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now make the year column the index and resort by year\n",
    "#df = df.set_index('year')\n",
    "#df.sort_index(inplace=True)\n",
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef8ed3d-cb60-4073-b099-d1526e2fc23b",
   "metadata": {},
   "source": [
    "At this stage, we now have all the data necessary to create a vector database! \n",
    "\n",
    "### BM25\n",
    "BM25 (Best Matching 25) is a ranking function used in information retrieval systems, particularly in search engines. It's an advanced form of TF-IDF (Term Frequency-Inverse Document Frequency) that provides a way to rank documents based on their relevance to a given search query.\n",
    "\n",
    "BM25 improves upon simpler ranking methods by incorporating document length normalization. This means it can account for the fact that longer documents are more likely to contain a given term simply due to their length, rather than because of relevance.\n",
    "\n",
    "The algorithm calculates a score for each document based on the query terms it contains. It considers both how often a term appears in a document (term frequency) and how rare the term is across all documents (inverse document frequency). However, it also applies a saturation function to prevent common terms from dominating the score.\n",
    "\n",
    "For those new to information retrieval, BM25 can be thought of as a way of determining which documents in a collection are most relevant to a user's search query. It's widely used in practice due to its effectiveness and relatively simple implementation.\n",
    "\n",
    "Now, let's look at how we can implement BM25 in Python using a DataFrame's 'speeches' field.\n",
    "\n",
    "To do that, we first need to tokenize our text. We could use `nlp()` to use our spaCy model from earlier, but that would be quite time consuming. To save time let's just split our documents up by whitespace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "49ffbe09-b55e-4003-9ad6-74f5bf7a80ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs = []\n",
    "for doc in df[\"text\"]:\n",
    "    split_doc = doc.split()\n",
    "    # to use a slower but better tokenizer using spaCy:\n",
    "    # split_doc = nlp(doc)\n",
    "    tokenized_docs.append(split_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32017f2c-852d-41eb-b4e1-a0123c894425",
   "metadata": {},
   "source": [
    "Our BM250kapi tool expects each document to be a list of tokens though, and our current speeches are strings. So let's convert them to make processing easier.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e75883-2af2-4b0a-a0dd-66c29780cb19",
   "metadata": {},
   "source": [
    "This single line of code will create a BM25 index for us! At this stage, we have essentially created a search engine index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "994660b0-9119-46c4-9b04-6857b9209f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_index = BM25Okapi(tokenized_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b731c6a-12cb-42e6-9a45-6d53a2868950",
   "metadata": {},
   "source": [
    "To query it, we need to create a query, tokenize the query, and then use the `get_scores()` method to retrieve the results. It's also best practice to sort these based on the scores. Finally, we can retrieve the results from the original DataFrame. In the cell below, we will have all the code necessary to perform these operations. I've done this so that you can more easily test this out with multiple queries. To understand what's happening in the code, though, let's breakdown each step here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c0f54ede-e26a-4478-a88e-6a98972bfb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 5.7038 - Speech 34: 1980 Carter\n",
      " This last few months has not been an easy time for any of us. As we meet\n",
      "tonight, it has never been more clear that the state of our Union depends\n",
      "on the state of the world. And tonight, as throughout our own generation,\n",
      "freedom and peace in the worl\n",
      "------------- \n",
      "\n",
      "Score: 5.3604 - Speech 204: 1951 Truman\n",
      " Mr. President, Mr. Speaker, Members of the Congress:\n",
      "\n",
      "This 82d Congress faces as grave a task as any Congress in the history of\n",
      "our Republic. The actions you take will be watched by the whole world.\n",
      "These actions will measure the ability of a free pe\n",
      "------------- \n",
      "\n",
      "Score: 5.2630 - Speech 206: 1953 Truman\n",
      " To the Congress of the United States:\n",
      "\n",
      "I have the honor to report to the Congress on the state of the Union.\n",
      "\n",
      "This is the eighth such report that, as President, I have been privileged\n",
      "to present to you and to the country. On previous occasions, it ha\n",
      "------------- \n",
      "\n",
      "Score: 5.2263 - Speech 33: 1979 Carter\n",
      " Tonight I want to examine in a broad sense the state of our American\n",
      "Union--how we are building a new foundation for a peaceful and a prosperous\n",
      "world.\n",
      "\n",
      "Our children who will be born this year will come of age in the 21st\n",
      "century. What kind of societ\n",
      "------------- \n",
      "\n",
      "Score: 5.0913 - Speech 35: 1981 Carter\n",
      " To the Congress of the United States:\n",
      "\n",
      "The State of the Union is sound. Our economy is recovering from a\n",
      "recession. A national energy plan is in place and our dependence on foreign\n",
      "oil is decreasing. We have been at peace for four uninterrupted years\n",
      "------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Soviet Union\"\n",
    "tokenized_query = query.split()\n",
    "doc_scores = bm25_index.get_scores(tokenized_query)\n",
    "ranked_docs = sorted(enumerate(doc_scores), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "for idx, score in ranked_docs:\n",
    "    print(f\"Score: {score:.4f} - Speech {idx}: {df['year'][idx]} {df['name'][idx]}\\n {df['text'][idx][0:250]}\")\n",
    "    print(\"-------------\", \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55db7fa3-b82d-4be2-871c-61e70b7459fd",
   "metadata": {},
   "source": [
    "- `query = \"Soviet\"`: This line defines the search query. \n",
    "- `tokenized_query = query.split()`: This line tokenizes the query string. The split() method without arguments splits the string on whitespace, creating a list of individual words. For our simple query, this results in [\"Soviet\", \"Union\"]. \n",
    "- `doc_scores = bm25_index.get_scores(tokenized_query)` - Use the pre-computed BM25 index (bm25_index) to score each document in the corpus based on the tokenized query.\n",
    "- `get_scores()` method calculates a relevance score for each document in relation to the query.\n",
    "- The result `doc_scores` is a list of floating-point numbers, where each number represents the relevance score of the corresponding document in the original corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008678a0-7a2d-424d-88a4-1995ae544754",
   "metadata": {},
   "source": [
    "### Creating a Vector Database\n",
    "Now that we have seen how to create a tradition text database and even learned to query it, let's compare these results to a vector database. To create our database, we will be using Annoy from Spotify. Annoy is a nearest neighbor algorithm that allows for us to easily and efficiently store millions of vectors in an index that we can then retrieve very efficiently. This is achieved by using a computationally efficient algorithm that is written in C. To use annoy, we first need to know the dimensions of our vectors. We can use `.shape` and examine index 1 to get our vector dimensions. Note, these will change from model to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8dd751c7-31f0-4f33-acd4-753305b7ac0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_dim = embeddings.shape[1]\n",
    "vector_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdddeb6-4d24-44c0-bc6b-f2dcd8275eaa",
   "metadata": {},
   "source": [
    "As we can see, we have a dimension of 384. Now that we know that, we can create our index which we will populate with vectors. When creating an AnnoyIndex, we need to specify two things: the number of vectors and the way in which want to measure similarity. We have a few options here. We are using angular.\n",
    "\n",
    "```AnnoyIndex(f, metric) returns a new index that's read-write and stores vector of f dimensions. Metric can be \"angular\", \"euclidean\", \"manhattan\", \"hamming\", or \"dot\". - Annoy docs```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9373bf65-465d-4fa2-af2e-7f775c8fee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "annoy_index = AnnoyIndex(vector_dim, 'angular')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c99906-fbc3-444f-897e-760e401ac4f5",
   "metadata": {},
   "source": [
    "Now that we have created our index, it's time to populate it with data. When we do this, we need to use the `add_item()` method. This will take two arguments: the index number and the embedding itself. We can use enumerate() to create our i variable that will tick up by one each time we loop over our data. This is the equivalent of doing `i=i+1` inside our loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8e57705d-223a-416e-8733-cb906e4536c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, embedding in enumerate(embeddings):\n",
    "    annoy_index.add_item(i, embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cdcb8c-e0e7-4b30-b4de-979f35720c2b",
   "metadata": {},
   "source": [
    "At this stage, our index has the data but it is not yet built. To build it, we can use the `build()` method. This will take 1 argument, the number of trees we want to use. In theory, the more trees, the better, but there's a point where you have diminished returns. Unless you are working with very complex data, 10 is usually a good starting number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b39459f3-4944-4ec2-8d0d-9b7a829748ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annoy_index.build(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5321fa1a-538b-4385-b213-414a66609c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search results for query: 'world war'\n",
      "------------------------------ \n",
      "\n",
      "Speech 1943 Roosevelt\n",
      " Mr. Vice President, Mr. Speaker, Members of the Seventy-eighth Congress:\n",
      "\n",
      "This Seventy-eighth Congress assembles in one of the great moments in the\n",
      "history of the Nation. The past year was perhaps the most crucial for\n",
      "modern civilization; the coming \n",
      "------------- \n",
      "\n",
      "Speech 1945 Roosevelt\n",
      " To the Congress:\n",
      "\n",
      "In considering the State of the Union, the war and the peace that is to\n",
      "follow are naturally uppermost in the minds of all of us.\n",
      "\n",
      "This war must be waged--it is being waged--with the greatest and most\n",
      "persistent intensity. Everythin\n",
      "------------- \n",
      "\n",
      "Speech 1915 Wilson\n",
      " GENTLEMEN OF THE CONGRESS:\n",
      "\n",
      "Since I last had the privilege of addressing you on the state of the Union\n",
      "the war of nations on the other side of the sea, which had then only begun\n",
      "to disclose its portentous proportions, has extended its threatening and\n",
      "------------- \n",
      "\n",
      "Speech 1942 Roosevelt\n",
      " In fulfilling my duty to report upon the State of the Union, I am proud to\n",
      "say to you that the spirit of the American people was never higher than it\n",
      "is today--the Union was never more closely knit together--this country was\n",
      "never more deeply determi\n",
      "------------- \n",
      "\n",
      "Speech 1922 Harding\n",
      " MEMBERS OF THE CONGRESS:\n",
      "\n",
      "So many problems are calling for solution that a recital of all of them, in\n",
      "the face of the known limitations of a short session of Congress, would\n",
      "seem to lack sincerity of purpose. It is four years since the World War\n",
      "ende\n",
      "------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"world war\"\n",
    "query_vector = model.encode(query)\n",
    "similar_item_ids = annoy_index.get_nns_by_vector(query_vector, 5)\n",
    "\n",
    "print(f\"Search results for query: '{query}'\")\n",
    "print(\"------------------------------\", \"\\n\")\n",
    "\n",
    "for idx, row in df.iloc[similar_item_ids].iterrows():\n",
    "    print(f\"Speech {df['year'][idx]} {df['name'][idx]}\\n {df['text'][idx][0:250]}\")\n",
    "    print(\"-------------\", \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7f4749-8b79-4ee4-93c9-7d8b28c541b5",
   "metadata": {},
   "source": [
    "### Chunking by sentence groups\n",
    "This is nice but it would be much more useful if we could find matches from the most relevant sentences within each document. It's hard to see from these results if a speech is particularly relevant to our search or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71986972-ad11-47e1-a0c2-eb95667a09e3",
   "metadata": {},
   "source": [
    "To chunk our data into sentences, we'll first need to separate the text into sentences. To do that, we will use spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cca4bf85-afe8-4b4e-b786-a469feca030a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x13e7f7590>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aa06e7-f320-49be-9849-f3887243ad89",
   "metadata": {},
   "source": [
    "With this pipeline we can now chunk our data. In the cell below, we will be creating a new dataset from our original one while preserving the original index (document) index. We'll make chunks of the original data and also preserve the subindex for each chunk within the document. This means that for any given index in our chunked data, we have access to its position within the document to which it belongs and the corpus as a whole.\n",
    "\n",
    "Let's skip running this code since it can take quite awhile (21 seconds on my machine). We can pre-load it from a saved pickle file instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "11155002-fbe5-4c14-8d72-668ad4b91105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking texts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 228/228 [00:21<00:00, 10.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# Function to chunk text into groups of 3 sentences\n",
    "def chunk_text(text, chunk_size=3):\n",
    "    # Use spaCy's NLP model to process the text and split it into sentences\n",
    "    doc = nlp(text)\n",
    "    # Convert the spaCy sentence objects into a list of sentences\n",
    "    sentences = list(doc.sents)\n",
    "    chunks = []\n",
    "    # Loop over the sentences in steps of 'chunk_size' (default is 3)\n",
    "    for i in range(0, len(sentences), chunk_size):\n",
    "        # Extract a group of 'chunk_size' sentences\n",
    "        chunk = sentences[i:i+chunk_size]\n",
    "        # Join the sentences in the chunk into a single string and append to the 'chunks' list\n",
    "        chunks.append(\" \".join([sent.text for sent in chunk]))\n",
    "    return chunks\n",
    "\n",
    "# Create chunks\n",
    "chunked_data = []\n",
    "# tqtdm gives us a progess bar while we loop through our df\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Chunking texts\"):\n",
    "    # Apply the chunk_text function to the 'text' column of the current row \n",
    "    chunks = chunk_text(row['text'])\n",
    "    # Loop through each chunk and keep track of its index within the document\n",
    "    for chunk_idx, chunk in enumerate(chunks):\n",
    "        # Convert the row's data to a dict\n",
    "        chunk_data = row.to_dict()\n",
    "        # Replace the 'text' field with the current chunk\n",
    "        chunk_data['text'] = chunk\n",
    "        # Add an index to keep track of which document this chunk belongs to\n",
    "        chunk_data['document_index'] = idx\n",
    "        # Add an index to keep track of the chunk's position within the document\n",
    "        chunk_data['chunk_index'] = chunk_idx\n",
    "        chunked_data.append(chunk_data)\n",
    "\n",
    "# Create new DataFrame with chunks\n",
    "chunked_df = pd.DataFrame(chunked_data)\n",
    "chunked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "910953c2-6765-412c-b9ab-36e0af154cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>name</th>\n",
       "      <th>year</th>\n",
       "      <th>embedding</th>\n",
       "      <th>document_index</th>\n",
       "      <th>chunk_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gentlemen of the Senate and Gentlemen of the H...</td>\n",
       "      <td>Adams</td>\n",
       "      <td>1797</td>\n",
       "      <td>[-0.0030435165, 0.09838831, 0.075595096, -0.00...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I submit, however, to your consideration wheth...</td>\n",
       "      <td>Adams</td>\n",
       "      <td>1797</td>\n",
       "      <td>[-0.0030435165, 0.09838831, 0.075595096, -0.00...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have received\\nintelligence of the arrival o...</td>\n",
       "      <td>Adams</td>\n",
       "      <td>1797</td>\n",
       "      <td>[-0.0030435165, 0.09838831, 0.075595096, -0.00...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\nIt may be confidently asserted that nothin...</td>\n",
       "      <td>Adams</td>\n",
       "      <td>1797</td>\n",
       "      <td>[-0.0030435165, 0.09838831, 0.075595096, -0.00...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The state\\nof society has so long been disturb...</td>\n",
       "      <td>Adams</td>\n",
       "      <td>1797</td>\n",
       "      <td>[-0.0030435165, 0.09838831, 0.075595096, -0.00...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20565</th>\n",
       "      <td>\\n\\nPermit me to emphasize once more the need ...</td>\n",
       "      <td>Wilson</td>\n",
       "      <td>1920</td>\n",
       "      <td>[-0.022463989, 0.05254909, 0.0055605615, 0.023...</td>\n",
       "      <td>227</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20566</th>\n",
       "      <td>Indeed, It would be very serviceable to the pu...</td>\n",
       "      <td>Wilson</td>\n",
       "      <td>1920</td>\n",
       "      <td>[-0.022463989, 0.05254909, 0.0055605615, 0.023...</td>\n",
       "      <td>227</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20567</th>\n",
       "      <td>which difficulties are to be met and removed a...</td>\n",
       "      <td>Wilson</td>\n",
       "      <td>1920</td>\n",
       "      <td>[-0.022463989, 0.05254909, 0.0055605615, 0.023...</td>\n",
       "      <td>227</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20568</th>\n",
       "      <td>\\n\\nAllow me to call your attention to the fac...</td>\n",
       "      <td>Wilson</td>\n",
       "      <td>1920</td>\n",
       "      <td>[-0.022463989, 0.05254909, 0.0055605615, 0.023...</td>\n",
       "      <td>227</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20569</th>\n",
       "      <td>I\\nbelieve this to be the faith of America, th...</td>\n",
       "      <td>Wilson</td>\n",
       "      <td>1920</td>\n",
       "      <td>[-0.022463989, 0.05254909, 0.0055605615, 0.023...</td>\n",
       "      <td>227</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20570 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text    name  year  \\\n",
       "0      Gentlemen of the Senate and Gentlemen of the H...   Adams  1797   \n",
       "1      I submit, however, to your consideration wheth...   Adams  1797   \n",
       "2      I have received\\nintelligence of the arrival o...   Adams  1797   \n",
       "3      \\n\\nIt may be confidently asserted that nothin...   Adams  1797   \n",
       "4      The state\\nof society has so long been disturb...   Adams  1797   \n",
       "...                                                  ...     ...   ...   \n",
       "20565  \\n\\nPermit me to emphasize once more the need ...  Wilson  1920   \n",
       "20566  Indeed, It would be very serviceable to the pu...  Wilson  1920   \n",
       "20567  which difficulties are to be met and removed a...  Wilson  1920   \n",
       "20568  \\n\\nAllow me to call your attention to the fac...  Wilson  1920   \n",
       "20569  I\\nbelieve this to be the faith of America, th...  Wilson  1920   \n",
       "\n",
       "                                               embedding  document_index  \\\n",
       "0      [-0.0030435165, 0.09838831, 0.075595096, -0.00...               0   \n",
       "1      [-0.0030435165, 0.09838831, 0.075595096, -0.00...               0   \n",
       "2      [-0.0030435165, 0.09838831, 0.075595096, -0.00...               0   \n",
       "3      [-0.0030435165, 0.09838831, 0.075595096, -0.00...               0   \n",
       "4      [-0.0030435165, 0.09838831, 0.075595096, -0.00...               0   \n",
       "...                                                  ...             ...   \n",
       "20565  [-0.022463989, 0.05254909, 0.0055605615, 0.023...             227   \n",
       "20566  [-0.022463989, 0.05254909, 0.0055605615, 0.023...             227   \n",
       "20567  [-0.022463989, 0.05254909, 0.0055605615, 0.023...             227   \n",
       "20568  [-0.022463989, 0.05254909, 0.0055605615, 0.023...             227   \n",
       "20569  [-0.022463989, 0.05254909, 0.0055605615, 0.023...             227   \n",
       "\n",
       "       chunk_index  \n",
       "0                0  \n",
       "1                1  \n",
       "2                2  \n",
       "3                3  \n",
       "4                4  \n",
       "...            ...  \n",
       "20565           20  \n",
       "20566           21  \n",
       "20567           22  \n",
       "20568           23  \n",
       "20569           24  \n",
       "\n",
       "[20570 rows x 6 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('chunked_df.pkl', 'rb') as f:\n",
    "    chunked_df = pickle.load(f)\n",
    "\n",
    "chunked_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f958d858-a378-4dac-a880-7b8dadddd6b6",
   "metadata": {},
   "source": [
    "Unfortunately, though, our embeddings are precisely the same for each chunk. This isn't good. We want new embeddings for each chunk. Let's go ahead and repeat the steps from earlier in the notebook. This will take quite a bit longer as we now have nearly 21,000 \"documents\". So, again, we'll skip this code to avoid the load to the server. This took a long time - about 12 minutes! - on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e0c7db5e-f0ff-4446-a06d-8fac04a65680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef2bc3eb9a049418f6c77458236966d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/643 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vectorize the chunks\n",
    "# chunks = chunked_df['text'].tolist()\n",
    "# chunk_embeddings = model.encode(chunks, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "44062d6b-f8e5-4cfa-8b42-b03966a1b3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings from the pickle file\n",
    "with open('chunk_embeddings.pkl', 'rb') as f:\n",
    "    chunk_embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "504faf05-bae7-49e4-872e-dd3dc83db4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the original embeddings with the new ones\n",
    "chunked_df['embedding'] = list(chunk_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "52a92985-436b-48a3-bbf3-c1c709cfb239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repeat the same indexing steps as above to create a searchable index on the chunks\n",
    "annoy_index = AnnoyIndex(vector_dim, 'angular')\n",
    "# Add items to the index\n",
    "for i, embedding in enumerate(embeddings):\n",
    "    annoy_index.add_item(i, embedding)\n",
    "annoy_index.build(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066d68a4-1467-4b33-aeab-150038d571a5",
   "metadata": {},
   "source": [
    "Query the new index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "95551bdc-5b92-4614-bb5d-cd77fe238e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search results for query: 'warfare'\n",
      "------------------------------ \n",
      "\n",
      "Speech 1827 Adams\n",
      " \n",
      "\n",
      "The expenditures of the year may perhaps amount to $22,300,000\n",
      "presenting a small excess over the receipts. But of these $22,000,000,\n",
      "upward of $6,000,000 have been applied to the discharge of the\n",
      "principal of the public debt, the whole amount of which, approaching\n",
      "$74,000,000 on January 1st, 1827, will on January 1st, 1828 fall short\n",
      "of $67,500,000. The balance in the Treasury on January 1st, 1828 it is\n",
      "expected will exceed $5,450,000, a sum exceeding that of January 1st,\n",
      "1825, though falling short of that exhibited on January 1st, 1827.\n",
      "------------- \n",
      "\n",
      "Speech 1826 Adams\n",
      " \n",
      "\n",
      "JOHN QUINCY ADAMS\n",
      "------------- \n",
      "\n",
      "Speech 1825 Adams\n",
      " It is with no feeling of\n",
      "pride as an American that the remark may be made that on the\n",
      "comparatively small territorial surface of Europe there are existing\n",
      "upward of 130 of these light-houses of the skies, while throughout the\n",
      "whole American hemisphere there is not one. If we reflect a moment upon\n",
      "the discoveries which in the last four centuries have been made in the\n",
      "physical constitution of the universe by the means of these buildings\n",
      "and of observers stationed in them, shall we doubt of their usefulness\n",
      "to every nation? And while scarcely a year passes over our heads\n",
      "without bringing some new astronomical discovery to light, which we\n",
      "must fain receive at second hand from Europe, are we not cutting\n",
      "ourselves off from the means of returning light for light while we have\n",
      "neither observatory nor observer upon our half of the globe and the\n",
      "earth revolves in perpetual darkness to our unsearching eyes?\n",
      "------------- \n",
      "\n",
      "Speech 1826 Adams\n",
      " They departed cheered by\n",
      "the benedictions of their country, to whom they left the inheritance of\n",
      "their fame and the memory of their bright example. \n",
      "\n",
      "If we turn our thoughts to the condition of their country, in the\n",
      "contrast of the first and last day of that half century, how\n",
      "resplendent and sublime is the transition from gloom to glory! Then,\n",
      "glancing through the same lapse of time, in the condition of the\n",
      "individuals we see the first day marked with the fullness and vigor of\n",
      "youth, in the pledge of their lives, their fortunes, and their sacred\n",
      "honor to the cause of freedom and of man-kind; and on the last,\n",
      "extended on the bed of death, with but sense and sensibility left to\n",
      "breathe a last aspiration to Heaven of blessing upon their country, may\n",
      "we not humbly hope that to them too it was a pledge of transition from\n",
      "gloom to glory, and that while their mortal vestments were sinking into\n",
      "the clod of the valley their emancipated spirits were ascending to the\n",
      "bosom of their God!\n",
      "------------- \n",
      "\n",
      "Speech 1825 Adams\n",
      " There are laws establishing an\n",
      "uniform militia throughout the United States and for arming and\n",
      "equipping its whole body. But it is a body of dislocated members,\n",
      "without the vigor of unity and having little of uniformity but the\n",
      "name. To infuse into this most important institution the power of which\n",
      "it is susceptible and to make it available for the defense of the Union\n",
      "at the shortest notice and at the smallest expense possible of time, of\n",
      "life, and of treasure are among the benefits to be expected from the\n",
      "persevering deliberations of Congress.\n",
      "------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"warfare\"\n",
    "query_vector = model.encode(query)\n",
    "similar_item_ids = annoy_index.get_nns_by_vector(query_vector, 5)\n",
    "\n",
    "print(f\"Search results for query: '{query}'\")\n",
    "print(\"------------------------------\", \"\\n\")\n",
    "for idx, row in chunked_df.iloc[similar_item_ids].iterrows():\n",
    "    print(f\"Speech {chunked_df['year'][idx]} {chunked_df['name'][idx]}\\n {chunked_df['text'][idx]}\")\n",
    "    print(\"-------------\", \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e85eb75-0051-4db3-9064-bf3425b6534d",
   "metadata": {},
   "source": [
    "### More information\n",
    "- [Transformers and LLMs](https://carpentries-incubator.github.io/python-text-analysis/10-finetuning-transformers/index.html)\n",
    "- [The Word2Vec algorithm](https://carpentries-incubator.github.io/python-text-analysis/08-wordEmbed_word2vec-algorithm/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
