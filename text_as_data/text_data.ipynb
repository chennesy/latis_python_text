{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Text as Data (UMN LATIS/Libraries)\n",
    "\n",
    "## Intro\n",
    "This workshop will cover how to:\n",
    "- Read and write text files in Python\n",
    "- Manipulate ‘strings’ of text\n",
    "- Pre-process and clean text for analysis\n",
    "- Count and plot word frequencies\n",
    "- Conduct sentiment analysis\n",
    "- Run basic topic models\n",
    "\n",
    "### Prereqs\n",
    "Be familiar with Python. Have some previous experience with an introductory Python workshop, for example.\n",
    "\n",
    "## JupyterLab - Get attendees set up\n",
    "We'll share JupyterLab features as we go, but to get started let's create a new blank `workshop.ipynb` file that you can work with throughout the workshop. \n",
    "\n",
    "Note the control icons above the Notebook. You can use these to:\n",
    "- add new cells\n",
    "- run code (which you can also do, using control-shift)\n",
    "- and switch cells from code to markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading files\n",
    "\n",
    "Let's start to work with text by reading it in from a series of text files.\n",
    "\n",
    "For most text as data projects, your first step is going to be to read in the files containing the data. Common file types for text data are: \n",
    "* `.txt`\n",
    "* `.csv`\n",
    "* `.json`\n",
    "* `.html` \n",
    "* `.xml`\n",
    "\n",
    "Each file format requires specific Python tools or methods to read, but for our case, we'll be working with .txt files.\n",
    "\n",
    "#### Reading in `.txt` files\n",
    "\n",
    "Python has built-in support for reading in `.txt` files.\n",
    "\n",
    "Let's take a look at the first file in our directory (folder) of State of the Union addresses (`/sotu_text`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new variable called file1 and read (\"r\") the first file in the sotu_text folder\n",
    "file1 = open('sotu_text/215.txt','r') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to print out the file, however, it's not yet stored as a Python string, but as an encoded text \"wrapper\" from the io (input/output) Python module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing doesn't print the contents\n",
    "print(file1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the text, we can use the .read() method of the `TextIOWrapper` and then use the string index of the text object to show the first 250 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = file1.read()\n",
    "print(text[0:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Once we've read in the data, a common next step is to split a longer string into words or-—more helpfully-—\"word chunks\" called \"tokens.\" This step is referred to as \"tokenization\". Tokens can be of any length (sentences, words, or parts of words), but usually we want to reduce our text into meaningful word chunks. \n",
    "\n",
    "#### Tokenizing by whitespace\n",
    "An easy way to tokenize a string, though not the most accurate, is to split up a string using whitespace. Let's save each word to a list variable called 'tokens.' `.split()` creates a Python list of the words from the string. If we leave the argument for `split()` blank, it will split on whitespace characters in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting on other characters\n",
    "\n",
    "Sentence segmentation involves identifying the boundaries of sentences, and provides a different way to tokenize our text.\n",
    "\n",
    "#### Sentence segmentation by splitting on punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of the default whitespace for split(), you can identify the character or characters you'd like to split on\n",
    "sentences = text.split('.')\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how many items are in any list using the len() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that this method doesn't break out sentences that end with other punctuation, like question marks\n",
    "sentences[35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a list of terms, we can count them up using a Python module called \"Counter\" in the collections package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the Counter module from the collections package\n",
    "from collections import Counter\n",
    "\n",
    "token_counts = Counter(tokens)\n",
    "\n",
    "#print(token_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you hit tab after ```token_counts.``` you can see Counter methods that are available to examine this data more closely. We can use ```.most_common(5)``` to look at the five most common words in the corpus. (You can change the argument to print out as many of the most common words as you want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also look at specific word counts by calling the term in the same way you would refer to a Python dictionary key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts['President']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve this token list for later analysis by using built-in string methods to clean the data a bit. We'll create a new list, called \"unigrams,\" to hold each token. Unigrams refer to single words, while \"bigrams\" are two word pairs, and \"trigrams\" three word pairs. Researchers often refer to these collections of tokens of different lengths as \"ngrams.\"\n",
    "\n",
    "We'll use the built-in string method `.lower()` to convert the text all to lower case. This helps us count up word frequencies regardless of capitalization. \n",
    "\n",
    "The `.replace()` method will replace specific characters in the first argument with the second argument. `'text'.replace('t', '')` for example, would change the string 'text' to 'ex', since it would replace every 't' character with an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new empty list\n",
    "unigrams = []\n",
    "\n",
    "# Loop through each token in the tokens list\n",
    "for token in tokens:\n",
    "    token = token.lower() # lowercase tokens\n",
    "    token = token.replace('.', '') # remove periods\n",
    "    token = token.replace('!', '') # remove exclamation points\n",
    "    token = token.replace('?', '') # remove question marks\n",
    "    unigrams.append(token) #append each updated token to the new unigrams list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert this list of unigrams using Counter and see if there were any changes in the most common five words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = Counter(unigrams)\n",
    "print('Original list:', token_counts.most_common(5))\n",
    "print('Cleaned up list:', word_counts.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do you think the counts have changed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing with the Natural Language Toolkit (NLTK)* *\n",
    "\n",
    "The steps we took above are pretty time-consuming, and require us to identify each specific character to remove. Most researchers will use the Natural Language Toolkit (NLTK) or Spacy to accomplish many of the steps we showed manually above with fewer steps. \n",
    "\n",
    "We can use word_tokenize tool to do a lot of the tokenizing work for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert our original sotu string, called ```text```, to lowercase, and tokenize it using `word_tokenize()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = word_tokenize(text.lower())\n",
    "print(word_tokens[0:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing punctuation\n",
    "\n",
    "In the example above, we have a lot of punctuation marks as tokens in our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can manually remove punctuation from strings, which can be useful in certain settings. Let's import the `string` library which includes a quick dictionary of common punctuation marks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip() will remove punctuation from the beginning or end of the string\n",
    "\"?Test 1, 2, 3!\".strip(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove all of the punctuation in a string we can loop through the every character in the string and use the ```.join()``` method to recombine them with an empty string. First let's see how ```.join()``` works on a simpler example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'x'.join('Put an x in between every character')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's remove all punctuation from our SOTU speech\n",
    "clean_text = ''.join(char.strip(string.punctuation) for char in text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize with word_tokenize\n",
    "Now that we removed the punctuation from our text, we can use retokenize the corpus using `clean_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(clean_text.lower())\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of tokens we can count their frequencies in the speech. Let's use a builtin NLTK function called `FreqDist()` to look at our most common words. This is similar to the `Counter` library we worked with earlier, though provides some extended functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the FreqDist function to our tokens variable\n",
    "fdist = FreqDist(tokens)\n",
    "\n",
    "#fdist is a dictionary of unique words and the number of times they occur\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fdist also includes a handy method to find the most common words \n",
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...which, to make things even more complicated, returns a list (see the square brackets continaing comma-separated items) containing tuples (those objects in parentheses, also containing comma-separated items). But we needn't get overly worried about that here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stop words\n",
    "\n",
    "You might have noticed that the most common words above aren't terribly exciting. They're words like \"am\", \"i\", \"the\" and \"a\": stop words. These are rarely useful to us in computational text analysis, so it's very common to remove them completely.\n",
    "\n",
    "NLTK includes a stopwords module we can use. Not all stopwords lists are equal though: for your own research you might want to customize a stopwords list, or find one that is best-suited to your domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# how many stopwords are on the list?\n",
    "len(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the first ten word on the stopword list?\n",
    "stop[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new list of tokens, removing our stopwords along the way. \n",
    "\n",
    "This loop checks each word in our original tokens list, and if it does *not* appear on the stopword list, it adds it to a new list called tokens_clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_clean = [] \n",
    "  \n",
    "for w in tokens: \n",
    "    if w not in stop: \n",
    "        tokens_clean.append(w)\n",
    "tokens_clean[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# advanced we can do the same thing quite efficiently with a list comprehension\n",
    "tokens_clean = [w for w in tokens if w not in stop]\n",
    "tokens_clean[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can re-count the most common words after stop words are removed\n",
    "freq = FreqDist(tokens_clean)\n",
    "freq.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm, still not terribly interesting but getting better..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "\n",
    "Stemming and lemmatization both refer to remove morphological affixes on words. For example, if we stem the word \"grows\", we get \"grow\". If we stem the word \"running\", we get \"run\". We do this because often we care more about the core content of the word (i.e. that it has something to do with growth or running, rather than the fact that it's a third person present tense verb, or progressive participle).\n",
    "\n",
    "NLTK provides many algorithms for stemming. For English, a great baseline is the [Porter](https://github.com/nltk/nltk/blob/develop/nltk/stem/porter.py) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the PorterStemmer and then stem the word \"states\" as an example\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmer.stem('states')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem('government')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem('roosevelt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar manner as the stopwords loop above, we can create a new list of stemmed tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_stemmed = []\n",
    "for t in tokens_clean:\n",
    "    tokens_stemmed.append(stemmer.stem(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#or as a comprehension:\n",
    "tokens_stemmed = [stemmer.stem(t) for t in tokens_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_stemmed[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the words are stemmed, are the most common words any different? \n",
    "\n",
    "Here are the stemmed top ten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_stemmed = FreqDist(tokens_stemmed)\n",
    "for f in freq_stemmed.most_common(10):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the unstemmed top ten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in freq.most_common(10):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar, but with some important differences. Notice that \"work\" went from 42 to 69 after stemming.  \n",
    "\n",
    "Why would that be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in multiple files\n",
    "\n",
    "Often, our text data is split across multiple files in a folder. We can read them all into a single variable using a Python tool called glob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all of the files that end with .txt in the sotu_text/ folder to a variable called sotu_all\n",
    "sotu_all = glob.glob(\"sotu_text/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this just saves the file-paths to a list though\n",
    "sotu_all[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are out of order though. Let's sort the list so that the list index is in the same order as the speeches themselves (sotu_all[0] would equal 001.txt).\n",
    "\n",
    "*Something important to note is that **glob** can pull files differently on different systems (Windows/Mac OS/Linux). If you have a numeric identifier to your files, sorting them is always a good idea for reproducibility of your code, regardless of what system it may be run on*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sotu_all.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sotu_all[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of all the files we need to cycle through each one and save the text from the file.\n",
    "\n",
    "To do that we'll create a new list variable, speeches. For each file in the sotu_all variable we'll open and read the file, and save the text to the speeches list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = []\n",
    "for speech in sotu_all:\n",
    "    s = open(speech, 'r')\n",
    "    text = s.read()\n",
    "    speeches.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can refer to each speech from the list using the list index\n",
    "speeches[45][0:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#which file is that?\n",
    "sotu_all[45]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a short comprehension version to tidy the open/append loop found above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = [open(speech, 'r').read() for speech in sotu_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches[235][0:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Cleaning Function*\n",
    "Now that we have all the text data loaded, we can think about working on the corpus as a whole.\n",
    "\n",
    "Let's create a function that combines all of our cleaning protocols so that we can clean each State of the Union speech with a single piece of code. \n",
    "\n",
    "The function definition opens with the keyword ```def``` followed by the name of the function (clean_speech) and a parenthesized list of parameter names (speech). The body of the function — the statements that are executed when it runs — is indented below the definition line. The body concludes with a return keyword followed by the return value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_speech(speech):\n",
    "    speech = ''.join(word.strip(string.punctuation) for word in speech.lower())\n",
    "    speech = [stemmer.stem(w) for w in word_tokenize(speech) if w not in stop]\n",
    "    return speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can call the function using the name of the function, and the variable you'd like to process as its parameter. To process only the first speech, for example, you could call:\n",
    "\n",
    "```clean_speech(speeches[0])```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also assign the function's output to a variable so you can work with it later:\n",
    "\n",
    "```first_cleaned = clean_speech(speeches[0])```\n",
    "\n",
    "Let's put it all together and clean all of the speeches, and assign them to a new list, ```cleaned_speeches```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell might take a few minutes to run!\n",
    "cleaned_speeches = [clean_speech(speech) for speech in speeches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cleaned_speeches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that each item in the cleaned_speeches list is also a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cleaned_speeches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_speeches[0][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word frequencies across the corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### enumerate()\n",
    "* The built-in enumerate() function allows us to keep a count of our place in a for loop, and to reference the enumerated variable counter later on. We'll call our counter variable 'i' for index. \n",
    "* The first for-loop iterates through each speech in cleaned_speeches\n",
    "* The second for-loop iterates through each word in the speech at hand.\n",
    "* We'll unpack the ```cfd['americ'][i]+=1``` code a bit more later on, but note for now that ```+=1``` counts each occurence of any word that starts with 'americ' and ```[i]``` refers to the speech index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = []\n",
    "\n",
    "for i,speech in enumerate(cleaned_speeches):\n",
    "    freqs.append(FreqDist(speech))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`FreqDist.N()` gives us the total 'outcomes', in other words the total number of words from each document. We can use this to calculate the relative frequency of a specific term to each document and track that value over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(freqs[0].N())\n",
    "print(freqs[0]['war'])\n",
    "print(freqs[0]['war']/freqs[0].N())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's edit the loop above to look at the token 'war' across the full corpus and plot its frequency in each speech. We'll create a Python dictionary where the key is the index of the cleaned_speeches and the value is the ratio of 'war' within the total number of words in each speech. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "war_ratios = []\n",
    "\n",
    "for i,speech in enumerate(cleaned_speeches):\n",
    "    freq = FreqDist(speech)\n",
    "    total_words = freq.N()\n",
    "    war_count = freq['war']\n",
    "    ratio = freq['war']/freq.N()   \n",
    "    tuple = (i, total_words, war_count, ratio)\n",
    "    war_ratios.append(tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert this list of tuples into a Pandas DataFrame so we can work with the data in a more easy-to-read tabular format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(war_ratios, columns =['speech', 'total_words', 'war_count', 'ratio'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotly\n",
    "Let's use an interactive plotting library, Plotly, to build a plot that will allow us to hover over specific points to find more information. This will allow us to pinpoint the spikes in the plot that show the highest frequency of 'war' mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install plotly\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(x=df.index, y=df['ratio'])\n",
    "fig.update_layout(xaxis_title=\"Speech\",\n",
    "    yaxis_title=\"Relative frequency of 'war'\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches[155][:800]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternately we could look at the plain word counts of 'war' from each speech. While the results are similar, it removes some spikes from speeches 24 and 128, which shows that 'war' was a frequent topic relative to the length of those speeches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(x=df.index, y=df['war_count'])\n",
    "fig.update_layout(xaxis_title=\"Speech\",\n",
    "    yaxis_title=\"N of 'war'\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# james madison, 1813, war of 1812\n",
    "print(speeches[24][-300:])\n",
    "\n",
    "# woodrow wilson, 1917, wwi\n",
    "print(speeches[128][:800])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis*\n",
    "\n",
    "Sentiment analysis is an exploratory data analysis technique that \"seeks to quantify the emotional intensity of words and phrases within a text.\" (quote from the [Programming Historian SA tutorial](https://programminghistorian.org/en/lessons/sentiment-analysis))\n",
    "\n",
    "We can use more NLTK tools to run a simple sentiment analysis on our SOTU corpus. We'll download the vader_lexicon for sentiment analysis and the Vader and Sentiment modules. Don't worry if you see a warning that we don't have the twython library. We won't be using that since we're not analyzing twitter text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we could use a tokenizer that works best for sentiment analysis (see the commented out code below). Since we've already tokenized our text we'll stick with that corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')\n",
    "#tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the vader SentimentIntensityAnalyzer and save it to a variable called sid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the features of the sentiment analysis tool. You can take a look at some of those features by typing sid. and then tabbing through the options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's look at the 'polarity_scores' for a specific speech. For Sentiment analysis we don't need the cleaned speech, so we'll go back to our original speeches list.\n",
    "\n",
    "polarity_scores will give us positive and negative scores. This feature is built into VADER and can be requested on demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = sid.polarity_scores(speeches[100])\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries\n",
    "We can format the output by looping through the scores dictionary. Remember that dictionaries are key:value pairs stored in curly brackets. We can cycle through the scores dictionary like so: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in sorted(scores):\n",
    "    print('{0}: {1}'.format(key, scores[key]), end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the scores for the entire speeches corpus.\n",
    "We'll create another dictionary, 'all_scores', that will use the speeches index as the key, and the scores as its value. Note that this means that the value for each item in 'all_scores' will contain *another* dictionary.\n",
    "\n",
    "This might take a few minutes to run because it has to analyze all 235 speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = {}\n",
    "for idx, speech in enumerate(speeches):\n",
    "    all_scores[idx] = sid.polarity_scores(speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take a look at the scores for specific speeches by referencing the index/key of all_scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores[235]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at a specific score by referencing the key within the scores dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores[235]['neg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we can list all of the negative scores for the corpus. \n",
    "\n",
    "To keep it somewhat simple, let's just create a new dictionary that will only contain negative scores. We can create an empty dictionary called negative, then cycle through each key:value item in the all_scores dictionary from above. For each item, we'll assign the index number as its key and the negative score as its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = {}\n",
    "for score in all_scores.items():\n",
    "    negative[score[0]] =  score[1]['neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_speech = list(negative.keys())\n",
    "y_neg = list(negative.values())\n",
    "fig = px.line(x=x_speech, y=y_neg)\n",
    "fig.update_layout(xaxis_title=\"Speech\",\n",
    "    yaxis_title=\"Negative value\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most negative speeches\n",
    "The graph gives us a nice visualization of some overall trends, and we can take a closer look at some of the most negative speeches here. We could also sort our dictionary, using the `sorted()` method, to list the speeches with the most negative scores in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(negative, key=negative.get, reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches[222][0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Least negative speeches\n",
    "We can use the default sort (ascending values) to view the least negative speeches in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(negative, key=negative.get)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic models (sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches[0][0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize regex tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Vectorize document using TF-IDF\n",
    "tfidf = TfidfVectorizer(lowercase=True,\n",
    "                        stop_words='english',\n",
    "                        ngram_range = (1,1),\n",
    "                        tokenizer = tokenizer.tokenize)\n",
    "\n",
    "# Fit and Transform the documents\n",
    "train_data = tfidf.fit_transform(speeches)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 12\n",
    "\n",
    "model=LatentDirichletAllocation(n_components=n_components)\n",
    "\n",
    "# Fit and Transform SVD model on data\n",
    "lda_matrix = model.fit_transform(train_data)\n",
    "\n",
    "# Get Components \n",
    "lda_components=model.components_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the topics with their terms\n",
    "terms = tfidf.get_feature_names_out()\n",
    "\n",
    "for index, component in enumerate(lda_components):\n",
    "    zipped = zip(terms, component)\n",
    "    top_terms_key=sorted(zipped, key = lambda t: t[1], reverse=True)[:7]\n",
    "    top_terms_list=list(dict(top_terms_key).keys())\n",
    "    print(\"Topic \"+str(index)+\": \",top_terms_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic models (Gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis.gensim_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# had issues with gensim and recent scipy package. had to back install 1.12.0\n",
    "#scipy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(cleaned_speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_count = len(cleaned_speeches)\n",
    "num_topics = 12 # Change the number of topics\n",
    "passes = 10 # The number of passes used to train the model\n",
    "# Remove terms that appear in less than 50 documents and terms that occur in more than 90% of documents.\n",
    "dictionary.filter_extremes(no_below=50, no_above=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(speech) for speech in cleaned_speeches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train the LDA model\n",
    "model = gensim.models.LdaModel(\n",
    "    corpus=bow_corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the coherence score using UMass\n",
    "# u_mass is measured from -14 to 14, higher is better\n",
    "coherence_model_lda = CoherenceModel(\n",
    "    model=model,\n",
    "    corpus=bow_corpus,\n",
    "    dictionary=dictionary, \n",
    "    coherence='u_mass'\n",
    ")\n",
    "\n",
    "# Compute Coherence Score using UMass\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic_num in range(0, num_topics):\n",
    "    word_ids = model.get_topic_terms(topic_num)\n",
    "    words = []\n",
    "    for wid, weight in word_ids:\n",
    "        word = dictionary.id2token[wid]\n",
    "        words.append(word)\n",
    "    print(\"Topic {}\".format(str(topic_num).ljust(5)), \" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "lda_viz = pyLDAvis.gensim_models.prepare(model, bow_corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to Go for Help\n",
    "- Contact LATISresearch@umn.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
