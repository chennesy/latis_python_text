{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Text as Data (UMN LATIS/Libraries)\n",
    "\n",
    "## Intro\n",
    "This workshop will cover how to:\n",
    "- Read and write text files in Python\n",
    "- Manipulate ‘strings’ of text\n",
    "- Pre-process and clean text for analysis\n",
    "- Count and plot word frequencies\n",
    "- Conduct sentiment analysis\n",
    "- ~Run basic topic models~\n",
    "\n",
    "### Prereqs\n",
    "Be familiar with Python. Have some previous experience with an introductory Python workshop, for example.\n",
    "\n",
    "## JupyterLab - Get attendees set up\n",
    "We'll share JupyterLab features as we go, but to get started let's create a new blank `workshop.ipynb` file that you can work with throughout the workshop. \n",
    "\n",
    "Note the control icons above the Notebook. You can use these to:\n",
    "- add new cells\n",
    "- run code (which you can also do, using control-shift)\n",
    "- and switch cells from code to markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading files\n",
    "\n",
    "Let's start to work with text by reading it in from a series of text files.\n",
    "\n",
    "For most text as data projects, your first step is going to be to read in the files containing the data. Common file types for text data are: \n",
    "* `.txt`\n",
    "* `.csv`\n",
    "* `.json`\n",
    "* `.html` \n",
    "* `.xml`\n",
    "\n",
    "Each file format requires specific Python tools or methods to read, but for our case, we'll be working with .txt files.\n",
    "\n",
    "#### Reading in `.txt` files\n",
    "\n",
    "Python has built-in support for reading in `.txt` files.\n",
    "\n",
    "Let's take a look at the first file in our directory (folder) of State of the Union addresses which is located one directory back from the `text_as_data` folder (`../sotu_kaggle`). These text files are available to [download on Kaggle](https://www.kaggle.com/datasets/rtatman/state-of-the-union-corpus-1989-2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new variable called file1 and read (\"r\") the first file in the sotu_text folder\n",
    "file1 = open('../sotu_kaggle/Clinton_1995.txt','r') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to print out the file, however, it's not yet stored as a Python string, but as an encoded text \"wrapper\" from the io (input/output) Python module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing doesn't print the contents\n",
    "print(file1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the text, we can use the .read() method of the `TextIOWrapper` and then use the string index of the text object to show the first 250 characters. Once we save the contents of the file1 text to a new variable we can close the file1 object so that our computer memory doesn't have to keep track of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = file1.read()\n",
    "print(text[0:250])\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more efficient way to work with text files in Python is to use a `with` statement, which will automatically close the file for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../sotu_kaggle/Clinton_1995.txt','r') as file1:\n",
    "    text = file1.read()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Once we've read in the data, a common next step is to split a longer string into words or-—more helpfully-—\"word chunks\" called \"tokens.\" This step is referred to as \"tokenization\". Tokens can be of any length (sentences, words, or parts of words), but usually we want to reduce our text into meaningful word chunks. \n",
    "\n",
    "#### Tokenizing by whitespace\n",
    "An easy way to tokenize a string, though not the most accurate, is to split up a string using whitespace. Let's save each word to a list variable called 'tokens.' `.split()` creates a Python list of the words from the string. If we leave the argument for `split()` blank, it will split on whitespace characters in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting on other characters\n",
    "\n",
    "Sentence segmentation involves identifying the boundaries of sentences, and provides a different way to tokenize our text.\n",
    "\n",
    "#### Sentence segmentation by splitting on punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of the default whitespace for split(), you can identify the character or characters you'd like to split on\n",
    "sentences = text.split('.')\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how many items are in any list using the len() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that this method doesn't break out sentences that end with other punctuation, like question marks\n",
    "sentences[40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a list of terms, we can count them up using a Python module called \"Counter\" in the collections package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the Counter module from the collections package\n",
    "from collections import Counter\n",
    "\n",
    "token_counts = Counter(tokens)\n",
    "\n",
    "#print(token_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you hit tab after ```token_counts.``` you can see Counter methods that are available to examine this data more closely. We can use ```.most_common(5)``` to look at the five most common words in the corpus. (You can change the argument to print out as many of the most common words as you want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also look at specific word counts by calling the term in the same way you would refer to a Python dictionary key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts['President']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve this token list for later analysis by using built-in string methods to clean the data a bit. We'll create a new list, called \"unigrams,\" to hold each token. Unigrams refer to single words, while \"bigrams\" are two word pairs, and \"trigrams\" three word pairs. Researchers often refer to these collections of tokens of different lengths as \"ngrams.\"\n",
    "\n",
    "We'll use the built-in string method `.lower()` to convert the text all to lower case. This helps us count up word frequencies regardless of capitalization. \n",
    "\n",
    "The `.replace()` method will replace specific characters in the first argument with the second argument. `'text'.replace('t', '')` for example, would change the string 'text' to 'ex', since it would replace every 't' character with an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new empty list\n",
    "unigrams = []\n",
    "\n",
    "# Loop through each token in the tokens list\n",
    "for token in tokens:\n",
    "    token = token.lower() # lowercase tokens\n",
    "    token = token.replace('.', '') # remove periods\n",
    "    token = token.replace('!', '') # remove exclamation points\n",
    "    token = token.replace('?', '') # remove question marks\n",
    "    unigrams.append(token) #append each updated token to the new unigrams list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert this list of unigrams using Counter and see if there were any changes in the most common five words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = Counter(unigrams)\n",
    "print('Original list:', token_counts.most_common(5))\n",
    "print('Cleaned up list:', word_counts.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do you think the counts have changed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing with the Natural Language Toolkit (NLTK)* *\n",
    "\n",
    "The steps we took above are pretty time-consuming, and require us to identify each specific character to remove. Most researchers will use the Natural Language Toolkit (NLTK) or Spacy to accomplish many of the steps we showed manually above with fewer steps. \n",
    "\n",
    "We can use word_tokenize tool to do a lot of the tokenizing work for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert our original sotu string, called ```text```, to lowercase, and tokenize it using `word_tokenize()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = word_tokenize(text.lower())\n",
    "print(word_tokens[0:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing punctuation\n",
    "\n",
    "In the example above, we have a lot of punctuation marks as tokens in our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can manually remove punctuation from strings, which can be useful in certain settings. Let's import the `string` library which includes a quick dictionary of common punctuation marks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip() will remove punctuation from the beginning or end of the string\n",
    "\"?Test 1, 2, 3!\".strip(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove all of the punctuation in a string we can loop through the every character in the string and use the ```.join()``` method to recombine them with an empty string. First let's see how ```.join()``` works on a simpler example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'x'.join('Put an x in between every character')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's remove all punctuation from our SOTU speech\n",
    "clean_text = ''.join(char.strip(string.punctuation) for char in text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize with word_tokenize\n",
    "Now that we removed the punctuation from our text, we can use retokenize the corpus using `clean_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(clean_text.lower())\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of tokens we can count their frequencies in the speech. Let's use a builtin NLTK function called `FreqDist()` to look at our most common words. This is similar to the `Counter` library we worked with earlier, though provides some extended functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the FreqDist function to our tokens variable\n",
    "fdist = FreqDist(tokens)\n",
    "\n",
    "#fdist is a dictionary of unique words and the number of times they occur\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fdist also includes a handy method to find the most common words \n",
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...which, to make things even more complicated, returns a list (see the square brackets continaing comma-separated items) containing tuples (those objects in parentheses, also containing comma-separated items). But we needn't get overly worried about that here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stop words\n",
    "\n",
    "You might have noticed that the most common words above aren't terribly exciting. They're words like \"am\", \"i\", \"the\" and \"a\": stop words. These are rarely useful to us in computational text analysis, so it's very common to remove them completely.\n",
    "\n",
    "NLTK includes a stopwords module we can use. Not all stopwords lists are equal though: for your own research you might want to customize a stopwords list, or find one that is best-suited to your domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# how many stopwords are on the list?\n",
    "len(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the first ten word on the stopword list?\n",
    "stop[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new list of tokens, removing our stopwords along the way. \n",
    "\n",
    "This loop checks each word in our original tokens list, and if it does *not* appear on the stopword list, it adds it to a new list called tokens_clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_clean = [] \n",
    "  \n",
    "for w in tokens: \n",
    "    if w not in stop: \n",
    "        tokens_clean.append(w)\n",
    "tokens_clean[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# advanced we can do the same thing quite efficiently with a list comprehension\n",
    "tokens_clean = [w for w in tokens if w not in stop]\n",
    "tokens_clean[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can re-count the most common words after stop words are removed\n",
    "freq = FreqDist(tokens_clean)\n",
    "freq.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm, still not terribly interesting but getting better..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "\n",
    "Stemming and lemmatization both refer to remove morphological affixes on words. For example, if we stem the word \"grows\", we get \"grow\". If we stem the word \"running\", we get \"run\". We do this because often we care more about the core content of the word (i.e. that it has something to do with growth or running, rather than the fact that it's a third person present tense verb, or progressive participle).\n",
    "\n",
    "NLTK provides many algorithms for stemming. For English, a great baseline is the [Porter](https://github.com/nltk/nltk/blob/develop/nltk/stem/porter.py) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the PorterStemmer and then stem the word \"states\" as an example\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmer.stem('states')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem('government')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem('roosevelt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar manner as the stopwords loop above, we can create a new list of stemmed tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_stemmed = []\n",
    "for t in tokens_clean:\n",
    "    tokens_stemmed.append(stemmer.stem(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#or as a comprehension:\n",
    "tokens_stemmed = [stemmer.stem(t) for t in tokens_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_stemmed[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the words are stemmed, are the most common words any different? \n",
    "\n",
    "Here are the stemmed top ten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_stemmed = FreqDist(tokens_stemmed)\n",
    "for f in freq_stemmed.most_common(10):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the unstemmed top ten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in freq.most_common(10):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar, but with some important differences. Notice that \"work\" went from 40 to 66 after stemming.  \n",
    "\n",
    "Why would that be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in multiple files\n",
    "\n",
    "Often, our text data is split across multiple files in a folder. We can read them all into a single variable using a Python tool called glob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all of the files that end with .txt in the sotu_text/ folder to a variable called sotu_all\n",
    "sotu_all = glob.glob(\"../sotu_kaggle/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this just saves the file-paths to a list though\n",
    "sotu_all[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are out of order though. Let's sort the list so that the list index is in the same order as the speeches themselves (sotu_all[0] would equal 001.txt).\n",
    "\n",
    "*Something important to note is that **glob** can pull files differently on different systems (Windows/Mac OS/Linux). If you have a numeric identifier to your files, sorting them is always a good idea for reproducibility of your code, regardless of what system it may be run on*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sotu_all.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sotu_all[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that they're in order by filename, so they're in alphabetical order. But the file names include both the President name and year. If we save the filenames as the key for a Python dictionary we can convert them to separate year and name fields later on.\n",
    "\n",
    "Now that we have a list of all the files we need to cycle through each one and save the text from the file. To do that we'll create a new dictionary variable, `speeches`. For each file in the `sotu_all` variable we'll first clean the file name to retain the `Name_YYYY` string to the dict key, and then open and read the file to save as the dict value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = {}\n",
    "for speech in sotu_all:\n",
    "    filename = speech.lstrip('../sotu_kaggle/')[:-4]\n",
    "    with open(speech,'r') as sotu:\n",
    "        speeches[filename] = sotu.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python dictionaries\n",
    "\n",
    "Note that we can't look at an item in a Python dict using an index number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to use a specific key from the dict to examine the value of an item in a Python dictionary. You can views the keys using `.keys()` and values using `.values()`. You can also loop through both using the `.items()` method, which we'll do a little later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can refer to each speech from the list using keys\n",
    "speeches['Carter_1981'][0:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(speeches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Cleaning Function*\n",
    "Now that we have all the text data loaded, we can think about working on the corpus as a whole.\n",
    "\n",
    "Let's create a function that combines all of our cleaning protocols so that we can clean each State of the Union speech with a single piece of code. \n",
    "\n",
    "The function definition opens with the keyword ```def``` followed by the name of the function (clean_speeches) and a parenthesized list of parameter names (speeches). The body of the function — the statements that are executed when it runs — is indented below the definition line. The body concludes with a return keyword followed by the return value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_speeches(speeches):\n",
    "    #create an empty dict to hold our cleaned content\n",
    "    cleaned_speeches = {}\n",
    "    #loop through the dict items\n",
    "    for key, speech in speeches.items():\n",
    "        speech = ''.join(word.strip(string.punctuation) for word in speech.lower())\n",
    "        speech = [stemmer.stem(w) for w in word_tokenize(speech) if w not in stop]\n",
    "        #assign the same key to the cleaned_speeches dict, with a value of speech (our cleaned speec)\n",
    "        cleaned_speeches[key] = speech\n",
    "    return cleaned_speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can call the function using the name of the function, and the variable you'd like to process as its parameter. We'll process our speeches list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell might take a few minutes to run!\n",
    "cleaned_speeches = clean_speeches(speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cleaned_speeches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first ten tokens from the cleaned Carter speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_speeches['Carter_1981'][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word frequencies across the corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to a dataframe\n",
    "Let's convert the dictionary to a dataframe so we can parse the years and names from the filenames, which will allow us to sort and plot our data by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the dictionary to a df\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(cleaned_speeches.items(), columns=['name_year', 'text'])   \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create two new columns to hold the output of the `split()` method applied on the underscore symbol since that conssitently separates our name and year values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split name_date column by delimiter\n",
    "df[['name','year']] = df['name_year'].str.split('_',expand=True)\n",
    "# drop the now duplicative name_year column\n",
    "df = df.drop('name_year', axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can sort our dataframe by year to help us plot concepts over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('year')\n",
    "df.sort_index(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's drop the empty first row\n",
    "df = df.drop(index='1790')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = [FreqDist(speech) for speech in df['text']]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(freqs[0].N())\n",
    "print(freqs[0]['war'])\n",
    "print(freqs[0]['war']/freqs[0].N())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`FreqDist.N()` gives us the total 'outcomes', in other words the total number of words from each document. We can use this to calculate the relative frequency of a specific term to each document and track that value over time. Let's assign the contents of the `freqs` list to a new column in our df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['freq_dist'] = freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the most common words by year now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df.iterrows():\n",
    "    print(i, row['name'])\n",
    "    print(row['freq_dist'].most_common(4))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's edit the loop above to look at the token 'war' across the full corpus and plot its frequency in each speech. We'll create three new columns:\n",
    "- `word_count` pulls the word count of 'war' from each `freq_dist` object\n",
    "- `total_words` uses `.N()` to add up the number of words from the `freq_dist` object (which is the cleaned text, not total word count from the speech)\n",
    "- `ratio` represents the ratio of the word 'war' to the total number of words per speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'war'\n",
    "df['word_count'] = [freq_dist[word] for freq_dist in df['freq_dist']]\n",
    "df['total_words'] = [words.N() for words in df['freq_dist']]\n",
    "df['ratio'] = df['word_count'] / df['total_words']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't get into much detail about matplotlib, but we can visualize elements from our dataframe using plt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create a line plot with the values of our word_count column\n",
    "ax = df['word_count'].plot(kind='line', figsize=(10, 6), title=f'Word count of \"{word}\"')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Count')\n",
    "# only show x axis label for every tenth year\n",
    "ax.set_xticks(range(0, len(df.index), 10))\n",
    "ax.set_xticklabels(df.index[::10], rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotly\n",
    "Let's use an interactive plotting library, Plotly, to build a plot that will allow us to hover over specific points to find more information. This will allow us to pinpoint the spikes in the plot that show the highest frequency of 'war' mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install plotly\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a line plot assigning the x and y axes\n",
    "fig = px.line(x=df.index, y=df['ratio'])\n",
    "# add labels\n",
    "fig.update_layout(xaxis_title=\"Speech\",\n",
    "    yaxis_title=\"Relative frequency of 'war'\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 1944 speech has the most frequent mention of the word 'war'. Let's take a closer look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc['1944']['name'])\n",
    "print(df.loc['1944']['freq_dist'].most_common(25))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternately we could look at the plain word counts of 'war' from each speech. While the results are similar, it removes some spikes from speeches in 1813 and 1917, which shows that 'war' was a frequent topic relative to the length of those speeches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(x=df.index, y=df['word_count'])\n",
    "fig.update_layout(xaxis_title=\"Speech\",\n",
    "    yaxis_title=\"N of 'war'\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# james madison, 1813, war of 1812\n",
    "print(df.loc['1813']['name'])\n",
    "print(df.loc['1813']['freq_dist'].most_common(10))\n",
    "\n",
    "\n",
    "# woodrow wilson, 1917, wwi\n",
    "print(df.loc['1917']['name'])\n",
    "print(df.loc['1917']['freq_dist'].most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis*\n",
    "\n",
    "Sentiment analysis is an exploratory data analysis technique that \"seeks to quantify the emotional intensity of words and phrases within a text.\" (quote from the [Programming Historian SA tutorial](https://programminghistorian.org/en/lessons/sentiment-analysis))\n",
    "\n",
    "We can use more NLTK tools to run a simple sentiment analysis on our SOTU corpus. We'll download the vader_lexicon for sentiment analysis and the Vader and Sentiment modules. Don't worry if you see a warning that we don't have the twython library. We won't be using that since we're not analyzing twitter text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import sentiment\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we could use a tokenizer that works best for sentiment analysis (see the commented out code below). Since we've already tokenized our text we'll stick with that corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')\n",
    "#tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the vader SentimentIntensityAnalyzer and save it to a variable called sid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the features of the sentiment analysis tool. You can take a look at some of those features by typing sid. and then tabbing through the options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's look at the 'polarity_scores' for a specific speech. For Sentiment analysis we don't need the cleaned speech, so we'll go back to our original speeches list.\n",
    "\n",
    "polarity_scores will give us positive and negative scores. This feature is built into VADER and can be requested on demand. We're going to go back to our original speeches dictionary, because we don't want to work with cleaned text for sentiment analysis. Punctuation, stop words, capitalization, etc. can all influence how positive or negative a text is deemed. For example, \"AWESOME!\" is more positive than \"awesome?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = sid.polarity_scores(speeches['Roosevelt_1938'])\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the scores for the entire speeches corpus.\n",
    "We'll create another dictionary, 'all_scores', that will use the speeches index as the key, and the polarity_scores output as its value. Note that this means that the value for each item in 'all_scores' will contain *another* dictionary.\n",
    "\n",
    "This might take a few minutes to run because it has to analyze all 235 speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = {}\n",
    "for k,v in speeches.items():\n",
    "    all_scores[k] = sid.polarity_scores(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take a look at the scores for specific speeches by referencing the key of `all_scores`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores['Trump_2017']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at a specific score by referencing the key within the scores dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores['Trump_2017']['neg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we can list all of the negative scores for the corpus. \n",
    "\n",
    "To keep it somewhat simple, let's just create a new dictionary that will only contain negative scores. We can create an empty dictionary called negative, then cycle through each key:value item in the all_scores dictionary from above. For each item, we'll assign the index number as its key and the negative score as its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = {}\n",
    "for score in all_scores.items():\n",
    "    negative[score[0]] =  score[1]['neg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a weird plot since our speeches dictionary, and our all_scores dictionaries are in alphabetical order, not in order by time. But we can still use it as a quick way to visualize the least and most negatives speeches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_speech = list(negative.keys())\n",
    "y_neg = list(negative.values())\n",
    "fig = px.line(x=x_speech, y=y_neg)\n",
    "fig.update_layout(xaxis_title=\"Speech\",\n",
    "    yaxis_title=\"Negative value\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most negative speeches\n",
    "The graph gives us a nice visualization of some overall trends, and we can take a closer look at some of the most negative speeches here. We could also sort our dictionary, using the `sorted()` method, to list the speeches with the most negative scores in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(negative, key=negative.get, reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Least negative speeches\n",
    "We can use the default sort (ascending values) to view the least negative speeches in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(negative, key=negative.get)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to go for help\n",
    "- Contact LATISresearch@umn.edu\n",
    "- [Libaries Text Mining guide](https://libguides.umn.edu/text-mining)\n",
    "- [Join the Text as Data Practice Group email list]()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
